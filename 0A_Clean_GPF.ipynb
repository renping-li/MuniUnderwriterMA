{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcf658d-6b15-49b2-8987-b218916c0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pickle\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "try:\n",
    "    del(FUN_proc_name)\n",
    "except:\n",
    "    pass\n",
    "import FUN_proc_name\n",
    "importlib.reload(FUN_proc_name)\n",
    "from FUN_proc_name import FUN_proc_name\n",
    "\n",
    "state_abbreviations = {\n",
    "    \"ALABAMA\": \"AL\",\"ALASKA\": \"AK\",\"ARIZONA\": \"AZ\",\"ARKANSAS\": \"AR\",\"CALIFORNIA\": \"CA\",\n",
    "    \"COLORADO\": \"CO\",\"CONNECTICUT\": \"CT\",\"DELAWARE\": \"DE\",\"FLORIDA\": \"FL\",\"GEORGIA\": \"GA\",\n",
    "    \"HAWAII\": \"HI\",\"IDAHO\": \"ID\",\"ILLINOIS\": \"IL\",\"INDIANA\": \"IN\",\"IOWA\": \"IA\",\n",
    "    \"KANSAS\": \"KS\",\"KENTUCKY\": \"KY\",\"LOUISIANA\": \"LA\",\"MAINE\": \"ME\",\"MARYLAND\": \"MD\",\n",
    "    \"MASSACHUSETTS\": \"MA\",\"MICHIGAN\": \"MI\",\"MINNESOTA\": \"MN\",\"MISSISSIPPI\": \"MS\",\"MISSOURI\": \"MO\",\n",
    "    \"MONTANA\": \"MT\",\"NEBRASKA\": \"NE\",\"NEVADA\": \"NV\",\"NEW HAMPSHIRE\": \"NH\",\"NEW JERSEY\": \"NJ\",\n",
    "    \"NEW MEXICO\": \"NM\",\"NEW YORK\": \"NY\",\"NORTH CAROLINA\": \"NC\",\"NORTH DAKOTA\": \"ND\",\"OHIO\": \"OH\",\n",
    "    \"OKLAHOMA\": \"OK\",\"OREGON\": \"OR\",\"PENNSYLVANIA\": \"PA\",\"RHODE ISLAND\": \"RI\",\"SOUTH CAROLINA\": \"SC\",\n",
    "    \"SOUTH DAKOTA\": \"SD\",\"TENNESSEE\": \"TN\",\"TEXAS\": \"TX\",\"UTAH\": \"UT\",\"VERMONT\": \"VT\",\n",
    "    \"VIRGINIA\": \"VA\",\"WASHINGTON\": \"WA\",\"WEST VIRGINIA\": \"WV\",\"WISCONSIN\": \"WI\",\"WYOMING\": \"WY\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72361c82-7d73-4089-9d4b-00f8ea38dcd5",
   "metadata": {},
   "source": [
    "# 1. SDC Global Public Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7e6e1-5ead-4567-8a4c-6db0e9129c4b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- In 2020 and onwards, there are many bonds with yield below 1%, and also lower than NIC (when that field is available).\n",
    "- SDC GPF does not have any field named \"offering price\" or \"reoffering price\", only a field named \"price/yield\" which could be either price or yield. However, Cestau, Green, Hollifield, Schürhoff (The Cost Burden of Negotiated Sales Restrictions) state that \"We obtain information about bonds and issuers from SDC Platinum. These data include issuer characteristics such as name, state, type, **reoffering prices or yields for each bond issue**, and issue characteristics such as issue description, maturity, sale date, coupon, coupon type, call schedule, taxable status, bank qualified indicator, ratings, a refunding indicator, and sinking fund provisions.\" So it is obviously reoffering price/yield in the raw data.\n",
    "- \"The TIC is defined as the discount rate which equates the principal and semi-annual interest payments on the bonds to the purchase price paid by the underwriter to the issuer\". And \"the purchase price of a bond issue is the aggregate par amount of all the maturities, plus accrued interest and original issue premium (or less any original issue discount), and **less the underwriter’s discount**.\" Hence, based on TIC and its implied offering price, and the initial trading price, I can create my own version of underwriting spread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c70053-ba6b-4eef-989d-4cbf79745378",
   "metadata": {},
   "source": [
    "## 1.1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc6e3b-6733-4326-a8db-ad0696306fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Import data #\n",
    "###############\n",
    "\n",
    "# Note that Pandas is unable to read excel formulas. I copy and value-only paste for all cells in \"GPF.xlsx\" files\n",
    "\n",
    "GPF_Full_19670101_19671231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19670101_19671231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19680101_19681231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19680101_19681231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19690101_19691231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19690101_19691231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19700101_19701231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19700101_19701231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19710101_19711231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19710101_19711231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19720101_19721231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19720101_19721231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19730101_19731231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19730101_19731231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19740101_19741231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19740101_19741231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19750101_19751231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19750101_19751231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19760101_19761231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19760101_19761231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19770101_19771231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19770101_19771231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19780101_19781231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19780101_19781231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19790101_19791231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19790101_19791231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19800101_19800331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19800101_19800331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19800401_19801231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19800401_19801231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19810101_19811231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19810101_19811231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19820101_19821231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19820101_19821231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19830101_19830630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19830101_19830630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19830701_19831231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19830701_19831231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19840101_19840630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19840101_19840630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19840701_19841231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19840701_19841231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19850101_19850630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19850101_19850630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19850701_19851231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19850701_19851231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19860101_19860630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19860101_19860630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19860701_19861231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19860701_19861231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19870101_19870630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19870101_19870630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19870701_19871231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19870701_19871231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19880101_19880630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19880101_19880630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19880701_19881231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19880701_19881231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19890101_19890630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19890101_19890630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19890701_19891231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19890701_19891231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19900101_19900331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19900101_19900331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19900401_19900831 = pd.read_excel(\"../RawData/SDC/GPF_Full_19900401_19900831.xlsx\",skiprows=[0])\n",
    "GPF_Full_19900901_19901231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19900901_19901231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19910101_19910430 = pd.read_excel(\"../RawData/SDC/GPF_Full_19910101_19910430.xlsx\",skiprows=[0])\n",
    "GPF_Full_19910501_19910831 = pd.read_excel(\"../RawData/SDC/GPF_Full_19910501_19910831.xlsx\",skiprows=[0])\n",
    "GPF_Full_19910901_19911031 = pd.read_excel(\"../RawData/SDC/GPF_Full_19910901_19911031.xlsx\",skiprows=[0])\n",
    "GPF_Full_19911101_19911231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19911101_19911231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19920101_19920430 = pd.read_excel(\"../RawData/SDC/GPF_Full_19920101_19920430.xlsx\",skiprows=[0])\n",
    "GPF_Full_19920501_19920831 = pd.read_excel(\"../RawData/SDC/GPF_Full_19920501_19920831.xlsx\",skiprows=[0])\n",
    "GPF_Full_19920901_19921231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19920901_19921231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19930101_19930430 = pd.read_excel(\"../RawData/SDC/GPF_Full_19930101_19930430.xlsx\",skiprows=[0])\n",
    "GPF_Full_19930501_19930630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19930501_19930630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19930701_19930930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19930701_19930930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19931001_19931231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19931001_19931231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19940101_19940331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19940101_19940331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19940401_19940630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19940401_19940630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19940701_19940930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19940701_19940930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19941001_19941231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19941001_19941231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19950101_19950331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19950101_19950331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19950401_19950630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19950401_19950630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19950701_19950930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19950701_19950930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19951001_19951231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19951001_19951231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19960101_19960430 = pd.read_excel(\"../RawData/SDC/GPF_Full_19960101_19960430.xlsx\",skiprows=[0])\n",
    "GPF_Full_19960501_19960831 = pd.read_excel(\"../RawData/SDC/GPF_Full_19960501_19960831.xlsx\",skiprows=[0])\n",
    "GPF_Full_19960901_19961231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19960901_19961231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19970101_19970331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19970101_19970331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19970401_19970630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19970401_19970630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19970701_19970930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19970701_19970930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19971001_19971231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19971001_19971231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19980101_19980331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19980101_19980331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19980401_19980630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19980401_19980630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19980701_19980930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19980701_19980930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19981001_19981231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19981001_19981231.xlsx\",skiprows=[0])\n",
    "GPF_Full_19990101_19990331 = pd.read_excel(\"../RawData/SDC/GPF_Full_19990101_19990331.xlsx\",skiprows=[0])\n",
    "GPF_Full_19990401_19990630 = pd.read_excel(\"../RawData/SDC/GPF_Full_19990401_19990630.xlsx\",skiprows=[0])\n",
    "GPF_Full_19990701_19990930 = pd.read_excel(\"../RawData/SDC/GPF_Full_19990701_19990930.xlsx\",skiprows=[0])\n",
    "GPF_Full_19991001_19991231 = pd.read_excel(\"../RawData/SDC/GPF_Full_19991001_19991231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20000101_20000331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20000101_20000331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20000401_20000630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20000401_20000630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20000701_20000930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20000701_20000930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20001001_20001231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20001001_20001231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20010101_20010331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20010101_20010331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20010401_20010630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20010401_20010630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20010701_20010930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20010701_20010930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20011001_20011231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20011001_20011231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20020101_20020331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20020101_20020331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20020401_20020630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20020401_20020630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20020701_20020930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20020701_20020930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20021001_20021231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20021001_20021231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20030101_20030331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20030101_20030331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20030401_20030630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20030401_20030630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20030701_20030930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20030701_20030930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20031001_20031231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20031001_20031231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20040101_20040331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20040101_20040331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20040401_20040630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20040401_20040630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20040701_20040930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20040701_20040930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20041001_20041231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20041001_20041231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20050101_20050331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20050101_20050331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20050401_20050630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20050401_20050630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20050701_20050930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20050701_20050930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20051001_20051231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20051001_20051231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20060101_20060331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20060101_20060331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20060401_20060630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20060401_20060630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20060701_20060930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20060701_20060930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20061001_20061231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20061001_20061231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20070101_20070331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20070101_20070331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20070401_20070630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20070401_20070630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20070701_20070930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20070701_20070930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20071001_20071231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20071001_20071231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20080101_20080331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20080101_20080331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20080401_20080630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20080401_20080630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20080701_20080930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20080701_20080930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20081001_20081231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20081001_20081231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20090101_20090331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20090101_20090331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20090401_20090630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20090401_20090630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20090701_20090930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20090701_20090930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20091001_20091231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20091001_20091231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20100101_20100331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20100101_20100331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20100401_20100630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20100401_20100630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20100701_20100930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20100701_20100930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20101001_20101231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20101001_20101231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20110101_20110331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20110101_20110331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20110401_20110630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20110401_20110630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20110701_20110930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20110701_20110930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20111001_20111231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20111001_20111231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20120101_20120331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20120101_20120331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20120401_20120630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20120401_20120630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20120701_20120930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20120701_20120930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20121001_20121231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20121001_20121231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20130101_20130331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20130101_20130331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20130401_20130630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20130401_20130630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20130701_20130930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20130701_20130930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20131001_20131231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20131001_20131231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20140101_20140331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20140101_20140331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20140401_20140630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20140401_20140630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20140701_20140930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20140701_20140930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20141001_20141231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20141001_20141231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20150101_20150331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20150101_20150331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20150401_20150630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20150401_20150630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20150701_20150930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20150701_20150930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20151001_20151231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20151001_20151231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20160101_20160331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20160101_20160331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20160401_20160630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20160401_20160630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20160701_20160930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20160701_20160930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20161001_20161231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20161001_20161231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20170101_20170331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20170101_20170331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20170401_20170630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20170401_20170630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20170701_20170930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20170701_20170930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20171001_20171231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20171001_20171231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20180101_20180331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20180101_20180331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20180401_20180630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20180401_20180630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20180701_20180930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20180701_20180930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20181001_20181231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20181001_20181231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20190101_20190331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20190101_20190331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20190401_20190630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20190401_20190630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20190701_20190930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20190701_20190930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20191001_20191231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20191001_20191231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20200101_20200331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20200101_20200331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20200401_20200630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20200401_20200630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20200701_20200930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20200701_20200930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20201001_20201231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20201001_20201231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20210101_20210331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20210101_20210331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20210401_20210630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20210401_20210630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20210701_20210930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20210701_20210930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20211001_20211231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20211001_20211231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20220101_20220331 = pd.read_excel(\"../RawData/SDC/GPF_Full_20220101_20220331.xlsx\",skiprows=[0])\n",
    "GPF_Full_20220401_20220630 = pd.read_excel(\"../RawData/SDC/GPF_Full_20220401_20220630.xlsx\",skiprows=[0])\n",
    "GPF_Full_20220701_20220930 = pd.read_excel(\"../RawData/SDC/GPF_Full_20220701_20220930.xlsx\",skiprows=[0])\n",
    "GPF_Full_20221001_20221231 = pd.read_excel(\"../RawData/SDC/GPF_Full_20221001_20221231.xlsx\",skiprows=[0])\n",
    "GPF_Full_20230101_20230430 = pd.read_excel(\"../RawData/SDC/GPF_Full_20230101_20230430.xlsx\",skiprows=[0])\n",
    "GPF_Full_20230501_20230831 = pd.read_excel(\"../RawData/SDC/GPF_Full_20230501_20230831.xlsx\",skiprows=[0])\n",
    "GPF_Full_20230901_20231031 = pd.read_excel(\"../RawData/SDC/GPF_Full_20230901_20231031.xlsx\",skiprows=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf22ad1-a21f-40e9-9114-e7df3d7b07bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "GPF = pd.concat([\n",
    "    GPF_Full_19670101_19671231,\n",
    "    GPF_Full_19680101_19681231,\n",
    "    GPF_Full_19690101_19691231,\n",
    "    GPF_Full_19700101_19701231,\n",
    "    GPF_Full_19710101_19711231,\n",
    "    GPF_Full_19720101_19721231,\n",
    "    GPF_Full_19730101_19731231,\n",
    "    GPF_Full_19740101_19741231,\n",
    "    GPF_Full_19750101_19751231,\n",
    "    GPF_Full_19760101_19761231,\n",
    "    GPF_Full_19770101_19771231,\n",
    "    GPF_Full_19780101_19781231,\n",
    "    GPF_Full_19790101_19791231,\n",
    "    GPF_Full_19800101_19800331,\n",
    "    GPF_Full_19800401_19801231,\n",
    "    GPF_Full_19810101_19811231,\n",
    "    GPF_Full_19820101_19821231,\n",
    "    GPF_Full_19830101_19830630,\n",
    "    GPF_Full_19830701_19831231,\n",
    "    GPF_Full_19840101_19840630,\n",
    "    GPF_Full_19840701_19841231,\n",
    "    GPF_Full_19850101_19850630,\n",
    "    GPF_Full_19850701_19851231,\n",
    "    GPF_Full_19860101_19860630,\n",
    "    GPF_Full_19860701_19861231,\n",
    "    GPF_Full_19870101_19870630,\n",
    "    GPF_Full_19870701_19871231,\n",
    "    GPF_Full_19880101_19880630,\n",
    "    GPF_Full_19880701_19881231,\n",
    "    GPF_Full_19890101_19890630,\n",
    "    GPF_Full_19890701_19891231,\n",
    "    GPF_Full_19900101_19900331,\n",
    "    GPF_Full_19900401_19900831,\n",
    "    GPF_Full_19900901_19901231,\n",
    "    GPF_Full_19910101_19910430,\n",
    "    GPF_Full_19910501_19910831,\n",
    "    GPF_Full_19910901_19911031,\n",
    "    GPF_Full_19911101_19911231,\n",
    "    GPF_Full_19920101_19920430,\n",
    "    GPF_Full_19920501_19920831,\n",
    "    GPF_Full_19920901_19921231,\n",
    "    GPF_Full_19930101_19930430,\n",
    "    GPF_Full_19930501_19930630,\n",
    "    GPF_Full_19930701_19930930,\n",
    "    GPF_Full_19931001_19931231,\n",
    "    GPF_Full_19940101_19940331,\n",
    "    GPF_Full_19940401_19940630,\n",
    "    GPF_Full_19940701_19940930,\n",
    "    GPF_Full_19941001_19941231,\n",
    "    GPF_Full_19950101_19950331,\n",
    "    GPF_Full_19950401_19950630,\n",
    "    GPF_Full_19950701_19950930,\n",
    "    GPF_Full_19951001_19951231,\n",
    "    GPF_Full_19960101_19960430,\n",
    "    GPF_Full_19960501_19960831,\n",
    "    GPF_Full_19960901_19961231,\n",
    "    GPF_Full_19970101_19970331,\n",
    "    GPF_Full_19970401_19970630,\n",
    "    GPF_Full_19970701_19970930,\n",
    "    GPF_Full_19971001_19971231,\n",
    "    GPF_Full_19980101_19980331,\n",
    "    GPF_Full_19980401_19980630,\n",
    "    GPF_Full_19980701_19980930,\n",
    "    GPF_Full_19981001_19981231,\n",
    "    GPF_Full_19990101_19990331,\n",
    "    GPF_Full_19990401_19990630,\n",
    "    GPF_Full_19990701_19990930,\n",
    "    GPF_Full_19991001_19991231,\n",
    "    GPF_Full_20000101_20000331,\n",
    "    GPF_Full_20000401_20000630,\n",
    "    GPF_Full_20000701_20000930,\n",
    "    GPF_Full_20001001_20001231,\n",
    "    GPF_Full_20010101_20010331,\n",
    "    GPF_Full_20010401_20010630,\n",
    "    GPF_Full_20010701_20010930,\n",
    "    GPF_Full_20011001_20011231,\n",
    "    GPF_Full_20020101_20020331,\n",
    "    GPF_Full_20020401_20020630,\n",
    "    GPF_Full_20020701_20020930,\n",
    "    GPF_Full_20021001_20021231,\n",
    "    GPF_Full_20030101_20030331,\n",
    "    GPF_Full_20030401_20030630,\n",
    "    GPF_Full_20030701_20030930,\n",
    "    GPF_Full_20031001_20031231,\n",
    "    GPF_Full_20040101_20040331,\n",
    "    GPF_Full_20040401_20040630,\n",
    "    GPF_Full_20040701_20040930,\n",
    "    GPF_Full_20041001_20041231,\n",
    "    GPF_Full_20050101_20050331,\n",
    "    GPF_Full_20050401_20050630,\n",
    "    GPF_Full_20050701_20050930,\n",
    "    GPF_Full_20051001_20051231,\n",
    "    GPF_Full_20060101_20060331,\n",
    "    GPF_Full_20060401_20060630,\n",
    "    GPF_Full_20060701_20060930,\n",
    "    GPF_Full_20061001_20061231,\n",
    "    GPF_Full_20070101_20070331,\n",
    "    GPF_Full_20070401_20070630,\n",
    "    GPF_Full_20070701_20070930,\n",
    "    GPF_Full_20071001_20071231,\n",
    "    GPF_Full_20080101_20080331,\n",
    "    GPF_Full_20080401_20080630,\n",
    "    GPF_Full_20080701_20080930,\n",
    "    GPF_Full_20081001_20081231,\n",
    "    GPF_Full_20090101_20090331,\n",
    "    GPF_Full_20090401_20090630,\n",
    "    GPF_Full_20090701_20090930,\n",
    "    GPF_Full_20091001_20091231,\n",
    "    GPF_Full_20100101_20100331,\n",
    "    GPF_Full_20100401_20100630,\n",
    "    GPF_Full_20100701_20100930,\n",
    "    GPF_Full_20101001_20101231,\n",
    "    GPF_Full_20110101_20110331,\n",
    "    GPF_Full_20110401_20110630,\n",
    "    GPF_Full_20110701_20110930,\n",
    "    GPF_Full_20111001_20111231,\n",
    "    GPF_Full_20120101_20120331,\n",
    "    GPF_Full_20120401_20120630,\n",
    "    GPF_Full_20120701_20120930,\n",
    "    GPF_Full_20121001_20121231,\n",
    "    GPF_Full_20130101_20130331,\n",
    "    GPF_Full_20130401_20130630,\n",
    "    GPF_Full_20130701_20130930,\n",
    "    GPF_Full_20131001_20131231,\n",
    "    GPF_Full_20140101_20140331,\n",
    "    GPF_Full_20140401_20140630,\n",
    "    GPF_Full_20140701_20140930,\n",
    "    GPF_Full_20141001_20141231,\n",
    "    GPF_Full_20150101_20150331,\n",
    "    GPF_Full_20150401_20150630,\n",
    "    GPF_Full_20150701_20150930,\n",
    "    GPF_Full_20151001_20151231,\n",
    "    GPF_Full_20160101_20160331,\n",
    "    GPF_Full_20160401_20160630,\n",
    "    GPF_Full_20160701_20160930,\n",
    "    GPF_Full_20161001_20161231,\n",
    "    GPF_Full_20170101_20170331,\n",
    "    GPF_Full_20170401_20170630,\n",
    "    GPF_Full_20170701_20170930,\n",
    "    GPF_Full_20171001_20171231,\n",
    "    GPF_Full_20180101_20180331,\n",
    "    GPF_Full_20180401_20180630,\n",
    "    GPF_Full_20180701_20180930,\n",
    "    GPF_Full_20181001_20181231,\n",
    "    GPF_Full_20190101_20190331,\n",
    "    GPF_Full_20190401_20190630,\n",
    "    GPF_Full_20190701_20190930,\n",
    "    GPF_Full_20191001_20191231,\n",
    "    GPF_Full_20200101_20200331,\n",
    "    GPF_Full_20200401_20200630,\n",
    "    GPF_Full_20200701_20200930,\n",
    "    GPF_Full_20201001_20201231,\n",
    "    GPF_Full_20210101_20210331,\n",
    "    GPF_Full_20210401_20210630,\n",
    "    GPF_Full_20210701_20210930,\n",
    "    GPF_Full_20211001_20211231,\n",
    "    GPF_Full_20220101_20220331,\n",
    "    GPF_Full_20220401_20220630,\n",
    "    GPF_Full_20220701_20220930,\n",
    "    GPF_Full_20221001_20221231,\n",
    "    GPF_Full_20230101_20230430,\n",
    "    GPF_Full_20230501_20230831,\n",
    "    GPF_Full_20230901_20231031,\n",
    "    ]).copy()\n",
    "GPF = GPF.reset_index(drop=True)\n",
    "\n",
    "# Divide the \"Lead Manager\" field into many subfields\n",
    "# Note that \"Co-Managers\" have other manager information, which is not utilized here\n",
    "new_columns = GPF['Lead Manager'].str.split('\\n', expand=True)\n",
    "raw_name_GPF_colnames = ['raw_name_GPF_'+str(column) for column in new_columns.columns]\n",
    "new_columns.columns = raw_name_GPF_colnames\n",
    "GPF = pd.concat([GPF,new_columns],axis=1)\n",
    "# Modify mistakes in sale time\n",
    "threshold_date = pd.to_datetime('2050-01-01')\n",
    "GPF['Sale\\nDate'] = GPF['Sale\\nDate'].apply(lambda x: x - pd.DateOffset(years=100) if x > threshold_date else x)\n",
    "# Note that such errors do not exist for dated date\n",
    "# Add a year\n",
    "GPF['sale_year'] = None\n",
    "GPF['sale_year'] = pd.to_datetime(GPF['Sale\\nDate']).dt.year\n",
    "\n",
    "# strip all the columns of blanks\n",
    "columns = [item.strip() for item in GPF.columns]\n",
    "GPF.columns = columns\n",
    "# Choose columns to keep & reorder columns\n",
    "GPF = GPF[[\n",
    "    \"Amount\\n   of   \\n Issue  \\n($ mils)\",\n",
    "    \"Amount\\n   of   \\nMaturity\\n($ mils)\",\n",
    "    \"Bid\",\n",
    "    \"Bk \\n Elig\",\n",
    "    \"Bond\\nBuyer\\nUOP.1\",\n",
    "    \"Call\\nIssue\",\n",
    "    \"County\",\n",
    "    \"Coupon Maturity\",\n",
    "    \"Coupon Type.1\",\n",
    "    \"Coupon\\n   of\\nMaturity\",\n",
    "    'Credit Enhancer.1',\n",
    "    'Credit\\nEnhance\\n ment\\n Type',\n",
    "    'Credit\\nEnhance\\n ment\\n Type.1',\n",
    "    'Credit Enhancer.2',\n",
    "    \"Cusip\",\n",
    "    'Dated Date',\n",
    "    'Dated\\nDate',\n",
    "    \"Financial Advisor.1\",\n",
    "    \"Financial Advisor.2\",\n",
    "    \"Financial\\nAdvisor\\nDeal(Y/N)\",\n",
    "    \"Fitch\\nInsured\\nLong Term\\nRating\",\n",
    "    \"Fitch\\nInsured\\nShort Term\\nRating\",\n",
    "    \"General Use of Proceeds\",\n",
    "    \"Gross\\nSpread\",\n",
    "    \"Insured\\nAmount\",\n",
    "    \"Issuer Type\\nDescription\",\n",
    "    \"Issuer\",\n",
    "    \"Issuer\\nType\",\n",
    "    \"Lead Manager\",\n",
    "    \"Main Use of Proceeds\",\n",
    "    \"Maturity Amount\",\n",
    "    \"Maturity Date\",\n",
    "    \"Maturity\",\n",
    "    \"Maturity\\n  Year\",\n",
    "    \"Moody's\\nInsured\\nLong Term\\nRating\",\n",
    "    \"Moody's\\nInsured\\nShort Term\\nRating\",\n",
    "    \"Net\\nInterest\\n  Cost\",\n",
    "    \"Price/\\n Yield\\n  of\\nMaturity\",\n",
    "    'Refunding',\n",
    "    'Refunding\\n  Amount\\n ($ mils)',\n",
    "    'Dealno of\\nRefunded\\n  Issue',\n",
    "    'Refunded Issue',\n",
    "    'Ref-\\nType',\n",
    "    'Refd',\n",
    "    \"Sale\\nDate\",\n",
    "    \"Security\\n  Type\",\n",
    "    \"State\",\n",
    "    \"Taxable\\n Code\",\n",
    "    'True\\nInterest\\n  Cost',\n",
    "    \"Yield Amount\",\n",
    "    \"sale_year\",\n",
    "    ]+raw_name_GPF_colnames]\n",
    "GPF['County_raw'] = GPF['County']\n",
    "# Format county\n",
    "GPF['County'] = GPF['County'].str.upper()\n",
    "GPF['County'] = GPF['County'].replace(' COUNTY','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776ce055-e56c-482d-9b40-589056966e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPF = GPF.rename(columns={\n",
    "    \"Amount\\n   of   \\n Issue  \\n($ mils)\":\"amount\",\n",
    "    \"Amount\\n   of   \\nMaturity\\n($ mils)\":\"amount_by_maturity\",\n",
    "    \"Bk \\n Elig\":\"CB_Eligible\",\n",
    "    \"Bond\\nBuyer\\nUOP.1\":\"use_of_proceeds_BB\",\n",
    "    \"Call\\nIssue\":\"if_callable\",\n",
    "    \"Coupon Maturity\":\"TOM_coupon_rate\",\n",
    "    \"Coupon Type.1\":\"coupon_type\",\n",
    "    \"Coupon\\n   of\\nMaturity\":\"coupon_rate\",\n",
    "    'Credit Enhancer.1':'credit_enhancer_short',\n",
    "    'Credit Enhancer.2':'credit_enhancer_long',\n",
    "    \"Cusip\":\"cusip\",\n",
    "    \"Dated Date\":\"dated_date\",\n",
    "    \"Financial Advisor.1\":\"advisor_short\",\n",
    "    \"Financial Advisor.2\":\"advisor_long\",\n",
    "    \"Financial\\nAdvisor\\nDeal(Y/N)\":\"if_advisor\",\n",
    "    \"Fitch\\nInsured\\nLong Term\\nRating\":\"Fitch_ILTR\",\n",
    "    \"Fitch\\nInsured\\nShort Term\\nRating\":\"Fitch_ISTR\",\n",
    "    \"General Use of Proceeds\":\"use_of_proceeds_general\",\n",
    "    \"Gross\\nSpread\":\"gross_spread\",\n",
    "    \"Insured\\nAmount\":\"insured_amount\",\n",
    "    \"Issuer Type\\nDescription\":\"issuer_type_full\",\n",
    "    \"Issuer\\nType\":\"issuer_type\",\n",
    "    \"Lead Manager\":\"lead_manager\",\n",
    "    \"Main Use of Proceeds\":\"use_of_proceeds_main\",\n",
    "    \"Maturity Amount\":\"TOM_amount_by_maturity\",\n",
    "    \"Maturity Date\":\"TOM_maturity_date\",\n",
    "    \"Maturity\\n  Year\":\"maturity_date\", # Somehow this rather than \"Maturity\" is more often non-missing\n",
    "    \"Moody's\\nInsured\\nLong Term\\nRating\":\"Moodys_ILTR\",\n",
    "    \"Moody's\\nInsured\\nShort Term\\nRating\":\"Moodys_ISTR\",\n",
    "    \"Net\\nInterest\\n  Cost\":\"net_interest_cost\",\n",
    "    \"Price/\\n Yield\\n  of\\nMaturity\":\"price_or_yield\",\n",
    "    \"Sale\\nDate\":\"sale_date\",\n",
    "    \"Security\\n  Type\":\"security_type\",\n",
    "    \"Taxable\\n Code\":\"taxable_code\",\n",
    "    'True\\nInterest\\n  Cost':'true_interest_cost',\n",
    "    \"Yield Amount\":\"TOM_price_or_yield\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b96ef-68f6-496a-9875-76b25d1aace9",
   "metadata": {},
   "source": [
    "### 1.1.1 Import and merge with TBB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca6c9a-c3ea-4a6b-ac2f-31c47f83a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------#\n",
    "# Import data #\n",
    "#-------------#\n",
    "\n",
    "BBIssueDataAll = []\n",
    "for year in range(2008,2025):\n",
    "    with open('../RawData/BondBuyer/WebPages/BBIssueData'+str(year)+'.pkl', 'rb') as file:\n",
    "        BBIssueData = pickle.load(file)\n",
    "        BBIssueData = pd.DataFrame(BBIssueData)\n",
    "        BBIssueDataAll = BBIssueDataAll+[BBIssueData]\n",
    "BBIssueDataAll = pd.concat(BBIssueDataAll).reset_index(drop=True)\n",
    "\n",
    "# When a bond issue has more than one effective rate, NIC, or TIC, such an issue is recorded as multiple data points in \n",
    "# GPF. Hence, reformat and explode data\n",
    "BBIssueDataAll['len_CaseEffRate_amounts'] = BBIssueDataAll['CaseEffRate_amounts'].apply(lambda x: len(x) if x is not None else 0)\n",
    "BBIssueDataAll_multi = BBIssueDataAll[BBIssueDataAll['len_CaseEffRate_amounts']>1].copy()\n",
    "BBIssueDataAll_single = BBIssueDataAll[BBIssueDataAll['len_CaseEffRate_amounts']<=1].copy()\n",
    "BBIssueDataAll_multi = BBIssueDataAll_multi.rename(columns={\n",
    "    'CaseEffRate_amounts':'a',\n",
    "    'CaseEffRate_purchasers':'b',\n",
    "    'CaseEffRate_coupon_rates':'c',\n",
    "    'CaseEffRate_purchase_price_minus_pars':'d',\n",
    "    'CaseEffRate_effective_rates':'e',\n",
    "    })\n",
    "BBIssueDataAll_multi = BBIssueDataAll_multi.explode(list('abcde'))\n",
    "BBIssueDataAll_single['CaseEffRate_amounts'] = \\\n",
    "    BBIssueDataAll_single['CaseEffRate_amounts'].apply(lambda x: None if x is None else x[0])\n",
    "BBIssueDataAll_single['CaseEffRate_purchasers'] = \\\n",
    "    BBIssueDataAll_single['CaseEffRate_purchasers'].apply(lambda x: None if x is None else x[0])\n",
    "BBIssueDataAll_single['CaseEffRate_coupon_rates'] = \\\n",
    "    BBIssueDataAll_single['CaseEffRate_coupon_rates'].apply(lambda x: None if x is None else x[0])\n",
    "BBIssueDataAll_single['CaseEffRate_purchase_price_minus_pars'] = \\\n",
    "    BBIssueDataAll_single['CaseEffRate_purchase_price_minus_pars'].apply(lambda x: None if x is None else x[0])\n",
    "BBIssueDataAll_single['CaseEffRate_effective_rates'] = \\\n",
    "    BBIssueDataAll_single['CaseEffRate_effective_rates'].apply(lambda x: None if x is None else x[0])\n",
    "BBIssueDataAll = pd.concat([BBIssueDataAll_single,BBIssueDataAll_multi])\n",
    "\n",
    "BBIssueDataAll['sale_date'] = pd.to_datetime(BBIssueDataAll['sale_date'])\n",
    "text = '</p> <table class=\"tblExcelHide\" id=\"tblRCSalesResult0\"><tbody></tbody></table> <p class=\"FootRCSalesResultcls\" '+\\\n",
    "    'id=\"divFootRCSalesResult0\" style=\"display:none\"></p> <p class=\"RCSalesResultcls\" id=\"divRCSalesResult1\">Oct 10, 2023'\n",
    "BBIssueDataAll.loc[BBIssueDataAll['dated_date']==text,'dated_date'] = 'Oct 10, 2023'\n",
    "# Handle an error case\n",
    "BBIssueDataAll['dated_date'] = pd.to_datetime(BBIssueDataAll['dated_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eaa7b-703e-4d07-bc88-7398898ff312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------#\n",
    "# Match and put data into GPF #\n",
    "#-----------------------------#\n",
    "\n",
    "# Go over each issue in BBIssueDataAll,\n",
    "# (1) Find all issues in SDC with the same state, sales date, and dated date.\n",
    "# Allow margin of error of seven days\n",
    "# (2) Find the issue with the smallest difference in amount.\n",
    "# (3) Check if amount is within margin of error (e.g., 0.05 million).\n",
    "# (4) If still not unique, check similarity in terms of issuer name.\n",
    "\n",
    "BBIssueDataAll['GPF_no_match'] = None\n",
    "BBIssueDataAll['GPF_multiple_match'] = None\n",
    "\n",
    "GPF['CaseEffRate_amounts'] = None\n",
    "GPF['CaseEffRate_purchasers'] = None\n",
    "GPF['CaseEffRate_coupon_rates'] = None\n",
    "GPF['CaseEffRate_purchase_price_minus_pars'] = None\n",
    "GPF['CaseEffRate_effective_rates'] = None\n",
    "GPF['CaseEffRate_lines_other_bidders'] = None\n",
    "\n",
    "GPF['CaseTIC_purchaser'] = None\n",
    "GPF['CaseTIC_purchase_price'] = None\n",
    "GPF['CaseTIC_TIC'] = None\n",
    "GPF['CaseTIC_lines_other_bidders'] = None\n",
    "\n",
    "GPF['CaseNIC_purchaser'] = None\n",
    "GPF['CaseNIC_purchase_price'] = None\n",
    "GPF['CaseNIC_NIC'] = None\n",
    "GPF['CaseNIC_lines_other_bidders'] = None\n",
    "\n",
    "GPF_pre2008 = GPF[GPF['sale_year']<2008]\n",
    "GPF_post2008 = GPF[GPF['sale_year']>=2008]\n",
    "data_for_parallel = []\n",
    "for year in range(2008,2024):\n",
    "    data_for_parallel = data_for_parallel+\\\n",
    "        [(GPF[GPF['sale_year']==year],BBIssueDataAll[BBIssueDataAll['notice_year']==year],state_abbreviations)]\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_Match_TBB_GPF)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_Match_TBB_GPF\n",
    "importlib.reload(FUN_0A_Match_TBB_GPF)\n",
    "from FUN_0A_Match_TBB_GPF import FUN_0A_Match_TBB_GPF\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        results_from_parallel = p.starmap(FUN_0A_Match_TBB_GPF, data_for_parallel)\n",
    "\n",
    "# Collect results\n",
    "GPF_parts = []\n",
    "BBIssueDataAll_parts = []\n",
    "for i in range(0,len(results_from_parallel)):\n",
    "    GPF_parts = GPF_parts+[results_from_parallel[i][0]]\n",
    "    BBIssueDataAll_parts = BBIssueDataAll_parts+[results_from_parallel[i][1]]\n",
    "GPF = pd.concat(GPF_parts+[GPF_pre2008])\n",
    "BBIssueDataAll = pd.concat(BBIssueDataAll_parts)\n",
    "\n",
    "# When TIC/NIC is provided in TBB, almost all the time it is available in GPF\n",
    "# GPF[['net_interest_cost','true_interest_cost','CaseTIC_TIC']][~pd.isnull(GPF['CaseTIC_TIC'])].sample(20)\n",
    "# GPF[['net_interest_cost','true_interest_cost','CaseNIC_NIC']][~pd.isnull(GPF['CaseNIC_NIC'])].sample(20)\n",
    "\n",
    "# On the contrary, TIC/NIC is usually missing when effective rate (which is TIC, but for very short term bonds) is provided\n",
    "# GPF[['net_interest_cost','true_interest_cost','CaseEffRate_effective_rates']]\\\n",
    "# [~pd.isnull(GPF['CaseEffRate_effective_rates'])].sample(20)\n",
    "\n",
    "# Put effective rates from TBB to TIC\n",
    "GPF.loc[~pd.isnull(GPF['CaseEffRate_effective_rates']),'CaseEffRate_effective_rates'] = \\\n",
    "    GPF[~pd.isnull(GPF['CaseEffRate_effective_rates'])]['CaseEffRate_effective_rates'].str.replace('%','')\n",
    "GPF.loc[~pd.isnull(GPF['CaseEffRate_effective_rates']),'CaseEffRate_effective_rates'] = \\\n",
    "    GPF[~pd.isnull(GPF['CaseEffRate_effective_rates'])]['CaseEffRate_effective_rates'].astype(float)\n",
    "GPF['if_true_interest_cost_fromTBB'] = False\n",
    "GPF.loc[pd.isnull(GPF['true_interest_cost'])&(~pd.isnull(GPF['CaseEffRate_effective_rates'])),'if_true_interest_cost_fromTBB'] = True\n",
    "GPF.loc[pd.isnull(GPF['true_interest_cost'])&(~pd.isnull(GPF['CaseEffRate_effective_rates'])),'true_interest_cost'] = \\\n",
    "    GPF[pd.isnull(GPF['true_interest_cost'])&(~pd.isnull(GPF['CaseEffRate_effective_rates']))]['CaseEffRate_effective_rates']\n",
    "\n",
    "# Count number of bidders\n",
    "GPF['TBB_n_bidders'] = None\n",
    "for idx,row in GPF.iterrows():\n",
    "    if str(row['CaseEffRate_lines_other_bidders'])!='nan':\n",
    "        GPF.at[idx,'TBB_n_bidders'] = len(eval(row['CaseEffRate_lines_other_bidders']))\n",
    "    if str(row['CaseTIC_lines_other_bidders'])!='nan':\n",
    "        GPF.at[idx,'TBB_n_bidders'] = len(eval(row['CaseTIC_lines_other_bidders']))\n",
    "    if str(row['CaseNIC_lines_other_bidders'])!='nan':\n",
    "        GPF.at[idx,'TBB_n_bidders'] = len(eval(row['CaseNIC_lines_other_bidders']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465714c9-2c37-4b95-845c-b83e279c9622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a41a8e-aa4f-4c42-94c5-1129071f81d6",
   "metadata": {},
   "source": [
    "## 1.2. Calculate yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f88806-6a0b-4ad2-9bd7-038e7995e297",
   "metadata": {},
   "source": [
    "### 1.2.1 Pre-process data for yield calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eaf23c-4d40-4e3e-b8f5-f4349d6a2a6b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Note that coupon type is tricky: I do not have a Thompson-researched version of coupon type, so it is tricky if I want to have a bond-level variable \"coupon type\". Therefore, I retain the original \"coupon_type\" variable, and determine if an issue is \"all\" fixed-rate/zero-coupon rate or \"any other type\" variable by checking that variable. I will not calculate yield if a bond issue has any bond with \"coupon_type\" being something other than fixed-rate, so, yield of floating-rate or zero-coupon bonds are not calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63534ac8-b5c6-434c-9d0c-654a18607bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Initialize variables #\n",
    "########################\n",
    "\n",
    "# Weighted average of yield across all maturities\n",
    "GPF['yield_by_maturity_list'] = None\n",
    "GPF['avg_yield'] = None\n",
    "\n",
    "# Weighted average maturity\n",
    "GPF['maturity_by_maturity_list'] = None\n",
    "GPF['avg_maturity'] = None\n",
    "\n",
    "# Amount by maturity\n",
    "GPF['amount_by_maturity_list'] = None\n",
    "\n",
    "# Whether it can be determined if the record is price or yield\n",
    "GPF['IF_price_or_yield_determined'] = None\n",
    "\n",
    "# Whether the number of tranches match in the coupon versus in the price/yield versus in the maturity field\n",
    "GPF['IF_n_tranches_not_match'] = None\n",
    "\n",
    "# Whether the bond has any coupon type other than \"fixed-rate\"\n",
    "GPF['IF_irregular_coupon_type'] = False\n",
    "\n",
    "GPF = GPF.reset_index(drop=True)\n",
    "\n",
    "############################\n",
    "# Handle exceptional cases #\n",
    "############################\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "\n",
    "    # (1) Handle zero-coupon cases. If \"Zero Coupon\" is one of the coupon types, there are other coupon types, and the number of\n",
    "    # coupon rates is exactly \"total number of maturities\" minus \"number of zero coupon bonds\", fill in the places of zero coupon\n",
    "    # bonds to have a coupon rate of 0. Note that I do not do this for the Thompson-researched version of data, as the \"coupon\n",
    "    # type\" variable is not available there\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "    \n",
    "        coupon_rate_filled = []\n",
    "        zero_coupon_idxes = []\n",
    "        non_zero_coupon_idxes = []\n",
    "        \n",
    "        coupon_type_original = str(row['coupon_type'])\n",
    "        coupon_type = str(row['coupon_type']).split('\\n')\n",
    "        coupon_rate = row['coupon_rate']\n",
    "    \n",
    "        IF_has_coupon_rate = \\\n",
    "            row['coupon_rate']!=None and \\\n",
    "            str(row['coupon_rate'])!='nan' \\\n",
    "            and str(row['coupon_rate'])!='None'\n",
    "        \n",
    "        # Handle cases where just one bond\n",
    "        if coupon_type_original==\"Zero Coupon\":\n",
    "            GPF.at[idx,'coupon_rate'] = 0\n",
    "            \n",
    "        # When there are multiple bonds. Note that no need to handle if there is just one bond and it is not zero coupon\n",
    "        elif len(coupon_type)>1 :\n",
    "            # Do not handle if no zero-coupon bond\n",
    "            if IF_has_coupon_rate and \"Zero Coupon\" in coupon_type:\n",
    "                zero_coupon_idxes = [index for index,item in enumerate(coupon_type) if item==\"Zero Coupon\"]\n",
    "                non_zero_coupon_idxes = [index for index,item in enumerate(coupon_type) if item!=\"Zero Coupon\"]\n",
    "                coupon_rate = str(coupon_rate).split('\\n')\n",
    "                if len(coupon_type)==len(coupon_rate)+len(zero_coupon_idxes):\n",
    "                    coupon_rate_filled = [' ']*len(coupon_type)\n",
    "                    physical_idx = 0\n",
    "                    for sub_idx in non_zero_coupon_idxes:\n",
    "                        coupon_rate_filled[sub_idx] = coupon_rate[physical_idx]\n",
    "                        physical_idx = physical_idx+1\n",
    "                    for sub_idx in zero_coupon_idxes:\n",
    "                        coupon_rate_filled[sub_idx] = \"0\"\n",
    "                    coupon_rate_new = coupon_rate_filled[0]\n",
    "                    for item in coupon_rate_filled[1:]:\n",
    "                        coupon_rate_new = coupon_rate_new+\"\\n\"+item\n",
    "                    GPF.at[idx,'coupon_rate'] = coupon_rate_new\n",
    "\n",
    "    # (2) Assume that the coupon rate applies to all maturities if there is one coupon rate but multiple tranches\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if row['coupon_rate']!=None and str(row['coupon_rate'])!='nan':\n",
    "            if '\\n' in str(row['maturity_date']):\n",
    "                if '\\n' not in str(row['coupon_rate']):\n",
    "                    coupon_rate = ''\n",
    "                    for tranch in range(0,row['maturity_date'].count('\\n')):\n",
    "                        coupon_rate = coupon_rate+str(row['coupon_rate'])+'\\n'\n",
    "                    coupon_rate = coupon_rate+str(row['coupon_rate'])+'\\n'\n",
    "                    GPF.loc[idx,'coupon_rate'] = coupon_rate\n",
    "    \n",
    "    # (3) Assume that the price/yield applies to all maturities if there is one price/yield rate but multiple tranches.\n",
    "    # Make this edit only if the maturities are all identical. Otherwise, it is more likely a data error and do not impute.\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if row['price_or_yield']!=None and str(row['price_or_yield'])!='nan':\n",
    "            if '\\n' in str(row['maturity_date']):\n",
    "                maturity_date = row['maturity_date'].split('\\n')\n",
    "                if_same = all(element == maturity_date[0] for element in maturity_date)\n",
    "                if if_same and ('\\n' not in str(row['price_or_yield'])):\n",
    "                    price_or_yield = ''\n",
    "                    for tranch in range(0,row['maturity_date'].count('\\n')):\n",
    "                        price_or_yield = price_or_yield+str(row['price_or_yield'])+'\\n'\n",
    "                    price_or_yield = price_or_yield+str(row['price_or_yield'])+'\\n'\n",
    "                    GPF.loc[idx,'price_or_yield'] = price_or_yield\n",
    "    \n",
    "    # (4) Take the value of \"Maturity\" to populate \"Maturity\\n  Year\" (i.e., \"maturity_date\") if the latter is missing\n",
    "    for idx,row in GPF.iterrows():\n",
    "        IF_has_maturity_date = \\\n",
    "            row['maturity_date']!=None and \\\n",
    "            str(row['maturity_date'])!='nan' and \\\n",
    "            str(row['maturity_date'])!='None' and \\\n",
    "            'None' not in str(row['maturity_date']) \n",
    "        if not IF_has_maturity_date:\n",
    "            GPF.at[idx,'maturity_date'] = row['Maturity']\n",
    "    \n",
    "    # (5) Remove if beginning or end of field is '\\n'\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['price_or_yield'],str):\n",
    "            if row['price_or_yield'][:1]=='\\n':\n",
    "                GPF.loc[idx,'price_or_yield'] = row['price_or_yield'][1:]\n",
    "            if row['price_or_yield'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'price_or_yield'] = row['price_or_yield'][:-1]\n",
    "        if isinstance(row['maturity_date'],str):\n",
    "            if row['maturity_date'][:1]=='\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][1:]\n",
    "            if row['maturity_date'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][:-1]\n",
    "        if isinstance(row['coupon_rate'],str):\n",
    "            if row['coupon_rate'][:1]=='\\n':\n",
    "                GPF.loc[idx,'coupon_rate'] = row['coupon_rate'][1:]\n",
    "            if row['coupon_rate'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'coupon_rate'] = row['coupon_rate'][:-1]\n",
    "        if isinstance(row['amount_by_maturity'],str):\n",
    "            if row['amount_by_maturity'][:1]=='\\n':\n",
    "                GPF.loc[idx,'amount_by_maturity'] = row['amount_by_maturity'][1:]\n",
    "            if row['amount_by_maturity'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'amount_by_maturity'] = row['amount_by_maturity'][:-1]\n",
    "        if isinstance(row['TOM_price_or_yield'],str):\n",
    "            if row['TOM_price_or_yield'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_price_or_yield'] = row['TOM_price_or_yield'][1:]\n",
    "            if row['TOM_price_or_yield'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_price_or_yield'] = row['TOM_price_or_yield'][:-1]\n",
    "        if isinstance(row['TOM_maturity_date'],str):\n",
    "            if row['TOM_maturity_date'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_maturity_date'] = row['TOM_maturity_date'][1:]\n",
    "            if row['TOM_maturity_date'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_maturity_date'] = row['TOM_maturity_date'][:-1]\n",
    "        if isinstance(row['TOM_coupon_rate'],str):\n",
    "            if row['TOM_coupon_rate'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_coupon_rate'] = row['TOM_coupon_rate'][1:]\n",
    "            if row['TOM_coupon_rate'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_coupon_rate'] = row['TOM_coupon_rate'][:-1]\n",
    "        if isinstance(row['TOM_amount_by_maturity'],str):\n",
    "            if row['TOM_amount_by_maturity'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_amount_by_maturity'] = row['TOM_amount_by_maturity'][1:]\n",
    "            if row['TOM_amount_by_maturity'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_amount_by_maturity'] = row['TOM_amount_by_maturity'][:-1]\n",
    "    \n",
    "    # (6) Remove if beginning or end or middle of field \"maturity_date\" is 'None'\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['maturity_date'],str):\n",
    "            if row['maturity_date'][:5]=='None\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][5:]\n",
    "            if row['maturity_date'][-5:]=='\\nNone':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][:-5]\n",
    "            GPF.at[idx,'maturity_date'] = GPF.at[idx,'maturity_date'].replace('None\\n','')\n",
    "    \n",
    "    # (7) After the prior step, there are cases where \"\\n\" is not in \"maturity_date\" and \"maturity_date\" is a string. To avoid\n",
    "    # incompatibility, convert type\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['maturity_date'],str) and '\\n' not in row['maturity_date'] and\\\n",
    "            row['maturity_date']!=None and str(row['maturity_date'])!='nan' and str(row['maturity_date'])!='None':\n",
    "            GPF.at[idx,'maturity_date'] = datetime.strptime(GPF.at[idx,'maturity_date'],\"%m/%d/%y\")\n",
    "    \n",
    "    # (8) Drop if two or three \"\\n\" come adjacent\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if '\\n\\n' in str(row['price_or_yield']):\n",
    "            GPF.at[idx,'price_or_yield'] = GPF.at[idx,'price_or_yield'].replace('\\n\\n','\\n')\n",
    "        if '\\n\\n\\n' in str(row['price_or_yield']):\n",
    "            GPF.at[idx,'price_or_yield'] = GPF.at[idx,'price_or_yield'].replace('\\n\\n\\n','\\n')\n",
    "        if '\\n\\n' in str(row['coupon_rate']):\n",
    "            GPF.at[idx,'coupon_rate'] = GPF.at[idx,'coupon_rate'].replace('\\n\\n','\\n')\n",
    "        if '\\n\\n\\n' in str(row['coupon_rate']):\n",
    "            GPF.at[idx,'coupon_rate'] = GPF.at[idx,'coupon_rate'].replace('\\n\\n\\n','\\n')\n",
    "\n",
    "    # (9) Put value of data from the non-Thompson-researched version to the Thompson-researched version, if the latter is missing.\n",
    "    # For consistency, make this change for all related fields, or do not do so at all\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if \\\n",
    "            (row['TOM_maturity_date']==None or \\\n",
    "            str(row['TOM_maturity_date'])=='nan' or \\\n",
    "            str(row['TOM_maturity_date'])=='None' or \\\n",
    "            'None' in str(row['TOM_maturity_date'])) \\\n",
    "            and \\\n",
    "            (row['TOM_coupon_rate']==None or \\\n",
    "            str(row['TOM_coupon_rate'])=='nan' \\\n",
    "            or str(row['TOM_coupon_rate'])=='None')\\\n",
    "            and \\\n",
    "            (row['TOM_price_or_yield']==None or \\\n",
    "            str(row['TOM_price_or_yield'])=='nan' \\\n",
    "            or str(row['TOM_price_or_yield'])=='None') \\\n",
    "            and \\\n",
    "            (row['TOM_amount_by_maturity']==None or \\\n",
    "            str(row['TOM_amount_by_maturity'])=='nan' or \\\n",
    "            str(row['TOM_amount_by_maturity'])=='None' or \\\n",
    "            'None' in str(row['TOM_amount_by_maturity'])):\n",
    "            GPF.at[idx,'TOM_maturity_date'] = row['maturity_date']\n",
    "            GPF.at[idx,'TOM_coupon_rate'] = row['coupon_rate']\n",
    "            GPF.at[idx,'TOM_price_or_yield'] = row['price_or_yield']\n",
    "            GPF.at[idx,'TOM_amount_by_maturity'] = row['amount_by_maturity']\n",
    "\n",
    "    # (10) Mark out if a bond issue has a bond that is not fixed rate or zero coupon. It is extremely uncommon for an issue to have both\n",
    "    # fixed rate/zero coupon and irregular (variable rate) types of bonds. Therefore, I do not consider cases where one issue has both\n",
    "    for idx,row in GPF.iterrows():\n",
    "        coupon_type = row['coupon_type'].split('\\n')\n",
    "        coupon_type = [item for item in coupon_type if item!='Fixed Rate']\n",
    "        coupon_type = [item for item in coupon_type if item!='Zero Coupon']\n",
    "        if len(coupon_type)>0:\n",
    "            GPF.at[idx,'IF_irregular_coupon_type'] = True\n",
    "\n",
    "    # (11) In the prior steps, I put non-Thompson data to those fields if Thompson-researched data are missing. In addition, I use \n",
    "    # Thompson-researched data completely for any issues post 2003\n",
    "    if row['sale_year']>=2003:\n",
    "        GPF.at[idx,'maturity_date'] = row['TOM_maturity_date']\n",
    "        GPF.at[idx,'coupon_rate'] = row['TOM_coupon_rate']\n",
    "        GPF.at[idx,'price_or_yield'] = row['TOM_price_or_yield']\n",
    "        GPF.at[idx,'amount_by_maturity'] = row['TOM_amount_by_maturity']\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06454208-ce6d-4b6e-81ab-c083b0c793ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns that describe data availablility for a particular bond issue\n",
    "def proc_list(GPF):\n",
    "    \n",
    "    GPF = GPF.copy()\n",
    "    \n",
    "    GPF['IF_has_maturity_date'] = None\n",
    "    GPF['IF_has_coupon_rate'] = None\n",
    "    GPF['IF_has_price_or_yield'] = None\n",
    "    GPF['IF_has_amount_by_maturity'] = None\n",
    "    GPF['IF_has_tic'] = None\n",
    "    GPF['IF_has_nic'] = None\n",
    "    GPF['IF_has_gross_spread'] = None\n",
    "    \n",
    "    GPF['N_coupon_rate'] = None\n",
    "    GPF['N_price_or_yield'] = None\n",
    "    GPF['N_maturity_date'] = None\n",
    "    GPF['N_amount'] = None\n",
    "    \n",
    "    GPF['IF_num_bonds_all_consistent'] = None\n",
    "    GPF['IF_num_bonds_yield_mat_amt_consistent'] = None\n",
    "    GPF['IF_num_bonds_mat_amt_consistent'] = None\n",
    "    \n",
    "    # Handle case by case of each variable being missing, and within each case allow for multiple maturities\n",
    "    for idx,row in GPF.iterrows():\n",
    "    \n",
    "        # Do nothing if the issue contains bonds with irregular type of coupon payments\n",
    "        if row['IF_irregular_coupon_type']==True:\n",
    "            continue\n",
    "    \n",
    "        # Do nothing if dated date is missing\n",
    "        if str(row['dated_date'])=='nan' or str(row['dated_date'])=='NaT':\n",
    "            continue\n",
    "    \n",
    "        # Initialize variables\n",
    "        IF_has_maturity_date = False\n",
    "        IF_has_coupon_rate = False\n",
    "        IF_has_price_or_yield = False\n",
    "        IF_has_amount_by_maturity = False\n",
    "        IF_has_tic = False\n",
    "        IF_has_gross_spread = False\n",
    "        \n",
    "        # Whether certain fields exist\n",
    "        IF_has_coupon_rate = \\\n",
    "            row['coupon_rate']!=None and \\\n",
    "            str(row['coupon_rate'])!='nan' \\\n",
    "            and str(row['coupon_rate'])!='None'\n",
    "        IF_has_maturity_date = \\\n",
    "            row['maturity_date']!=None and \\\n",
    "            str(row['maturity_date'])!='nan' and \\\n",
    "            str(row['maturity_date'])!='None' and \\\n",
    "            'None' not in str(row['maturity_date']) \n",
    "        IF_has_price_or_yield = \\\n",
    "            row['price_or_yield']!=None and \\\n",
    "            str(row['price_or_yield'])!='nan' \\\n",
    "            and str(row['price_or_yield'])!='None'\n",
    "        IF_has_amount_by_maturity = \\\n",
    "            row['amount_by_maturity']!=None and \\\n",
    "            str(row['amount_by_maturity'])!='nan' and \\\n",
    "            str(row['amount_by_maturity'])!='None' and \\\n",
    "            'None' not in str(row['amount_by_maturity']) \n",
    "        IF_has_tic = \\\n",
    "            row['true_interest_cost']!=None and \\\n",
    "            str(row['true_interest_cost'])!='nan' and \\\n",
    "            str(row['true_interest_cost'])!='None' and \\\n",
    "            'None' not in str(row['true_interest_cost']) and \\\n",
    "            (isinstance(row['true_interest_cost'],int)|isinstance(row['true_interest_cost'],float))\n",
    "        IF_has_nic = \\\n",
    "            row['net_interest_cost']!=None and \\\n",
    "            str(row['net_interest_cost'])!='nan' and \\\n",
    "            str(row['net_interest_cost'])!='None' and \\\n",
    "            'None' not in str(row['net_interest_cost']) and \\\n",
    "            (isinstance(row['net_interest_cost'],int)|isinstance(row['net_interest_cost'],float))\n",
    "        IF_has_gross_spread = \\\n",
    "            row['gross_spread']!=None and \\\n",
    "            str(row['gross_spread'])!='nan' and \\\n",
    "            str(row['gross_spread'])!='None' and \\\n",
    "            'None' not in str(row['gross_spread']) and \\\n",
    "            (isinstance(row['gross_spread'],int)|isinstance(row['gross_spread'],float))\n",
    "\n",
    "        # Number of entries in certain fields\n",
    "        N_coupon_rate = str(row['coupon_rate']).count('\\n')+1\n",
    "        N_price_or_yield = str(row['price_or_yield']).count('\\n')+1\n",
    "        N_maturity_date = str(row['maturity_date']).count('\\n')+1\n",
    "        N_amount = str(row['amount_by_maturity']).count('\\n')+1\n",
    "        IF_num_bonds_all_consistent = \\\n",
    "            (N_coupon_rate==N_price_or_yield) and \\\n",
    "            (N_coupon_rate==N_maturity_date) and \\\n",
    "            (N_coupon_rate==N_amount)\n",
    "        # The following indicator can be applied when coupon is not available\n",
    "        IF_num_bonds_yield_mat_amt_consistent = \\\n",
    "            (N_price_or_yield==N_maturity_date) and \\\n",
    "            (N_price_or_yield==N_amount)\n",
    "        # The following indicator can be applied when coupon and yield/price is not available\n",
    "        IF_num_bonds_mat_amt_consistent = \\\n",
    "            (N_maturity_date==N_amount)\n",
    "        \n",
    "        GPF.at[idx,'IF_has_maturity_date'] = IF_has_maturity_date\n",
    "        GPF.at[idx,'IF_has_coupon_rate'] = IF_has_coupon_rate\n",
    "        GPF.at[idx,'IF_has_price_or_yield'] = IF_has_price_or_yield\n",
    "        GPF.at[idx,'IF_has_amount_by_maturity'] = IF_has_amount_by_maturity\n",
    "        GPF.at[idx,'IF_has_tic'] = IF_has_tic\n",
    "        GPF.at[idx,'IF_has_nic'] = IF_has_nic\n",
    "        GPF.at[idx,'IF_has_gross_spread'] = IF_has_gross_spread\n",
    "        \n",
    "        GPF.at[idx,'N_coupon_rate'] = N_coupon_rate\n",
    "        GPF.at[idx,'N_price_or_yield'] = N_price_or_yield\n",
    "        GPF.at[idx,'N_maturity_date'] = N_maturity_date\n",
    "        GPF.at[idx,'N_amount'] = N_amount\n",
    "        \n",
    "        GPF.at[idx,'IF_num_bonds_all_consistent'] = IF_num_bonds_all_consistent\n",
    "        GPF.at[idx,'IF_num_bonds_yield_mat_amt_consistent'] = IF_num_bonds_yield_mat_amt_consistent\n",
    "        GPF.at[idx,'IF_num_bonds_mat_amt_consistent'] = IF_num_bonds_mat_amt_consistent\n",
    "\n",
    "    return GPF\n",
    "\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8710036-2ab4-4774-84dc-c24f8205263d",
   "metadata": {},
   "source": [
    "### 1.2.2 Calculate yield at issuance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a47cf-d356-4e2f-b3a9-ee08f51d5046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "\n",
    "    # Handle case by case of each variable being missing, and within each case allow for multiple maturities\n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Do nothing if the issue contains bonds with irregular type of coupon payments\n",
    "        if row['IF_irregular_coupon_type']==True:\n",
    "            continue\n",
    "\n",
    "        # Do nothing if dated date is missing\n",
    "        if str(row['dated_date'])=='nan' or str(row['dated_date'])=='NaT':\n",
    "            continue\n",
    "\n",
    "        # Extract vallues that describe data availablility for a particular bond issue\n",
    "\n",
    "        IF_has_maturity_date = row['IF_has_maturity_date']\n",
    "        IF_has_coupon_rate = row['IF_has_coupon_rate']\n",
    "        IF_has_price_or_yield = row['IF_has_price_or_yield']\n",
    "        IF_has_amount_by_maturity = row['IF_has_amount_by_maturity']\n",
    "        IF_has_tic = row['IF_has_tic']\n",
    "        \n",
    "        N_coupon_rate = row['N_coupon_rate']\n",
    "        N_price_or_yield = row['N_price_or_yield']\n",
    "        N_maturity_date = row['N_maturity_date']\n",
    "        N_amount = row['N_amount']\n",
    "\n",
    "        IF_num_bonds_all_consistent = row['IF_num_bonds_all_consistent']\n",
    "        IF_num_bonds_yield_mat_amt_consistent = row['IF_num_bonds_yield_mat_amt_consistent']\n",
    "        IF_num_bonds_mat_amt_consistent = row['IF_num_bonds_mat_amt_consistent']\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Case 1 #\n",
    "        ##########\n",
    "\n",
    "        # Case 1: \"coupon_rate\",\"maturity_date\",\"price_or_yield\" are all available\n",
    "        if IF_has_maturity_date and IF_has_coupon_rate and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and IF_num_bonds_all_consistent:\n",
    "    \n",
    "            # Case 1A: If single maturity\n",
    "            if N_maturity_date==1 :\n",
    "                maturity = (row['maturity_date']-row['dated_date']).days\n",
    "                GPF.loc[idx,'avg_maturity'] = maturity\n",
    "                GPF.loc[idx,'maturity_by_maturity_list'] = [[maturity]]\n",
    "                GPF.loc[idx,'amount_by_maturity_list'] = [[row['amount_by_maturity']]]\n",
    "                # Assume that if a number is more than 80 and less than 120, it is issuing price. If less than 20, it is issuing \n",
    "                # yield. Otherwise, undetermined\n",
    "                if float(row['price_or_yield'])<20:\n",
    "                    GPF.loc[idx,'avg_yield'] = row['price_or_yield']/100\n",
    "                    GPF.loc[idx,'yield_by_maturity_list'] = [[GPF.loc[idx,'avg_yield']]]\n",
    "                elif float(row['price_or_yield'])>80 and float(row['price_or_yield'])<120:\n",
    "                    # Number of coupons to be paid\n",
    "                    n_coupon = round(maturity/182)\n",
    "                    GPF.loc[idx,'avg_yield'] = \\\n",
    "                        (1+npf.irr([-row['price_or_yield']]+[row['coupon_rate']/2]*(n_coupon-1)+[100+row['coupon_rate']/2]))\\\n",
    "                        **2-1\n",
    "                    GPF.loc[idx,'yield_by_maturity_list'] = [[GPF.loc[idx,'avg_yield']]]\n",
    "                else:\n",
    "                    GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "    \n",
    "            # Case 1B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_coupon_rate:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = True\n",
    "                elif N_price_or_yield!=N_maturity_date:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = True\n",
    "                else:\n",
    "                    maturities = []\n",
    "                    yields = []\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row['coupon_rate'].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row['maturity_date'].split('\\n')[tranch],\"%m/%d/%y\")\\\n",
    "                            -row['dated_date']).days\n",
    "                        coupon_rate = float(row['coupon_rate'].split('\\n')[tranch])\n",
    "                        price_or_yield = float(row['price_or_yield'].split('\\n')[tranch])\n",
    "                        n_coupon = round(maturity/182)\n",
    "                        amount = float(row['amount_by_maturity'].split('\\n')[tranch].replace(',',''))\n",
    "                        maturities = maturities+[maturity]\n",
    "                        amounts = amounts+[amount]\n",
    "                        if price_or_yield<20:\n",
    "                            yields = yields+[price_or_yield/100]\n",
    "                        elif price_or_yield>80 and price_or_yield<120:\n",
    "                            tranch_yield = (1+npf.irr([-price_or_yield]+[coupon_rate/2]*(n_coupon-1)+[100+coupon_rate/2]))**2-1\n",
    "                            yields = yields+[tranch_yield]\n",
    "                        else:\n",
    "                            yields = yields+[None]\n",
    "                            GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "                    if GPF.at[idx,'IF_price_or_yield_determined']!=False:\n",
    "                        amountsXmaturities = np.multiply(np.array(amounts),np.array(maturities))\n",
    "                        avg_yield = np.sum(np.multiply(np.array(yields),np.array(amountsXmaturities)))/np.sum(amountsXmaturities)\n",
    "                        GPF.loc[idx,'avg_yield'] = avg_yield\n",
    "                        GPF.loc[idx,'avg_maturity'] = np.dot(maturities,amounts)/np.sum(amounts)\n",
    "                    GPF.at[idx,'maturity_by_maturity_list'] = maturities\n",
    "                    GPF.at[idx,'yield_by_maturity_list'] = yields\n",
    "                    GPF.at[idx,'amount_by_maturity_list'] = amounts\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Case 2 #\n",
    "        ##########          \n",
    "\n",
    "        # Case 2: \"coupon_rate\" is not available, but \"price_or_yield\" and \"maturity_date\" is\n",
    "        elif IF_has_maturity_date and (not IF_has_coupon_rate) and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and IF_num_bonds_yield_mat_amt_consistent:\n",
    "\n",
    "            # Case 2A: If single maturity\n",
    "            if N_maturity_date==1:\n",
    "                maturity = (row['maturity_date']-row['dated_date']).days\n",
    "                GPF.loc[idx,'avg_maturity'] = maturity\n",
    "                GPF.loc[idx,'maturity_by_maturity_list'] = [[maturity]]\n",
    "                GPF.loc[idx,'amount_by_maturity_list'] = [[row['amount_by_maturity']]]\n",
    "                # Assume that if a number is more than 80, it is issuing price. If less than 20, it is issuing yield. \n",
    "                # Otherwise, undetermined\n",
    "                if float(row['price_or_yield'])<20:\n",
    "                    GPF.loc[idx,'avg_yield'] = row['price_or_yield']/100\n",
    "                    GPF.loc[idx,'yield_by_maturity_list'] = [[GPF.loc[idx,'avg_yield']]]\n",
    "                # Cannot do anything if coupon rate is unavailable and only price is given\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "            # Case 2B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_maturity_date:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = True\n",
    "                else:\n",
    "                    maturities = []\n",
    "                    yields = []\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row['maturity_date'].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row['maturity_date'].split('\\n')[tranch],\"%m/%d/%y\")-row['dated_date']).days\n",
    "                        price_or_yield = float(row['price_or_yield'].split('\\n')[tranch])\n",
    "                        amount = float(row['amount_by_maturity'].split('\\n')[tranch].replace(',',''))\n",
    "                        maturities = maturities+[maturity]\n",
    "                        amounts = amounts+[amount]\n",
    "                        if price_or_yield<20:\n",
    "                            yields = yields+[price_or_yield/100]\n",
    "                        else:\n",
    "                            # Note that it is impossible to calculate yield if only price is available for one tranch, or if I cannot\n",
    "                            # decide whether it is price or yield\n",
    "                            yields = yields+[None]\n",
    "                    if len(yields)>0:\n",
    "                        if None not in yields:\n",
    "                            amountsXmaturities = np.multiply(np.array(amounts),np.array(maturities))\n",
    "                            avg_yield = np.sum(np.multiply(np.array(yields),np.array(amountsXmaturities)))/np.sum(amountsXmaturities)\n",
    "                            GPF.loc[idx,'avg_yield'] = avg_yield\n",
    "                        else:\n",
    "                            GPF.loc[idx,'avg_yield'] = None\n",
    "                        GPF.loc[idx,'avg_maturity'] = np.dot(maturities,amounts)/np.sum(amounts)\n",
    "                    GPF.at[idx,'maturity_by_maturity_list'] = maturities\n",
    "                    GPF.at[idx,'yield_by_maturity_list'] = yields\n",
    "                    GPF.at[idx,'amount_by_maturity_list'] = amounts\n",
    "    \n",
    "    return GPF\n",
    "\n",
    "GPF = GPF.copy()\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d5d35-c352-43e6-b9b4-f72496ef045d",
   "metadata": {},
   "source": [
    "### 1.2.3 Calculate underwriting spread based on yield and TIC or NIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d38a0-ceee-4c54-a872-86f36a50f282",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- With TIC and coupon rate, I can calculate the purchase price (i.e., proceeds of the issuer). Then with price, or yield which implies price, I can obtain the reoffering price. The difference is the stipulated underwriting spread.\n",
    "- Without coupon rate, I cannot reliably stipulate underwriting spread.  \n",
    "    - E.g., if I have both reoffering yield and TIC, then  \n",
    "=> reoffering price = value of future coupon and principal payment, discounted by reoffering yield  \n",
    "=> purchase price = value of future coupon and principal payment, discounted by TIC  \n",
    "So how large the difference of the two is depends on the size of the coupon.  \n",
    "Still, I can do some stipulation of the coupon rate based on similar bonds, and have a guess of the underwriting spread.  \n",
    "    - If I have reoffering price and TIC, then, however,  \n",
    "=> purchase price = value of future coupon and principal payment, discounted by TIC  \n",
    "This will be much more sensitive to what I assume about coupon rate and hence not reliable.  \n",
    "    - Benefits of these stipulation is very low, given that, in most cases, when TIC is available, coupon rate is already available. Hence, not doing them.\n",
    "- When TIC and NIC are both available, use TIC. If only NIC is available and coupon rate is available, I can calculate the underwriter's discount as  \n",
    "NIC = (Total interest payment + Discount of reoffering price relative to par - Premium of reoffering price relative to par + underwriter's discount)/bond years of this issue,  \n",
    "noting that every term above is in dollar amounts for the whole issue.\n",
    "Then, the the underwriter's discount can be converted into in terms of every $100 of par value.  \n",
    "I can also calculate a TIC based on that, which is to be compared with the NIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed93cb-a2bc-409f-bd9b-73e536254b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "    GPF['gross_spread_tic_based'] = None\n",
    "    GPF['gross_spread_nic_based'] = None\n",
    "    GPF['tic_nic_based'] = None\n",
    "\n",
    "    # Handle case by case of each variable being missing, and within each case allow for multiple maturities\n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Do nothing if the issue contains bonds with irregular type of coupon payments\n",
    "        if row['IF_irregular_coupon_type']==True:\n",
    "            continue\n",
    "    \n",
    "        # Do nothing if dated date is missing\n",
    "        if str(row['dated_date'])=='nan' or str(row['dated_date'])=='NaT':\n",
    "            continue\n",
    "    \n",
    "        # Extract vallues that describe data availablility for a particular bond issue\n",
    "\n",
    "        IF_has_maturity_date = row['IF_has_maturity_date']\n",
    "        IF_has_coupon_rate = row['IF_has_coupon_rate']\n",
    "        IF_has_price_or_yield = row['IF_has_price_or_yield']\n",
    "        IF_has_amount_by_maturity = row['IF_has_amount_by_maturity']\n",
    "        IF_has_tic = row['IF_has_tic']\n",
    "        IF_has_nic = row['IF_has_nic']\n",
    "        \n",
    "        N_coupon_rate = row['N_coupon_rate']\n",
    "        N_price_or_yield = row['N_price_or_yield']\n",
    "        N_maturity_date = row['N_maturity_date']\n",
    "        N_amount = row['N_amount']\n",
    "    \n",
    "        IF_num_bonds_all_consistent = row['IF_num_bonds_all_consistent']\n",
    "        IF_num_bonds_yield_mat_amt_consistent = row['IF_num_bonds_yield_mat_amt_consistent']\n",
    "        IF_num_bonds_mat_amt_consistent = row['IF_num_bonds_mat_amt_consistent']\n",
    "    \n",
    "        # Extract TIC, one number for each bond issue\n",
    "        tic = None\n",
    "        tic_half = None\n",
    "        if isinstance(row['true_interest_cost'],float) or isinstance(row['true_interest_cost'],int):\n",
    "            tic = row['true_interest_cost']/100\n",
    "            tic_half = sqrt(1+tic)-1\n",
    "\n",
    "        # Extract NIC, one number for each bond issue\n",
    "        nic = None\n",
    "        if isinstance(row['net_interest_cost'],float) or isinstance(row['net_interest_cost'],int):\n",
    "            nic = row['net_interest_cost']/100\n",
    "\n",
    "        # Note that function \"npf.npv\" start with a period 0 cash flow\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Case 1 #\n",
    "        ##########\n",
    "    \n",
    "        # Case 1: \"coupon_rate\",\"maturity_date\",\"price_or_yield\", \"true_interest_cost\" are all available\n",
    "        if IF_has_maturity_date and IF_has_coupon_rate and IF_has_price_or_yield and IF_has_amount_by_maturity and IF_has_tic \\\n",
    "            and IF_num_bonds_all_consistent:\n",
    "\n",
    "            if tic>0.2:\n",
    "                continue\n",
    "    \n",
    "            # Case 1A: If single maturity\n",
    "            if N_maturity_date==1:\n",
    "                maturity = (row['maturity_date']-row['dated_date']).days\n",
    "                n_coupon = round(maturity/182)\n",
    "                npv_by_tic = npf.npv(tic_half,[0]+[row['coupon_rate']/2]*(n_coupon-1)+[100+row['coupon_rate']/2])\n",
    "                # Assume that if a number is more than 80 and less than 120, it is issuing price. If less than 20, it is issuing \n",
    "                # yield. Otherwise, undetermined\n",
    "                if float(row['price_or_yield'])<20:\n",
    "                    reoffering_yield = row['price_or_yield']/100\n",
    "                    npv_by_reoffering_yield = npf.npv(sqrt(1+reoffering_yield)-1,[0]+[row['coupon_rate']/2]*(n_coupon-1)+\\\n",
    "                        [100+row['coupon_rate']/2])\n",
    "                    GPF.loc[idx,'gross_spread_tic_based'] = npv_by_reoffering_yield-npv_by_tic\n",
    "                elif float(row['price_or_yield'])>80 and float(row['price_or_yield'])<120:\n",
    "                    npv_by_reoffering_yield = row['price_or_yield']\n",
    "                    GPF.loc[idx,'gross_spread_tic_based'] = npv_by_reoffering_yield-npv_by_tic\n",
    "    \n",
    "            # Case 1B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_coupon_rate:\n",
    "                    continue\n",
    "                elif N_price_or_yield!=N_maturity_date:\n",
    "                    continue\n",
    "                else:\n",
    "                    npv_by_tics = []\n",
    "                    npv_by_reoffering_yields = []\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row['coupon_rate'].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row['maturity_date'].split('\\n')[tranch],\"%m/%d/%y\")\\\n",
    "                            -row['dated_date']).days\n",
    "                        coupon_rate = float(row['coupon_rate'].split('\\n')[tranch])\n",
    "                        price_or_yield = float(row['price_or_yield'].split('\\n')[tranch])\n",
    "                        n_coupon = round(maturity/182)\n",
    "                        amount = float(row['amount_by_maturity'].split('\\n')[tranch].replace(',',''))\n",
    "                        amounts = amounts+[amount]\n",
    "                        npv_by_tic = npf.npv(tic_half,[0]+[coupon_rate/2]*(n_coupon-1)+[100+coupon_rate/2])\n",
    "                        npv_by_tics = npv_by_tics+[npv_by_tic]\n",
    "                        if price_or_yield<20:\n",
    "                            reoffering_yield = price_or_yield/100\n",
    "                            npv_by_reoffering_yield = npf.npv(sqrt(1+reoffering_yield)-1,[0]+[coupon_rate/2]*(n_coupon-1)+\\\n",
    "                                [100+coupon_rate/2])\n",
    "                            npv_by_reoffering_yields = npv_by_reoffering_yields+[npv_by_reoffering_yield]\n",
    "                        elif price_or_yield>80 and price_or_yield<120:\n",
    "                            npv_by_reoffering_yield = price_or_yield\n",
    "                            npv_by_reoffering_yields = npv_by_reoffering_yields+[npv_by_reoffering_yield]\n",
    "                        else:\n",
    "                            npv_by_reoffering_yields = npv_by_reoffering_yields+[None]\n",
    "                            GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "\n",
    "                # Average the gross spread across tranches\n",
    "                if None in npv_by_reoffering_yields:\n",
    "                    continue\n",
    "                else:\n",
    "                    # In the following averaging, as underwriter's discount is a one-time cost, averaging does not use maturity \n",
    "                    # as part of the weight\n",
    "                    GPF.loc[idx,'gross_spread_tic_based'] = \\\n",
    "                        np.dot(np.array(npv_by_reoffering_yields)-np.array(npv_by_tics),np.array(amounts))/ \\\n",
    "                        np.sum(np.array(amounts))\n",
    "\n",
    "    \n",
    "        ##########\n",
    "        # Case 2 #\n",
    "        ##########\n",
    "    \n",
    "        # Case 2: \"coupon_rate\",\"maturity_date\",\"price_or_yield\" are all available. \"true_interest_cost\" is not available, while\n",
    "        # \"net_interest_cost\" is available\n",
    "\n",
    "        # The calculation below is based on dollar amounts of the whole bond issue, rather than every $100 par value\n",
    "        \n",
    "        if IF_has_maturity_date and IF_has_coupon_rate and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and (not IF_has_tic) and IF_has_nic \\\n",
    "            and IF_num_bonds_all_consistent:\n",
    "\n",
    "            if nic>0.2:\n",
    "                continue\n",
    "\n",
    "            # Case 2A: If single maturity\n",
    "            if N_maturity_date==1:\n",
    "    \n",
    "                maturity = (row['maturity_date']-row['dated_date']).days\n",
    "                n_coupon = round(maturity/182)\n",
    "    \n",
    "                # Obtain the discount of reoffering price relative to par vlaue\n",
    "                # Assume that if a number is more than 80 and less than 120, it is issuing price. If less than 20, it is issuing \n",
    "                # yield. Otherwise, undetermined\n",
    "                if float(row['price_or_yield'])<20:\n",
    "                    reoffering_yield = row['price_or_yield']/100\n",
    "                    npv_by_reoffering_yield = npf.npv(sqrt(1+reoffering_yield)-1,\n",
    "                        [0]+[row['amount']*row['coupon_rate']/100/2]*(n_coupon-1)+[row['amount']+row['amount']*row['coupon_rate']/100/2])\n",
    "                elif float(row['price_or_yield'])>80 and float(row['price_or_yield'])<120:\n",
    "                    npv_by_reoffering_yield = row['price_or_yield']/100*row['amount']\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "                # Calculate underwriter discount implied by NIC\n",
    "                # Discount of reoffering price relative to par value\n",
    "                discount_reoff_to_par = npv_by_reoffering_yield-row['amount']\n",
    "                total_interest = row['coupon_rate']/100/2*row['amount']*n_coupon\n",
    "                # The sum of the product of each year's maturity value and the number of years to its maturity\n",
    "                bondyear = row['amount']*(n_coupon/2)\n",
    "                underwriter_discount = nic*bondyear-total_interest+discount_reoff_to_par\n",
    "                underwriter_discount_100par = underwriter_discount/row['amount']*100\n",
    "    \n",
    "                # Calculate TIC implied by NIC\n",
    "                tic_nic_based = (1+npf.irr([-(row['amount']+discount_reoff_to_par-underwriter_discount)]+\\\n",
    "                    [row['coupon_rate']/2/100*row['amount']]*(n_coupon-1)+[row['amount']+row['coupon_rate']/2/100*row['amount']]))\\\n",
    "                    **2-1\n",
    "    \n",
    "                GPF.loc[idx,'gross_spread_nic_based'] = underwriter_discount_100par\n",
    "                GPF.loc[idx,'tic_nic_based'] = tic_nic_based\n",
    "    \n",
    "    \n",
    "            # Case 2B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_coupon_rate:\n",
    "                    continue\n",
    "                elif N_price_or_yield!=N_maturity_date:\n",
    "                    continue\n",
    "                else:\n",
    "                    bondyears = []\n",
    "                    total_interests = []\n",
    "                    discount_reoff_to_pars = []\n",
    "                    cash_flows = [] # Cash flow of the whole bond, not every $100 par value\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row['coupon_rate'].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row['maturity_date'].split('\\n')[tranch],\"%m/%d/%y\")\\\n",
    "                            -row['dated_date']).days\n",
    "                        coupon_rate = float(row['coupon_rate'].split('\\n')[tranch])\n",
    "                        price_or_yield = float(row['price_or_yield'].split('\\n')[tranch])\n",
    "                        n_coupon = round(maturity/182)\n",
    "                        amount = float(row['amount_by_maturity'].split('\\n')[tranch].replace(',',''))\n",
    "                        amounts = amounts+[amount]\n",
    "                        cash_flows = cash_flows+[[0]+[coupon_rate/2/100*amount]*(n_coupon-1)+[amount+coupon_rate/2/100*amount]]\n",
    "                        if price_or_yield<20:\n",
    "                            reoffering_yield = price_or_yield/100\n",
    "                            # Below is a value per every $100 par value, which needs to be converted to the value of the whole bond\n",
    "                            npv_by_reoffering_yield = npf.npv(sqrt(1+reoffering_yield)-1,[0]+[coupon_rate/2]*(n_coupon-1)+[100+coupon_rate/2])\n",
    "                            discount_reoff_to_par = npv_by_reoffering_yield/100*amount-amount\n",
    "                            total_interest = coupon_rate/2/100*amount*n_coupon\n",
    "                            bondyear = amount*(n_coupon/2)\n",
    "                            bondyears = bondyears+[bondyear]\n",
    "                            total_interests = total_interests+[total_interest]\n",
    "                            discount_reoff_to_pars = discount_reoff_to_pars+[discount_reoff_to_par]\n",
    "                        elif price_or_yield>80 and price_or_yield<120:\n",
    "                            npv_by_reoffering_yield = price_or_yield\n",
    "                            discount_reoff_to_par = npv_by_reoffering_yield/100*amount-amount\n",
    "                            total_interest = coupon_rate/2/100*amount*n_coupon\n",
    "                            bondyear = amount*(n_coupon/2)\n",
    "                            bondyears = bondyears+[bondyear]\n",
    "                            total_interests = total_interests+[total_interest]\n",
    "                            discount_reoff_to_pars = discount_reoff_to_pars+[discount_reoff_to_par]\n",
    "                        else:\n",
    "                            bondyears = bondyears+[None]\n",
    "                            total_interests = total_interests+[None]\n",
    "                            discount_reoff_to_pars = discount_reoff_to_pars+[None]\n",
    "                            GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "    \n",
    "                # Aggregate across trenches and calculate the NIC implied underwriter discount and TIC\n",
    "                if None in bondyears:\n",
    "                    continue\n",
    "                else:\n",
    "                    # underwriter_discount of the whole issue, not in every $100 par value\n",
    "                    underwriter_discount = nic*np.sum(bondyears)-np.sum(total_interests)+np.sum(discount_reoff_to_pars)\n",
    "                    underwriter_discount_100par = underwriter_discount/np.sum(amounts)*100\n",
    "                    GPF.loc[idx,'gross_spread_nic_based'] = underwriter_discount_100par\n",
    "    \n",
    "                    max_length = max(len(lst) for lst in cash_flows)\n",
    "                    padded_lists = [lst+[0]*(max_length-len(lst)) for lst in cash_flows]\n",
    "                    cash_flow = [sum(elements) for elements in zip(*padded_lists)]\n",
    "                    cash_flow[0] = cash_flow[0]-(np.sum(amounts)+np.sum(discount_reoff_to_pars)-underwriter_discount)\n",
    "                    tic_nic_based = (1+npf.irr(cash_flow))**2-1\n",
    "                    GPF.loc[idx,'tic_nic_based'] = tic_nic_based\n",
    "\n",
    "    return GPF\n",
    "\n",
    "GPF = GPF.copy()\n",
    "meta_columns = list(proc_list(GPF.sample(10)).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d52406-7cac-413a-9eda-9ef3d8e44a65",
   "metadata": {},
   "source": [
    "### 1.2.4 Calculate spread relative to treasury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02121f08-5a20-4c04-84ad-21291b447db6",
   "metadata": {},
   "source": [
    "#### 1.2.4.1 Calculate yield of synthetic risk-free treasury bond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0712f2-cc52-46a9-ac1d-da39aa2080e3",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Without coupon rate, it is impossible to calculate the price (yield) of the synthetic treasury bond. For example, consider Bond A: Pays \\\\$5 one year from now, and \\\\$5 two years from now, and Bond B: Pays \\\\$100*(1+5%)^2 two years from now. These two have the same yield. Suppose that treasury yield in one year is 0% and in two years is 20%. Then price of first synthetic bond is higher than the second, and the yield of the first synthetic bond is lower than the second. In other words, without coupon rate, I do not know when the cash flow is going to come, so I do not know what is the component of risk-free rate that I should tease out from the return of the municipal bond yield. An INACCURATE approximation can be simply using yield of municipal bond minus that of treasury bond, but it is erraneous to do so.\n",
    "- Luckily except for later parts of the sample, coupon rate is usually available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4694d3-e25e-4d97-abfb-f0407a7e41ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "    \n",
    "    # Treasury yield\n",
    "    feds200628 = pd.read_csv(\"../RawData/FedBOG/feds200628.csv\", header=9)\n",
    "    feds200628 = feds200628[~pd.isnull(feds200628['SVENY01'])]\n",
    "    columns = ['Date']+ \\\n",
    "        ['SVENY0'+str(i) for i in range(1,10)]+ \\\n",
    "        ['SVENY'+str(i) for i in range(10,31)]\n",
    "    feds200628 = feds200628[columns]\n",
    "    new_columns = ['Date']+ \\\n",
    "        ['SVENY'+str(i) for i in range(1,10)]+ \\\n",
    "        ['SVENY'+str(i) for i in range(10,31)]\n",
    "    feds200628.columns = new_columns\n",
    "    feds200628['Date'] = pd.to_datetime(feds200628['Date'])\n",
    "    threshold_date = pd.to_datetime('2050-01-01')\n",
    "    feds200628['Date'] = feds200628['Date'].apply(lambda x: x - pd.DateOffset(years=100) if x > threshold_date else x)\n",
    "    \n",
    "    max_year_7 = [pd.Timestamp(1961,6,14,0,0,0),pd.Timestamp(1971,8,15,0,0,0)]\n",
    "    max_year_10 = [pd.Timestamp(1971,8,16,0,0,0),pd.Timestamp(1971,11,14,0,0,0)]\n",
    "    max_year_15 = [pd.Timestamp(1971,11,15,0,0,0),pd.Timestamp(1981,7,1,0,0,0)]\n",
    "    max_year_20 = [pd.Timestamp(1981,7,2,0,0,0),pd.Timestamp(1985,11,24,0,0,0)]\n",
    "    max_year_30 = [pd.Timestamp(1985,11,25,0,0,0),pd.Timestamp(2023,11,3,0,0,0)]\n",
    "\n",
    "    # Initialize yield of synthetic bond\n",
    "    GPF['sync_treasury_bond_yield_by_maturity_list'] = None\n",
    "    GPF['sync_treasury_bond_avg_yield'] = None\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Do nothing if dated date is missing\n",
    "        if str(row['dated_date'])=='nan' or str(row['dated_date'])=='NaT':\n",
    "            continue\n",
    "\n",
    "        # For all bond issues before 30 years length of treasury yield is available, do not construct an equivalent treasury security\n",
    "        if row['dated_date']<max_year_30[0]:\n",
    "            continue\n",
    "\n",
    "        # Do nothing if the issue contains bonds with irregular type of coupon payments\n",
    "        if row['IF_irregular_coupon_type']==True:\n",
    "            continue\n",
    "\n",
    "        IF_has_maturity_date = row['IF_has_maturity_date']\n",
    "        IF_has_coupon_rate = row['IF_has_coupon_rate']\n",
    "        IF_has_price_or_yield = row['IF_has_price_or_yield']\n",
    "        IF_has_amount_by_maturity = row['IF_has_amount_by_maturity']\n",
    "        \n",
    "        if (not IF_has_coupon_rate) or \\\n",
    "            (not IF_has_maturity_date) or \\\n",
    "            (not IF_has_price_or_yield) or \\\n",
    "            (not IF_has_amount_by_maturity):\n",
    "            continue\n",
    "\n",
    "        N_coupon_rate = row['N_coupon_rate']\n",
    "        N_price_or_yield = row['N_price_or_yield']\n",
    "        N_maturity_date = row['N_maturity_date']\n",
    "        N_amount = row['N_amount']\n",
    "\n",
    "        IF_num_bonds_all_consistent = row['IF_num_bonds_all_consistent']\n",
    "        IF_num_bonds_yield_mat_amt_consistent = row['IF_num_bonds_yield_mat_amt_consistent']\n",
    "        IF_num_bonds_mat_amt_consistent = row['IF_num_bonds_mat_amt_consistent']\n",
    "\n",
    "        # Obtain the treasury zero-coupon yield curve at the closest date\n",
    "        feds200628_copy = feds200628.copy()\n",
    "        feds200628_copy['dif_date'] = np.abs(feds200628_copy['Date']-row['dated_date'])\n",
    "        feds200628_copy = feds200628_copy.sort_values('dif_date').reset_index()\n",
    "    \n",
    "        sync_bond_yield_by_maturity = []\n",
    "    \n",
    "        # If single maturity\n",
    "        if N_coupon_rate==1 and IF_num_bonds_all_consistent:\n",
    "\n",
    "            if row['price_or_yield']>20 and row['price_or_yield']<80:\n",
    "                continue\n",
    "            \n",
    "            coupon_rate = float(row['coupon_rate'])\n",
    "            maturity = (row['maturity_date']-row['dated_date']).days\n",
    "\n",
    "            if maturity>30*365:\n",
    "                continue\n",
    "            else:\n",
    "            \n",
    "                cf = []\n",
    "                discount_factor = []\n",
    "                N_coupons = int(np.max([1,np.around(maturity/(365/2))]))\n",
    "        \n",
    "                # Construct a series of cash flow for each bond\n",
    "                for cf_idx in range(0,N_coupons):\n",
    "                    cf = cf+[coupon_rate/2]\n",
    "                cf[N_coupons-1] = cf[N_coupons-1]+100\n",
    "        \n",
    "                # Construct a series of discount factor for each bond\n",
    "                for cf_idx in range(0,N_coupons):\n",
    "                    if cf_idx==0:\n",
    "                        discount_factor = discount_factor+[feds200628_copy['SVENY1'][0]]\n",
    "                    elif cf_idx%2==1:\n",
    "                        discount_factor = discount_factor+[feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]]\n",
    "                    elif cf_idx%2==0:\n",
    "                        discount_factor = discount_factor+\\\n",
    "                            [(feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]\n",
    "                            +feds200628_copy['SVENY'+str(ceil(cf_idx/2)+1)][0])/2]\n",
    "                discount_factor = [(1/(1+discount_factor[disc_idx]/100))**((disc_idx+1)/2) for disc_idx in range(0,N_coupons)]\n",
    "    \n",
    "                # Bond price and yield of synthetic bond\n",
    "                sync_bond_price = np.sum(np.dot(cf,discount_factor))\n",
    "                cf = [-sync_bond_price]+cf\n",
    "                sync_bond_yield = (1+npf.irr(cf))**2-1\n",
    "                sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[sync_bond_yield]\n",
    "    \n",
    "                # Record data\n",
    "                GPF.at[idx,'sync_treasury_bond_yield_by_maturity_list'] = sync_bond_yield_by_maturity\n",
    "                GPF.at[idx,'sync_treasury_bond_avg_yield'] = sync_bond_yield\n",
    "\n",
    "    \n",
    "        # If multiple maturity, go over bond by bond\n",
    "        if N_coupon_rate>1 and IF_num_bonds_all_consistent:\n",
    "\n",
    "            cfs = []\n",
    "    \n",
    "            for bond_idx in range(0,N_maturity_date):\n",
    "    \n",
    "                if (float(row['price_or_yield'].split('\\n')[bond_idx])>20) and \\\n",
    "                    (float(row['price_or_yield'].split('\\n')[bond_idx])<80):\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[None]\n",
    "                    continue\n",
    "    \n",
    "                coupon_rate = float(row['coupon_rate'].split('\\n')[bond_idx])\n",
    "                maturity = row['maturity_by_maturity_list'][bond_idx]\n",
    "\n",
    "                if maturity>30*365:\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[None]\n",
    "                    continue\n",
    "                else:\n",
    "\n",
    "                    cf = []\n",
    "                    discount_factor = []\n",
    "                    N_coupons = int(np.max([1,np.around(maturity/(365/2))]))\n",
    "            \n",
    "                    # Construct a series of cash flow for each bond\n",
    "                    for cf_idx in range(0,N_coupons):\n",
    "                        cf = cf+[coupon_rate/2]\n",
    "                    cf[N_coupons-1] = cf[N_coupons-1]+100\n",
    "            \n",
    "                    # Construct a series of discount factor for each bond\n",
    "                    for cf_idx in range(0,N_coupons):\n",
    "                        if cf_idx==0:\n",
    "                            discount_factor = discount_factor+[feds200628_copy['SVENY1'][0]]\n",
    "                        elif cf_idx%2==1:\n",
    "                            discount_factor = discount_factor+[feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]]\n",
    "                        elif cf_idx%2==0:\n",
    "                            discount_factor = discount_factor+\\\n",
    "                                [(feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]\n",
    "                                +feds200628_copy['SVENY'+str(ceil(cf_idx/2)+1)][0])/2]\n",
    "                    discount_factor = [(1/(1+discount_factor[disc_idx]/100))**((disc_idx+1)/2) for disc_idx in range(0,N_coupons)]\n",
    "        \n",
    "                    # Bond price and yield of synthetic bond\n",
    "                    sync_bond_price = np.sum(np.dot(cf,discount_factor))\n",
    "                    cf = [-sync_bond_price]+cf\n",
    "                    cfs = cfs+[cf]\n",
    "                    sync_bond_yield = (1+npf.irr(cf))**2-1\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[sync_bond_yield]\n",
    "        \n",
    "            GPF.at[idx,'sync_treasury_bond_yield_by_maturity_list'] = sync_bond_yield_by_maturity\n",
    "            if len(cfs)>0 and None not in sync_bond_yield_by_maturity:\n",
    "                max_length = max(len(lst) for lst in cfs)\n",
    "                padded_lists = [lst+[0]*(max_length-len(lst)) for lst in cfs]\n",
    "                cf = [sum(elements) for elements in zip(*padded_lists)]\n",
    "                GPF.at[idx,'sync_treasury_bond_avg_yield'] = (1+npf.irr(cf))**2-1\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd9356-61f6-46c7-8d93-c7028dfe7f8b",
   "metadata": {},
   "source": [
    "#### 1.2.4.2 Calculate spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e5fd-c6f8-4f73-bff8-70572b95d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spread. Note that I calculate spread a bit differently from Li and Zhu: Theirs is from the perspective of a taxed\n",
    "# individual, while mine is from the perspective of a non-taxed individual\n",
    "\n",
    "tax_rate = {\n",
    "    1967:0.700,1968:0.700,1969:0.700,1970:0.700,1971:0.700,1972:0.700,1973:0.700,\n",
    "    1974:0.700,1975:0.700,1976:0.700,1977:0.700,1978:0.700,1979:0.700,1980:0.700,\n",
    "    1981:0.700,1982:0.500,1983:0.500,1984:0.500,1985:0.500,1986:0.500,1987:0.385,\n",
    "    1988:0.280,1989:0.280,1990:0.280,1991:0.310,1992:0.310,1993:0.396,1994:0.396,\n",
    "    1995:0.396,1996:0.396,1997:0.396,1998:0.396,1999:0.396,2000:0.396,2001:0.391,\n",
    "    2002:0.386,2003:0.350,2004:0.350,2005:0.350,2006:0.350,2007:0.350,2008:0.350,\n",
    "    2009:0.350,2010:0.350,2011:0.350,2012:0.350,2013:0.396,2014:0.396,2015:0.396,\n",
    "    2016:0.396,2017:0.396,2018:0.370,2019:0.370,2020:0.370,2021:0.370,2022:0.370,\n",
    "    2023:0.370,\n",
    "    }\n",
    "\n",
    "GPF = GPF.reset_index(drop=True)\n",
    "GPF['treasury_spread_by_maturity_list'] = None\n",
    "GPF['treasury_avg_spread'] = None\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    for idx,row in GPF.iterrows():\n",
    "        \n",
    "        spread_by_maturity = []\n",
    "        if row['sync_treasury_bond_yield_by_maturity_list']!=None and row['yield_by_maturity_list']!=None:\n",
    "            for bond_idx in range(0,len(GPF.at[idx,'sync_treasury_bond_yield_by_maturity_list'])):\n",
    "                if row['sync_treasury_bond_yield_by_maturity_list'][bond_idx]==None or \\\n",
    "                    row['yield_by_maturity_list'][bond_idx]==None:\n",
    "                    spread_by_maturity = spread_by_maturity+[None]\n",
    "                else:\n",
    "                    # Adjust for tax here\n",
    "                    if row['taxable_code']=='E':\n",
    "                        spread_by_maturity = spread_by_maturity+\\\n",
    "                            [row['yield_by_maturity_list'][bond_idx]\n",
    "                            -row['sync_treasury_bond_yield_by_maturity_list'][bond_idx]*(1-tax_rate[row['sale_year']])]\n",
    "                    elif row['taxable_code']=='A' or 'T':\n",
    "                        spread_by_maturity = spread_by_maturity+\\\n",
    "                            [row['yield_by_maturity_list'][bond_idx]*(1-tax_rate[row['sale_year']])\n",
    "                            -row['sync_treasury_bond_yield_by_maturity_list'][bond_idx]*(1-tax_rate[row['sale_year']])]\n",
    "            GPF.at[idx,'treasury_spread_by_maturity_list'] = spread_by_maturity\n",
    "\n",
    "        if GPF.at[idx,'treasury_spread_by_maturity_list']!=None:\n",
    "            if None not in GPF.at[idx,'treasury_spread_by_maturity_list']:\n",
    "                amountsXmaturities = np.multiply(np.array(row['amount_by_maturity_list']),np.array(row['maturity_by_maturity_list']))\n",
    "                if np.sum(amountsXmaturities)==0:\n",
    "                    continue\n",
    "                avg_spread = np.sum(np.multiply(np.array(spread_by_maturity),np.array(amountsXmaturities)))/np.sum(amountsXmaturities)\n",
    "                GPF.at[idx,'treasury_avg_spread'] = avg_spread\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f544da9-1f59-460d-aad9-3bba7c13fb2b",
   "metadata": {},
   "source": [
    "### 1.2.5 Calculate spread relative to MMA AAA curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee678b-a8cd-427d-8e00-6604c2f6683a",
   "metadata": {},
   "source": [
    "Following Goldsmith-Pinkham et al and Liang. Note that unlike constructing synthetic treasury, I do not consider coupon rates below and rather directly match on maturity and substract the reoffering yield and the matched point on the AAA curve on the **sale date**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798e9ed-0aec-4a9f-ae68-b32e757d6b1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def proc_list(GPF):\n",
    "        \n",
    "    MMAICurve = []\n",
    "    for maturity in range(1,31):\n",
    "        MMAI = pd.read_excel(\"../RawData/Bloomberg/MMAI.xlsx\",sheet_name='Sheet'+str(maturity))\n",
    "        MMAI['Maturity'] = maturity\n",
    "        MMAICurve = MMAICurve+[MMAI]\n",
    "    MMAICurve = pd.concat(MMAICurve)\n",
    "    MMAICurve = MMAICurve.sort_values(['Date','Maturity']).reset_index(drop=True)\n",
    "    \n",
    "    # Initialize yield of synthetic bond\n",
    "    GPF['MMA_yield_by_maturity_list'] = None\n",
    "    GPF['MMA_spread_by_maturity_list'] = None\n",
    "    GPF['MMA_avg_yield'] = None\n",
    "    GPF['MMA_avg_spread'] = None\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "    \n",
    "        N_coupon_rate = row['N_coupon_rate']\n",
    "        N_maturity_date = row['N_maturity_date']\n",
    "    \n",
    "        if row['yield_by_maturity_list']==None:\n",
    "            continue\n",
    "        if row['maturity_by_maturity_list']==None:\n",
    "            continue\n",
    "        if row['amount_by_maturity_list']==None:\n",
    "            continue\n",
    "        if row['sale_date']==None:\n",
    "            continue\n",
    "    \n",
    "        # Obtain the MMA yield curve at the sale date\n",
    "        MMAICurve_oneday = MMAICurve[MMAICurve['Date']==row['sale_date']].copy()\n",
    "        if len(MMAICurve_oneday)==0:\n",
    "            continue\n",
    "        \n",
    "        MMA_yield_by_maturity = []\n",
    "        MMA_spread_by_maturity = []\n",
    "    \n",
    "        # If single maturity\n",
    "        if N_coupon_rate==1:\n",
    "    \n",
    "            # Determine if maturity is above 30 years, above which there is not a corresponding yield point on the curve\n",
    "            if row['maturity_by_maturity_list'][0]==None:\n",
    "                continue\n",
    "            if row['maturity_by_maturity_list'][0]>=365*30.5:\n",
    "                continue\n",
    "    \n",
    "            MMAICurve_oneday['dif_mat'] = np.absolute(row['maturity_by_maturity_list'][0]-MMAICurve_oneday['Maturity']*365)\n",
    "            MMAICurve_oneday = MMAICurve_oneday.sort_values('dif_mat').reset_index(drop=True)\n",
    "            MMA_yield = MMAICurve_oneday['Mid Price'][0]/100\n",
    "            MMA_yield_by_maturity = MMA_yield_by_maturity+[MMA_yield]\n",
    "            if row['yield_by_maturity_list'][0]!=None:\n",
    "                MMA_spread_by_maturity = MMA_spread_by_maturity+[row['yield_by_maturity_list'][0]-MMA_yield]\n",
    "            else:\n",
    "                MMA_spread_by_maturity = MMA_spread_by_maturity+[None]\n",
    "    \n",
    "            GPF.at[idx,'MMA_yield_by_maturity_list'] = MMA_yield_by_maturity\n",
    "            GPF.at[idx,'MMA_spread_by_maturity_list'] = MMA_spread_by_maturity\n",
    "            GPF.at[idx,'MMA_avg_yield'] = MMA_yield_by_maturity[0]\n",
    "            GPF.at[idx,'MMA_avg_spread'] = MMA_spread_by_maturity[0]\n",
    "    \n",
    "    \n",
    "        # If multiple maturity, go over bond by bond\n",
    "        if N_coupon_rate>1:\n",
    "    \n",
    "            for bond_idx in range(0,N_maturity_date):\n",
    "    \n",
    "                # Determine if maturity is above 30 years, above which there is not a corresponding yield point on the curve\n",
    "                if row['maturity_by_maturity_list'][bond_idx]==None:\n",
    "                    MMA_yield_by_maturity = MMA_yield_by_maturity+[None]\n",
    "                    MMA_spread_by_maturity = MMA_spread_by_maturity+[None]\n",
    "                    continue\n",
    "                if row['maturity_by_maturity_list'][bond_idx]>=365*30.5:\n",
    "                    MMA_yield_by_maturity = MMA_yield_by_maturity+[None]\n",
    "                    MMA_spread_by_maturity = MMA_spread_by_maturity+[None]\n",
    "                    continue\n",
    "    \n",
    "                MMAICurve_oneday['dif_mat'] = np.absolute(row['maturity_by_maturity_list'][bond_idx]-MMAICurve_oneday['Maturity']*365)\n",
    "                MMAICurve_oneday = MMAICurve_oneday.sort_values('dif_mat').reset_index(drop=True)\n",
    "                MMA_yield = MMAICurve_oneday['Mid Price'][0]/100\n",
    "                MMA_yield_by_maturity = MMA_yield_by_maturity+[MMA_yield]\n",
    "                if row['yield_by_maturity_list'][bond_idx]!=None:\n",
    "                    MMA_spread_by_maturity = MMA_spread_by_maturity+[row['yield_by_maturity_list'][bond_idx]-MMA_yield]\n",
    "                else:\n",
    "                    MMA_spread_by_maturity = MMA_spread_by_maturity+[None]\n",
    "    \n",
    "            GPF.at[idx,'MMA_yield_by_maturity_list'] = MMA_yield_by_maturity\n",
    "            GPF.at[idx,'MMA_spread_by_maturity_list'] = MMA_spread_by_maturity\n",
    "            amountsXmaturities = np.multiply(np.array(row['amount_by_maturity_list']),np.array(row['maturity_by_maturity_list']))\n",
    "            if np.sum(amountsXmaturities)==0:\n",
    "                continue\n",
    "            if None in MMA_spread_by_maturity:\n",
    "                GPF.at[idx,'MMA_avg_spread'] = None\n",
    "                continue\n",
    "            avg_yield = np.sum(np.multiply(np.array(MMA_yield_by_maturity),np.array(amountsXmaturities)))/np.sum(amountsXmaturities)\n",
    "            avg_spread = np.sum(np.multiply(np.array(MMA_spread_by_maturity),np.array(amountsXmaturities)))/np.sum(amountsXmaturities)\n",
    "            GPF.at[idx,'MMA_avg_yield'] = avg_yield\n",
    "            GPF.at[idx,'MMA_avg_spread'] = avg_spread\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF.sample(1000)).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead56c49-2253-48a1-afb8-79111cf05ed0",
   "metadata": {},
   "source": [
    "## 1.3 Export a central \"GPF\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8606d48-378c-46ca-a4da-91e7909db69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "first_columns = ['dated_date','net_interest_cost','true_interest_cost',\n",
    "    'maturity_date','coupon_rate','price_or_yield','amount',\n",
    "    'maturity_by_maturity_list','amount_by_maturity_list',\n",
    "    'yield_by_maturity_list','sync_treasury_bond_yield_by_maturity_list','treasury_spread_by_maturity_list',\n",
    "    'MMA_yield_by_maturity_list','MMA_spread_by_maturity_list',\n",
    "    'avg_yield','avg_maturity',\n",
    "    'sync_treasury_bond_avg_yield','MMA_avg_yield',\n",
    "    'treasury_avg_spread','MMA_avg_spread',\n",
    "    'gross_spread','gross_spread_tic_based','gross_spread_nic_based','tic_nic_based',\n",
    "    'IF_price_or_yield_determined','IF_irregular_coupon_type']\n",
    "GPF = GPF[first_columns+sorted([item for item in GPF.columns if item not in first_columns])]\n",
    "GPF.to_csv(\"../CleanData/SDC/0A_GPF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a1907-5550-46fd-85eb-81a2fff80b5f",
   "metadata": {},
   "source": [
    "# 2. Quantity of issuance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045b12b-c054-421f-b1ef-d2852e477cd7",
   "metadata": {},
   "source": [
    "Obtain yearly quantity of debt at county level, by aggregate and also by \n",
    "1. The method of placement\n",
    "2. The use of proceeds\n",
    "3. Type of borrowing entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fd6c8-c57a-47f6-9397-8d23810dc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_GetQ_byPlacement)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_GetQ_byPlacement\n",
    "importlib.reload(FUN_0A_GetQ_byPlacement)\n",
    "from FUN_0A_GetQ_byPlacement import FUN_0A_GetQ_byPlacement\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_GetQ_byUsageBB)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_GetQ_byUsageBB\n",
    "importlib.reload(FUN_0A_GetQ_byUsageBB)\n",
    "from FUN_0A_GetQ_byUsageBB import FUN_0A_GetQ_byUsageBB\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_GetQ_byUsageGeneral)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_GetQ_byUsageGeneral\n",
    "importlib.reload(FUN_0A_GetQ_byUsageGeneral)\n",
    "from FUN_0A_GetQ_byUsageGeneral import FUN_0A_GetQ_byUsageGeneral\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_GetQ_byUsageMain)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_GetQ_byUsageMain\n",
    "importlib.reload(FUN_0A_GetQ_byUsageMain)\n",
    "from FUN_0A_GetQ_byUsageMain import FUN_0A_GetQ_byUsageMain\n",
    "\n",
    "try:\n",
    "    del(FUN_0A_GetQ_byIssuerType)\n",
    "except:\n",
    "    pass\n",
    "import FUN_0A_GetQ_byIssuerType\n",
    "importlib.reload(FUN_0A_GetQ_byIssuerType)\n",
    "from FUN_0A_GetQ_byIssuerType import FUN_0A_GetQ_byIssuerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb7dea-bcfb-49f8-9bf1-965eaab1e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# %%time\n",
    "\n",
    "# For speed reasons, proceed year by year\n",
    "Years = list(range(1967,2023))\n",
    "\n",
    "GPFAmount = GPF[['State','County','sale_year','amount',\n",
    "    'issuer_type_full','Bid',\n",
    "    'use_of_proceeds_BB','use_of_proceeds_general','use_of_proceeds_main']].copy()\n",
    "\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='nan']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='AS']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='DC']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='FF']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='GU']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='MR']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='PR']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='TT']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='VI']\n",
    "\n",
    "GPFAmount = GPFAmount.reset_index(drop=True)\n",
    "GPFAmount = GPFAmount[~pd.isnull(GPFAmount['County'])]\n",
    "\n",
    "def proc_list(GPFAmount):\n",
    "    GPFAmount = GPFAmount.copy()\n",
    "    GPFAmount['County'] = GPFAmount['County'].str.replace(' AND ','/')\n",
    "    GPFAmount_New = []\n",
    "    for idx,row in GPFAmount.iterrows():\n",
    "        if '/' not in row['County']:\n",
    "            GPFAmount_New = GPFAmount_New+[dict(row)]\n",
    "        else:\n",
    "            Countys = row['County'].split('/')\n",
    "            for County in Countys:\n",
    "                row_new = dict(row)\n",
    "                row_new['County'] = County\n",
    "                row_new['amount'] = row['amount']/len(Countys)\n",
    "                GPFAmount_New = GPFAmount_New+[row_new]\n",
    "    GPFAmount_New = pd.DataFrame(GPFAmount_New)\n",
    "    GPFAmount_New['County'] = GPFAmount_New['County'].str.strip()\n",
    "    return GPFAmount_New\n",
    "\n",
    "meta_columns = list(proc_list(GPFAmount[:10]).columns)\n",
    "GPFAmount_dd = dd.from_pandas(GPFAmount, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPFAmount = GPFAmount_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "\n",
    "#---------------------#\n",
    "# Method of placement #\n",
    "#---------------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXBid = p.starmap(FUN_0A_GetQ_byPlacement, input_list)\n",
    "StateXCountyXBid = pd.concat(StateXCountyXBid)\n",
    "StateXCountyXBid.to_parquet(\"../CleanData/SDC/0A_StateXCountyXBid.parquet\")\n",
    "\n",
    "#-----------------#\n",
    "# Use of proceeds #\n",
    "#-----------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageBB = p.starmap(FUN_0A_GetQ_byUsageBB, input_list)\n",
    "StateXCountyXUsageBB = pd.concat(StateXCountyXUsageBB)\n",
    "StateXCountyXUsageBB.to_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageBB.parquet\")\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageGeneral = p.starmap(FUN_0A_GetQ_byUsageGeneral, input_list)\n",
    "StateXCountyXUsageGeneral = pd.concat(StateXCountyXUsageGeneral)\n",
    "StateXCountyXUsageGeneral.to_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageGeneral.parquet\")\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageMain = p.starmap(FUN_0A_GetQ_byUsageMain, input_list)\n",
    "StateXCountyXUsageMain = pd.concat(StateXCountyXUsageMain)\n",
    "StateXCountyXUsageMain.to_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageMain.parquet\")\n",
    "\n",
    "#----------------#\n",
    "# Type of issuer #\n",
    "#----------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXIssuerType = p.starmap(FUN_0A_GetQ_byIssuerType, input_list)\n",
    "StateXCountyXIssuerType = pd.concat(StateXCountyXIssuerType)\n",
    "StateXCountyXIssuerType.to_parquet(\"../CleanData/SDC/0A_StateXCountyXIssuerType.parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
