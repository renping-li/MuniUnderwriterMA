{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7332f2-8102-4987-bca1-4db197860ea8",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- The state CT is not listed as counties in \"CBSA.xlsx\", but instead as \"planning regions\". As a result, CT is not in my sample. As a final step, clean up on that and augment the \"CBSA\" file with CT. Check if any other states has this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "50df2366-7603-4a3d-83d1-5beb76179376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "try:\n",
    "    del(FUN_proc_name)\n",
    "except:\n",
    "    pass\n",
    "import FUN_proc_name\n",
    "importlib.reload(FUN_proc_name)\n",
    "from FUN_proc_name import FUN_proc_name\n",
    "\n",
    "try:\n",
    "    del(FUN_GetQ_byPlacement)\n",
    "except:\n",
    "    pass\n",
    "import FUN_GetQ_byPlacement\n",
    "importlib.reload(FUN_GetQ_byPlacement)\n",
    "from FUN_GetQ_byPlacement import FUN_GetQ_byPlacement\n",
    "\n",
    "try:\n",
    "    del(FUN_GetQ_byUsageBB)\n",
    "except:\n",
    "    pass\n",
    "import FUN_GetQ_byUsageBB\n",
    "importlib.reload(FUN_GetQ_byUsageBB)\n",
    "from FUN_GetQ_byUsageBB import FUN_GetQ_byUsageBB\n",
    "\n",
    "try:\n",
    "    del(FUN_GetQ_byUsageGeneral)\n",
    "except:\n",
    "    pass\n",
    "import FUN_GetQ_byUsageGeneral\n",
    "importlib.reload(FUN_GetQ_byUsageGeneral)\n",
    "from FUN_GetQ_byUsageGeneral import FUN_GetQ_byUsageGeneral\n",
    "\n",
    "try:\n",
    "    del(FUN_GetQ_byUsageMain)\n",
    "except:\n",
    "    pass\n",
    "import FUN_GetQ_byUsageMain\n",
    "importlib.reload(FUN_GetQ_byUsageMain)\n",
    "from FUN_GetQ_byUsageMain import FUN_GetQ_byUsageMain\n",
    "\n",
    "try:\n",
    "    del(FUN_GetQ_byIssuerType)\n",
    "except:\n",
    "    pass\n",
    "import FUN_GetQ_byIssuerType\n",
    "importlib.reload(FUN_GetQ_byIssuerType)\n",
    "from FUN_GetQ_byIssuerType import FUN_GetQ_byIssuerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d19d8-6052-4e8c-ba46-aa079e3f5be4",
   "metadata": {},
   "source": [
    "# 1. SDC Global Public Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eaaaf7-8481-4007-883e-e0d249a77cec",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- In 2020 and onwards, there are many bonds with yield below 1%, and also lower than NIC (when that field is available). Why? And is this pattern limited to very short-term bonds?\n",
    "- There is a regime change in 1984: Before that NIC is more complete, but after that (including that year) price/yield is more complete. Note that for some reason, NIC is consistently higher than yield.\n",
    "- SDC data on yield do not seem to be of high quality. For example,\n",
    "    - For the offer by CUSIP 357010 on 07/15/2003 (https://emma.msrb.org/MS209450-MS184758-MD358401.pdf), there are multiple maturities with different interest rates, but SDC only records one of them.\n",
    "    - SDC often misses bonds that repay principal ahead of time, instead of making a balloon payment at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072de13b-cff4-4338-b064-3ab04606efd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "###############\n",
    "# Import data #\n",
    "###############\n",
    "\n",
    "%run -i SCRIPT_import_GPF.py\n",
    "\n",
    "# Divide the \"Lead Manager\" field into many subfields\n",
    "# Note that \"Co-Managers\" have other manager information, which is not utilized here\n",
    "new_columns = GPF['Lead Manager'].str.split('\\n', expand=True)\n",
    "raw_name_GPF_colnames = ['raw_name_GPF_'+str(column) for column in new_columns.columns]\n",
    "new_columns.columns = raw_name_GPF_colnames\n",
    "GPF = pd.concat([GPF,new_columns],axis=1)\n",
    "# Modify mistakes in sale time\n",
    "threshold_date = pd.to_datetime('2050-01-01')\n",
    "GPF['Sale\\nDate'] = GPF['Sale\\nDate'].apply(lambda x: x - pd.DateOffset(years=100) if x > threshold_date else x)\n",
    "# Add a year\n",
    "GPF['sale_year'] = None\n",
    "GPF['sale_year'] = pd.to_datetime(GPF['Sale\\nDate']).dt.year\n",
    "\n",
    "# strip all the columns of blanks\n",
    "columns = [item.strip() for item in GPF.columns]\n",
    "GPF.columns = columns\n",
    "# Choose columns to keep & reorder columns\n",
    "GPF = GPF[[\n",
    "    \"Amount\\n   of   \\n Issue  \\n($ mils)\",\n",
    "    \"Amount\\n   of   \\nMaturity\\n($ mils)\",\n",
    "    \"Bid\",\n",
    "    \"Bk \\n Elig\",\n",
    "    \"Bond\\nBuyer\\nUOP.1\",\n",
    "    \"Call\\nIssue\",\n",
    "    \"County\",\n",
    "    \"Coupon Maturity\",\n",
    "    \"Coupon Type.1\",\n",
    "    \"Coupon\\n   of\\nMaturity\",\n",
    "    \"Financial Advisor.1\",\n",
    "    \"Financial Advisor.2\",\n",
    "    \"Financial\\nAdvisor\\nDeal(Y/N)\",\n",
    "    \"Fitch\\nInsured\\nLong Term\\nRating\",\n",
    "    \"Fitch\\nInsured\\nShort Term\\nRating\",\n",
    "    \"General Use of Proceeds\",\n",
    "    \"Gross\\nSpread\",\n",
    "    \"Insured\\nAmount\",\n",
    "    \"Issuer Type\\nDescription\",\n",
    "    \"Issuer\",\n",
    "    \"Issuer\\nType\",\n",
    "    \"Lead Manager\",\n",
    "    \"Main Use of Proceeds\",\n",
    "    \"Maturity Amount\",\n",
    "    \"Maturity Date\",\n",
    "    \"Maturity\",\n",
    "    \"Maturity\\n  Year\",\n",
    "    \"Moody's\\nInsured\\nLong Term\\nRating\",\n",
    "    \"Moody's\\nInsured\\nShort Term\\nRating\",\n",
    "    \"Net\\nInterest\\n  Cost\",\n",
    "    \"Price/\\n Yield\\n  of\\nMaturity\",\n",
    "    \"Sale\\nDate\",\n",
    "    \"Security\\n  Type\",\n",
    "    \"State\",\n",
    "    \"Taxable\\n Code\",\n",
    "    \"Yield Amount\",\n",
    "    \"sale_year\",\n",
    "    ]+raw_name_GPF_colnames]\n",
    "GPF['County_raw'] = GPF['County']\n",
    "# Format county\n",
    "GPF['County'] = GPF['County'].str.upper()\n",
    "GPF['County'] = GPF['County'].replace(' COUNTY','')\n",
    "GPF = GPF.rename(columns={\n",
    "    \"Amount\\n   of   \\n Issue  \\n($ mils)\":\"amount\",\n",
    "    \"Amount\\n   of   \\nMaturity\\n($ mils)\":\"amount_by_maturity\",\n",
    "    \"Bk \\n Elig\":\"CB_Eligible\",\n",
    "    \"Bond\\nBuyer\\nUOP.1\":\"use_of_proceeds_BB\",\n",
    "    \"Call\\nIssue\":\"if_callable\",\n",
    "    \"Coupon Maturity\":\"TOM_coupon_rate\",\n",
    "    \"Coupon Type.1\":\"coupon_type\",\n",
    "    \"Coupon\\n   of\\nMaturity\":\"coupon_rate\",\n",
    "    \"Financial Advisor.1\":\"advisor_short\",\n",
    "    \"Financial Advisor.2\":\"advisor_long\",\n",
    "    \"Financial\\nAdvisor\\nDeal(Y/N)\":\"if_advisor\",\n",
    "    \"Fitch\\nInsured\\nLong Term\\nRating\":\"Fitch_ILTR\",\n",
    "    \"Fitch\\nInsured\\nShort Term\\nRating\":\"Fitch_ISTR\",\n",
    "    \"General Use of Proceeds\":\"use_of_proceeds_general\",\n",
    "    \"Gross\\nSpread\":\"gross_spread\",\n",
    "    \"Insured\\nAmount\":\"insured_amount\",\n",
    "    \"Issuer Type\\nDescription\":\"issuer_type_full\",\n",
    "    \"Issuer\\nType\":\"issuer_type\",\n",
    "    \"Lead Manager\":\"lead_manager\",\n",
    "    \"Main Use of Proceeds\":\"use_of_proceeds_main\",\n",
    "    \"Maturity Amount\":\"TOM_amount_by_maturity\",\n",
    "    \"Maturity Date\":\"TOM_maturity_date\",\n",
    "    \"Maturity\\n  Year\":\"maturity_date\", # Somehow this rather than \"Maturity\" is more often non-missing\n",
    "    \"Moody's\\nInsured\\nLong Term\\nRating\":\"Moodys_ILTR\",\n",
    "    \"Moody's\\nInsured\\nShort Term\\nRating\":\"Moodys_ISTR\",\n",
    "    \"Net\\nInterest\\n  Cost\":\"net_interest_cost\",\n",
    "    \"Price/\\n Yield\\n  of\\nMaturity\":\"price_or_yield\",\n",
    "    \"Sale\\nDate\":\"sale_date\",\n",
    "    \"Security\\n  Type\":\"security_type\",\n",
    "    \"Taxable\\n Code\":\"taxable_code\",\n",
    "    \"Yield Amount\":\"TOM_price_or_yield\",\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f0cdcc-3392-469c-8aa7-49046fc3807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Pre-process data for yield calculation #\n",
    "##########################################\n",
    "\n",
    "# Weighted average of yield across all maturities\n",
    "GPF['avg_yield'] = None\n",
    "GPF['yield_by_maturity'] = None\n",
    "\n",
    "# Weighted average maturity\n",
    "GPF['avg_maturity'] = None\n",
    "GPF['all_maturity'] = None\n",
    "\n",
    "# Amount by maturity\n",
    "GPF['all_amount'] = None\n",
    "\n",
    "# Note that coupon type is tricky: I do not have a Thompson-researched version of coupon type, so it is tricky if I want to\n",
    "# have a bond-level variable \"coupon type\". Therefore, I retain the original \"coupon_type\" variable, and determine if an issue\n",
    "# is \"all\" fixed-rate or \"any\" variable by checking that variable\n",
    "\n",
    "# Whether it can be determined if the record is price or yield\n",
    "GPF['IF_price_or_yield_determined'] = None\n",
    "# Whether the number of tranches match in the coupon versus in the price/yield versus in the maturity field\n",
    "GPF['IF_n_tranches_not_match'] = None\n",
    "# Whether the yield data is collected from the \"net interest cost\" field. To adjust systematic differences (possibly) when \n",
    "# yield data is pulled from NIC direclty, add this as a fixed effect, and also interact it with time\n",
    "GPF['IF_yield_from_NIC'] = False\n",
    "\n",
    "GPF = GPF.reset_index(drop=True)\n",
    "\n",
    "############################\n",
    "# Handle exceptional cases #\n",
    "############################\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    # (1) Assume that bond is sold at par if \"price_or_yield\" is missing, conditional on that \"net interest cost\" is missing. If\n",
    "    # instead \"net interest cost\" is not missing, the priority would be to get yield from there. So should not stipulate \n",
    "    # \"price_or_yield\" here or the indicator variable \"IF_has_price_or_yield\" will be confued. Note that cases with multiple\n",
    "    # tranches are also handled here. Also note that this is done for the Thompson-researched version as well\n",
    "\n",
    "    # This step is disabled: Not sure if bond is really issued at par if there is no price/yield data. Out of conservativeness,\n",
    "    # do not make the imputation\n",
    "    \n",
    "    if False:\n",
    "        for idx,row in GPF.iterrows():\n",
    "            IF_has_net_interest_cost = \\\n",
    "                (isinstance(row['net_interest_cost'],float) or isinstance(row['net_interest_cost'],int)) and \\\n",
    "                row['net_interest_cost']!=None and \\\n",
    "                str(row['net_interest_cost'])!='nan' and \\\n",
    "                str(row['net_interest_cost'])!='None' and \\\n",
    "                'None' not in str(row['net_interest_cost'])\n",
    "            if (row['price_or_yield']==None or str(row['price_or_yield'])=='nan') and (not IF_has_net_interest_cost):\n",
    "                if '\\n' not in str(row['coupon_rate']):\n",
    "                    GPF.loc[idx,'price_or_yield'] = 100\n",
    "                else:\n",
    "                    price_or_yield = ''\n",
    "                    for tranch in range(0,row['coupon_rate'].count('\\n')):\n",
    "                        price_or_yield = price_or_yield+'100\\n'\n",
    "                    price_or_yield = price_or_yield+'100'\n",
    "                    GPF.loc[idx,'price_or_yield'] = price_or_yield\n",
    "            if (row['TOM_price_or_yield']==None or str(row['TOM_price_or_yield'])=='nan') and (not IF_has_net_interest_cost):\n",
    "                if '\\n' not in str(row['TOM_coupon_rate']):\n",
    "                    GPF.loc[idx,'TOM_price_or_yield'] = 100\n",
    "                else:\n",
    "                    price_or_yield = ''\n",
    "                    for tranch in range(0,row['TOM_coupon_rate'].count('\\n')):\n",
    "                        price_or_yield = price_or_yield+'100\\n'\n",
    "                    price_or_yield = price_or_yield+'100'\n",
    "                    GPF.loc[idx,'TOM_price_or_yield'] = price_or_yield\n",
    "\n",
    "    # (2) Handle zero-coupon cases. If \"Zero Coupon\" is one of the coupon types, there are other coupon types, and the number of\n",
    "    # coupon rates is exactly \"total number of maturities\" minus \"number of zero coupon bonds\", fill in the places of zero coupon\n",
    "    # bonds to have a coupon rate of 0. Note that I do not do this for the Thompson-researched version of data, as the \"coupon\n",
    "    # type\" variable is not available there\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "    \n",
    "        coupon_rate_filled = []\n",
    "        zero_coupon_idxes = []\n",
    "        non_zero_coupon_idxes = []\n",
    "        \n",
    "        coupon_type_original = str(row['coupon_type'])\n",
    "        coupon_type = str(row['coupon_type']).split('\\n')\n",
    "        coupon_rate = row['coupon_rate']\n",
    "    \n",
    "        IF_has_coupon_rate = \\\n",
    "            row['coupon_rate']!=None and \\\n",
    "            str(row['coupon_rate'])!='nan' \\\n",
    "            and str(row['coupon_rate'])!='None'\n",
    "        \n",
    "        # Handle cases where just bond\n",
    "        if coupon_type_original==\"Zero Coupon\":\n",
    "            GPF.at[idx,'coupon_rate'] = 0\n",
    "            \n",
    "        # When there are multiple bonds. Note that no need to handle if there is just one bond and it is not zero coupon\n",
    "        elif len(coupon_type)>1 :\n",
    "            # Do not handle if no zero-coupon bond\n",
    "            if IF_has_coupon_rate and \"Zero Coupon\" in coupon_type:\n",
    "                zero_coupon_idxes = [index for index,item in enumerate(coupon_type) if item==\"Zero Coupon\"]\n",
    "                non_zero_coupon_idxes = [index for index,item in enumerate(coupon_type) if item!=\"Zero Coupon\"]\n",
    "                coupon_rate = str(coupon_rate).split('\\n')\n",
    "                if len(coupon_type)==len(coupon_rate)+len(zero_coupon_idxes):\n",
    "                    coupon_rate_filled = [' ']*len(coupon_type)\n",
    "                    physical_idx = 0\n",
    "                    for sub_idx in non_zero_coupon_idxes:\n",
    "                        coupon_rate_filled[sub_idx] = coupon_rate[physical_idx]\n",
    "                        physical_idx = physical_idx+1\n",
    "                    for sub_idx in zero_coupon_idxes:\n",
    "                        coupon_rate_filled[sub_idx] = \"0\"\n",
    "                    coupon_rate_new = coupon_rate_filled[0]\n",
    "                    for item in coupon_rate_filled[1:]:\n",
    "                        coupon_rate_new = coupon_rate_new+\"\\n\"+item\n",
    "                    GPF.at[idx,'coupon_rate'] = coupon_rate_new\n",
    "                    \n",
    "    # (3) Assume that the coupon rate applies to all maturities if there is one coupon rate but multiple tranches\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if row['coupon_rate']!=None and str(row['coupon_rate'])!='nan':\n",
    "            if '\\n' in str(row['maturity_date']):\n",
    "                if '\\n' not in str(row['coupon_rate']):\n",
    "                    coupon_rate = ''\n",
    "                    for tranch in range(0,row['maturity_date'].count('\\n')):\n",
    "                        coupon_rate = coupon_rate+str(row['coupon_rate'])+'\\n'\n",
    "                    coupon_rate = coupon_rate+str(row['coupon_rate'])+'\\n'\n",
    "                    GPF.loc[idx,'coupon_rate'] = coupon_rate\n",
    "    \n",
    "    # (4) Assume that the price/yield applies to all maturities if there is one price/yield rate but multiple tranches.\n",
    "    # Make this edit only if the maturities are all identical. Otherwise, it is more likely a data error and do not impute.\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if row['price_or_yield']!=None and str(row['price_or_yield'])!='nan':\n",
    "            if '\\n' in str(row['maturity_date']):\n",
    "                maturity_date = row['maturity_date'].split('\\n')\n",
    "                if_same = all(element == maturity_date[0] for element in maturity_date)\n",
    "                if if_same and ('\\n' not in str(row['price_or_yield'])):\n",
    "                    price_or_yield = ''\n",
    "                    for tranch in range(0,row['maturity_date'].count('\\n')):\n",
    "                        price_or_yield = price_or_yield+str(row['price_or_yield'])+'\\n'\n",
    "                    price_or_yield = price_or_yield+str(row['price_or_yield'])+'\\n'\n",
    "                    GPF.loc[idx,'price_or_yield'] = price_or_yield\n",
    "    \n",
    "    # (5) Take the value of \"Maturity\" to populate \"Maturity\\n  Year\" (i.e., \"maturity_date\") if the latter is missing\n",
    "    for idx,row in GPF.iterrows():\n",
    "        IF_has_maturity_date = \\\n",
    "            row['maturity_date']!=None and \\\n",
    "            str(row['maturity_date'])!='nan' and \\\n",
    "            str(row['maturity_date'])!='None' and \\\n",
    "            'None' not in str(row['maturity_date']) \n",
    "        if not IF_has_maturity_date:\n",
    "            GPF.at[idx,'maturity_date'] = row['Maturity']\n",
    "    \n",
    "    # (6) Remove if beginning or end of field is '\\n'\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['price_or_yield'],str):\n",
    "            if row['price_or_yield'][:1]=='\\n':\n",
    "                GPF.loc[idx,'price_or_yield'] = row['price_or_yield'][1:]\n",
    "            if row['price_or_yield'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'price_or_yield'] = row['price_or_yield'][:-1]\n",
    "        if isinstance(row['maturity_date'],str):\n",
    "            if row['maturity_date'][:1]=='\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][1:]\n",
    "            if row['maturity_date'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][:-1]\n",
    "        if isinstance(row['coupon_rate'],str):\n",
    "            if row['coupon_rate'][:1]=='\\n':\n",
    "                GPF.loc[idx,'coupon_rate'] = row['coupon_rate'][1:]\n",
    "            if row['coupon_rate'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'coupon_rate'] = row['coupon_rate'][:-1]\n",
    "        if isinstance(row['amount_by_maturity'],str):\n",
    "            if row['amount_by_maturity'][:1]=='\\n':\n",
    "                GPF.loc[idx,'amount_by_maturity'] = row['amount_by_maturity'][1:]\n",
    "            if row['amount_by_maturity'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'amount_by_maturity'] = row['amount_by_maturity'][:-1]\n",
    "        if isinstance(row['TOM_price_or_yield'],str):\n",
    "            if row['TOM_price_or_yield'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_price_or_yield'] = row['TOM_price_or_yield'][1:]\n",
    "            if row['TOM_price_or_yield'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_price_or_yield'] = row['TOM_price_or_yield'][:-1]\n",
    "        if isinstance(row['TOM_maturity_date'],str):\n",
    "            if row['TOM_maturity_date'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_maturity_date'] = row['TOM_maturity_date'][1:]\n",
    "            if row['TOM_maturity_date'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_maturity_date'] = row['TOM_maturity_date'][:-1]\n",
    "        if isinstance(row['TOM_coupon_rate'],str):\n",
    "            if row['TOM_coupon_rate'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_coupon_rate'] = row['TOM_coupon_rate'][1:]\n",
    "            if row['TOM_coupon_rate'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_coupon_rate'] = row['TOM_coupon_rate'][:-1]\n",
    "        if isinstance(row['TOM_amount_by_maturity'],str):\n",
    "            if row['TOM_amount_by_maturity'][:1]=='\\n':\n",
    "                GPF.loc[idx,'TOM_amount_by_maturity'] = row['TOM_amount_by_maturity'][1:]\n",
    "            if row['TOM_amount_by_maturity'][-1:]=='\\n':\n",
    "                GPF.loc[idx,'TOM_amount_by_maturity'] = row['TOM_amount_by_maturity'][:-1]\n",
    "    \n",
    "    # (7) Remove if beginning or end or middle of field \"maturity_date\" is 'None'\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['maturity_date'],str):\n",
    "            if row['maturity_date'][:5]=='None\\n':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][5:]\n",
    "            if row['maturity_date'][-5:]=='\\nNone':\n",
    "                GPF.loc[idx,'maturity_date'] = row['maturity_date'][:-5]\n",
    "            GPF.at[idx,'maturity_date'] = GPF.at[idx,'maturity_date'].replace('None\\n','')\n",
    "    \n",
    "    # (8) After the prior step, there are cases where \"\\n\" is not in \"maturity_date\" and \"maturity_date\" is a string. To avoid\n",
    "    # incompatibility, convert type\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if isinstance(row['maturity_date'],str) and '\\n' not in row['maturity_date'] and\\\n",
    "            row['maturity_date']!=None and str(row['maturity_date'])!='nan' and str(row['maturity_date'])!='None':\n",
    "            GPF.at[idx,'maturity_date'] = datetime.strptime(GPF.at[idx,'maturity_date'],\"%m/%d/%y\")\n",
    "    \n",
    "    # (9) Drop if two or three \"\\n\" come adjacent\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if '\\n\\n' in str(row['price_or_yield']):\n",
    "            GPF.at[idx,'price_or_yield'] = GPF.at[idx,'price_or_yield'].replace('\\n\\n','\\n')\n",
    "        if '\\n\\n\\n' in str(row['price_or_yield']):\n",
    "            GPF.at[idx,'price_or_yield'] = GPF.at[idx,'price_or_yield'].replace('\\n\\n\\n','\\n')\n",
    "        if '\\n\\n' in str(row['coupon_rate']):\n",
    "            GPF.at[idx,'coupon_rate'] = GPF.at[idx,'coupon_rate'].replace('\\n\\n','\\n')\n",
    "        if '\\n\\n\\n' in str(row['coupon_rate']):\n",
    "            GPF.at[idx,'coupon_rate'] = GPF.at[idx,'coupon_rate'].replace('\\n\\n\\n','\\n')\n",
    "\n",
    "    # (10) Put value of data from the non-Thompson-researched version to the Thompson-researched version, if the latter is missing.\n",
    "    # For consistency, make this change for all related fields, or do not do so at all\n",
    "    for idx,row in GPF.iterrows():\n",
    "        if \\\n",
    "            (row['TOM_maturity_date']==None or \\\n",
    "            str(row['TOM_maturity_date'])=='nan' or \\\n",
    "            str(row['TOM_maturity_date'])=='None' or \\\n",
    "            'None' in str(row['TOM_maturity_date'])) \\\n",
    "            and \\\n",
    "            (row['TOM_coupon_rate']==None or \\\n",
    "            str(row['TOM_coupon_rate'])=='nan' \\\n",
    "            or str(row['TOM_coupon_rate'])=='None')\\\n",
    "            and \\\n",
    "            (row['TOM_price_or_yield']==None or \\\n",
    "            str(row['TOM_price_or_yield'])=='nan' \\\n",
    "            or str(row['TOM_price_or_yield'])=='None') \\\n",
    "            and \\\n",
    "            (row['TOM_amount_by_maturity']!=None or \\\n",
    "            str(row['TOM_amount_by_maturity'])!='nan' or \\\n",
    "            str(row['TOM_amount_by_maturity'])!='None' or \\\n",
    "            'None' not in str(row['TOM_amount_by_maturity'])):\n",
    "            GPF.at[idx,'TOM_maturity_date'] = row['maturity_date']\n",
    "            GPF.at[idx,'TOM_coupon_rate'] = row['coupon_rate']\n",
    "            GPF.at[idx,'TOM_price_or_yield'] = row['price_or_yield']\n",
    "            GPF.at[idx,'TOM_amount_by_maturity'] = row['amount_by_maturity']\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52025095-7852-48f9-ad07-dc2798c09f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "###############################\n",
    "# Calculate yield at issuance #\n",
    "###############################\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    # Handle case by case of each variable being missing, and within each case allow for multiple maturities\n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Initialize variables\n",
    "        IF_has_net_interest_cost = False\n",
    "        IF_has_maturity_date = False\n",
    "        IF_has_coupon_rate = False\n",
    "        IF_has_price_or_yield = False\n",
    "        IF_has_amount_by_maturity = False\n",
    "\n",
    "        # Determine whether to use the Thompson-researched version of data. Note that below can be redundant, especially after \n",
    "        # I decide Thompson-researched version is better and put non-Thompson data to those fields if Thompson-researched data\n",
    "        # are missing. I retain this structure as a legacy and because it does not create real problems\n",
    "        if row['sale_year']<2003:\n",
    "            COL_maturity_date = 'maturity_date'\n",
    "            COL_coupon_rate = 'coupon_rate'\n",
    "            COL_price_or_yield = 'price_or_yield'\n",
    "            COL_amount_by_maturity = 'amount_by_maturity'\n",
    "        else:\n",
    "            COL_maturity_date = 'TOM_maturity_date'\n",
    "            COL_coupon_rate = 'TOM_coupon_rate'\n",
    "            COL_price_or_yield = 'TOM_price_or_yield'\n",
    "            COL_amount_by_maturity = 'TOM_amount_by_maturity'\n",
    "        \n",
    "        # Whether certain fields exist\n",
    "        IF_has_maturity_date = \\\n",
    "            row[COL_maturity_date]!=None and \\\n",
    "            str(row[COL_maturity_date])!='nan' and \\\n",
    "            str(row[COL_maturity_date])!='None' and \\\n",
    "            'None' not in str(row[COL_maturity_date]) \n",
    "        IF_has_coupon_rate = \\\n",
    "            row[COL_coupon_rate]!=None and \\\n",
    "            str(row[COL_coupon_rate])!='nan' \\\n",
    "            and str(row[COL_coupon_rate])!='None'\n",
    "        IF_has_price_or_yield = \\\n",
    "            row[COL_price_or_yield]!=None and \\\n",
    "            str(row[COL_price_or_yield])!='nan' \\\n",
    "            and str(row[COL_price_or_yield])!='None'\n",
    "        IF_has_amount_by_maturity = \\\n",
    "            row[COL_amount_by_maturity]!=None and \\\n",
    "            str(row[COL_amount_by_maturity])!='nan' and \\\n",
    "            str(row[COL_amount_by_maturity])!='None' and \\\n",
    "            'None' not in str(row[COL_amount_by_maturity]) \n",
    "        IF_has_net_interest_cost = \\\n",
    "            (isinstance(row['net_interest_cost'],float) or isinstance(row['net_interest_cost'],int)) and \\\n",
    "            row['net_interest_cost']!=None and \\\n",
    "            str(row['net_interest_cost'])!='nan' and \\\n",
    "            str(row['net_interest_cost'])!='None' and \\\n",
    "            'None' not in str(row['net_interest_cost'])\n",
    "\n",
    "        # Number of entries in certain fields\n",
    "        N_coupon_rate = str(row[COL_coupon_rate]).count('\\n')+1\n",
    "        N_price_or_yield = str(row[COL_price_or_yield]).count('\\n')+1\n",
    "        N_maturity_date = str(row[COL_maturity_date]).count('\\n')+1\n",
    "        N_amount = str(row[COL_amount_by_maturity]).count('\\n')+1\n",
    "        IF_num_bonds_all_consistent = \\\n",
    "            (N_coupon_rate==N_price_or_yield) and \\\n",
    "            (N_coupon_rate==N_maturity_date) and \\\n",
    "            (N_coupon_rate==N_amount)\n",
    "        # The following indicator can be applied when coupon is not available\n",
    "        IF_num_bonds_yield_mat_amt_consistent = \\\n",
    "            (N_price_or_yield==N_maturity_date) and \\\n",
    "            (N_price_or_yield==N_amount)\n",
    "        # The following indicator can be applied when coupon and yield/price is not available\n",
    "        IF_num_bonds_mat_amt_consistent = \\\n",
    "            (N_maturity_date==N_amount)\n",
    "\n",
    "        ##########\n",
    "        # Case 1 #\n",
    "        ##########\n",
    "\n",
    "        # Case 1: \"coupon_rate\",\"maturity_date\",\"price_or_yield\" are all available\n",
    "        if IF_has_maturity_date and IF_has_coupon_rate and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and IF_num_bonds_all_consistent:\n",
    "    \n",
    "            # Case 1A: If single maturity\n",
    "            if N_maturity_date==1 :\n",
    "                maturity = (row[COL_maturity_date]-row['sale_date']).days\n",
    "                GPF.loc[idx,'avg_maturity'] = maturity\n",
    "                GPF.loc[idx,'all_maturity'] = [maturity]\n",
    "                GPF.loc[idx,'all_amount'] = [row[COL_amount_by_maturity]]\n",
    "                # Assume that if a number is more than 80 and less than 120, it is issuing price. If less than 20, it is issuing \n",
    "                # yield. Otherwise, undetermined\n",
    "                if float(row[COL_price_or_yield])<20:\n",
    "                    GPF.loc[idx,'avg_yield'] = row[COL_price_or_yield]/100\n",
    "                    GPF.loc[idx,'yield_by_maturity'] = [GPF.loc[idx,'avg_yield']]\n",
    "                elif float(row[COL_price_or_yield])>80 and float(row[COL_price_or_yield])<120:\n",
    "                    # Number of coupons to be paid\n",
    "                    n_coupon = round(maturity/182)\n",
    "                    GPF.loc[idx,'avg_yield'] = \\\n",
    "                        (1+npf.irr([-row[COL_price_or_yield]]+[row[COL_coupon_rate]/2]*(n_coupon-1)+[100+row[COL_coupon_rate]/2]))\\\n",
    "                        **2-1\n",
    "                    GPF.loc[idx,'yield_by_maturity'] = [GPF.loc[idx,'avg_yield']]\n",
    "                else:\n",
    "                    GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "    \n",
    "            # Case 1B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_coupon_rate:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = False\n",
    "                elif N_price_or_yield!=N_maturity_date:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = False\n",
    "                else:\n",
    "                    maturities = []\n",
    "                    yields = []\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row[COL_coupon_rate].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row[COL_maturity_date].split('\\n')[tranch],\"%m/%d/%y\")\\\n",
    "                            -row['sale_date']).days\n",
    "                        coupon_rate = float(row[COL_coupon_rate].split('\\n')[tranch])\n",
    "                        price_or_yield = float(row[COL_price_or_yield].split('\\n')[tranch])\n",
    "                        n_coupon = round(maturity/182)\n",
    "                        amount = float(row[COL_amount_by_maturity].split('\\n')[tranch].replace(',',''))\n",
    "                        maturities = maturities+[maturity]\n",
    "                        amounts = amounts+[amount]\n",
    "                        if price_or_yield<20:\n",
    "                            yields = yields+[price_or_yield/100]\n",
    "                        elif price_or_yield>80 and price_or_yield<120:\n",
    "                            tranch_yield = (1+npf.irr([-price_or_yield]+[coupon_rate/2]*(n_coupon-1)+[100+coupon_rate/2]))**2-1\n",
    "                            yields = yields+[tranch_yield]\n",
    "                        else:\n",
    "                            yields = yields+[None]\n",
    "                            GPF.loc[idx,'IF_price_or_yield_determined'] = False\n",
    "                    if GPF.at[idx,'IF_price_or_yield_determined']!=False:\n",
    "                        GPF.loc[idx,'avg_yield'] = np.dot(yields,amounts)/np.sum(amounts)\n",
    "                        GPF.loc[idx,'avg_maturity'] = np.dot(maturities,amounts)/np.sum(amounts)\n",
    "                    GPF.at[idx,'all_maturity'] = maturities\n",
    "                    GPF.at[idx,'yield_by_maturity'] = yields\n",
    "                    GPF.at[idx,'all_amount'] = amounts\n",
    "\n",
    "        ##########\n",
    "        # Case 2 #\n",
    "        ##########          \n",
    "\n",
    "        # Case 2: \"coupon_rate\" is not available, but \"price_or_yield\" and \"maturity_date\" is\n",
    "        elif IF_has_maturity_date and (not IF_has_coupon_rate) and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and IF_num_bonds_yield_mat_amt_consistent:\n",
    "    \n",
    "            # Case 2A: If single maturity\n",
    "            if N_maturity_date==1:\n",
    "                maturity = (row[COL_maturity_date]-row['sale_date']).days\n",
    "                GPF.loc[idx,'avg_maturity'] = maturity\n",
    "                GPF.loc[idx,'all_maturity'] = [maturity]\n",
    "                GPF.loc[idx,'all_amount'] = [row[COL_amount_by_maturity]]\n",
    "                # Assume that if a number is more than 80, it is issuing price. If less than 20, it is issuing yield. \n",
    "                # Otherwise, undetermined\n",
    "                if float(row[COL_price_or_yield])<20:\n",
    "                    GPF.loc[idx,'avg_yield'] = row[COL_price_or_yield]/100\n",
    "                    GPF.loc[idx,'yield_by_maturity'] = [GPF.loc[idx,'avg_yield']]\n",
    "                # Cannot do anything if coupon rate is unavailable and only price is given\n",
    "                elif row[COL_price_or_yield]>80:\n",
    "                    continue\n",
    "    \n",
    "            # Case 2B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_maturity_date:\n",
    "                    GPF.loc[idx,'IF_n_tranches_not_match'] = False\n",
    "                else:\n",
    "                    maturities = []\n",
    "                    yields = []\n",
    "                    amounts = []\n",
    "                    for tranch in range(0,row[COL_maturity_date].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row[COL_maturity_date].split('\\n')[tranch],\"%m/%d/%y\")-row['sale_date']).days\n",
    "                        price_or_yield = float(row[COL_price_or_yield].split('\\n')[tranch])\n",
    "                        amount = float(row[COL_amount_by_maturity].split('\\n')[tranch].replace(',',''))\n",
    "                        maturities = maturities+[maturity]\n",
    "                        amounts = amounts+[amount]\n",
    "                        if price_or_yield<20:\n",
    "                            yields = yields+[price_or_yield/100]\n",
    "                        else:\n",
    "                            # Note that it is impossible to calculate yield if only price is available for one tranch, or if I cannot\n",
    "                            # decide whether it is price or yield\n",
    "                            yields = yields+[None]\n",
    "                    if len(yields)>0:\n",
    "                        if None not in yields:\n",
    "                            GPF.loc[idx,'avg_yield'] = np.dot(yields,amounts)/np.sum(amounts)\n",
    "                        else:\n",
    "                            GPF.loc[idx,'avg_yield'] = None\n",
    "                        GPF.loc[idx,'avg_maturity'] = np.dot(maturities,amounts)/np.sum(amounts)\n",
    "                    GPF.at[idx,'all_maturity'] = maturities\n",
    "                    GPF.at[idx,'yield_by_maturity'] = yields\n",
    "                    GPF.at[idx,'all_amount'] = amounts\n",
    "\n",
    "        ##########\n",
    "        # Case 3 #\n",
    "        ##########\n",
    "\n",
    "        # Case 3: When \"net interest cost\" is available, use it directly and do not compute yield myself\n",
    "        # Note that \"net interest cost\" tend to be populated in 1983 and before\n",
    "        elif IF_has_maturity_date and IF_has_net_interest_cost and IF_has_amount_by_maturity \\\n",
    "            and IF_num_bonds_mat_amt_consistent:\n",
    "            GPF.loc[idx,'IF_yield_from_NIC'] = True\n",
    "            GPF.loc[idx,'avg_yield'] = row['net_interest_cost']/100\n",
    "            # If single maturity\n",
    "            if N_maturity_date==1:\n",
    "                maturity = (row[COL_maturity_date]-row['sale_date']).days\n",
    "                GPF.loc[idx,'avg_maturity'] = maturity\n",
    "                GPF.at[idx,'all_maturity'] = [maturity]\n",
    "                GPF.loc[idx,'all_amount'] = [row[COL_amount_by_maturity]]\n",
    "            # If multiple maturity\n",
    "            else:\n",
    "                maturities = []\n",
    "                amounts = []\n",
    "                for tranch in range(0,row[COL_maturity_date].count('\\n')+1):\n",
    "                    maturity = (datetime.strptime(row[COL_maturity_date].split('\\n')[tranch],\"%m/%d/%y\")-row['sale_date']).days\n",
    "                    amount = float(row[COL_amount_by_maturity].split('\\n')[tranch].replace(',',''))\n",
    "                    maturities = maturities+[maturity]\n",
    "                    amounts = amounts+[amount]\n",
    "                if None not in maturities:\n",
    "                    GPF.loc[idx,'avg_maturity'] = np.mean(maturities)\n",
    "                else:\n",
    "                    GPF.loc[idx,'avg_maturity'] = None\n",
    "                GPF.at[idx,'all_maturity'] = maturities\n",
    "                GPF.at[idx,'all_amount'] = amounts\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "\n",
    "# Reorder columns\n",
    "first_columns = ['sale_date','net_interest_cost',\n",
    "    'maturity_date','coupon_rate','price_or_yield','amount_by_maturity',\n",
    "    'TOM_maturity_date','TOM_coupon_rate','TOM_price_or_yield','TOM_amount_by_maturity',\n",
    "    'avg_yield','avg_maturity','yield_by_maturity','all_maturity','all_amount',\n",
    "    'IF_price_or_yield_determined']\n",
    "GPF = GPF[first_columns+sorted([item for item in GPF.columns if item not in first_columns])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36495e2f-4f14-4993-a423-fe3c25fda1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate yield of synthetic risk-free treasury bond #\n",
    "########################################################\n",
    "\n",
    "# Notes:\n",
    "# (1) Without coupon rate, it is impossible to calculate the price (yield) of the synthetic treasury bond. For example, consider \n",
    "# Bond A: Pays $5 one year from now, and $5 two years from now, and Bond B: Pays $100*(1+5%)^2 two years from now. These two have\n",
    "# the same yield. Suppose that treasury yield in one year is 0% and in two years is 20%. Then price of first synthetic bond is\n",
    "# higher than the second, and the yield of the first synthetic bond is lower than the second. In other words, without coupon rate,\n",
    "# I do not know when the cash flow is going to come, so I do not know what is the component of risk-free rate that I should tease\n",
    "# out from the return of the municipal bond yield. An INACCURATE approximation can be simply using yield of municipal bond minus \n",
    "# that of treasury bond, but it is erraneous to do so.\n",
    "# \n",
    "# Luckily except for later parts of the sample, coupon rate is usually available.\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "    \n",
    "    # Treasury yield\n",
    "    feds200628 = pd.read_csv(\"../RawData/FedBOG/feds200628.csv\", header=9)\n",
    "    feds200628 = feds200628[~pd.isnull(feds200628['SVENY01'])]\n",
    "    columns = ['Date']+ \\\n",
    "        ['SVENY0'+str(i) for i in range(1,10)]+ \\\n",
    "        ['SVENY'+str(i) for i in range(10,31)]\n",
    "    feds200628 = feds200628[columns]\n",
    "    new_columns = ['Date']+ \\\n",
    "        ['SVENY'+str(i) for i in range(1,10)]+ \\\n",
    "        ['SVENY'+str(i) for i in range(10,31)]\n",
    "    feds200628.columns = new_columns\n",
    "    feds200628['Date'] = pd.to_datetime(feds200628['Date'])\n",
    "    threshold_date = pd.to_datetime('2050-01-01')\n",
    "    feds200628['Date'] = feds200628['Date'].apply(lambda x: x - pd.DateOffset(years=100) if x > threshold_date else x)\n",
    "    \n",
    "    max_year_7 = [pd.Timestamp(1961,6,14,0,0,0),pd.Timestamp(1971,8,15,0,0,0)]\n",
    "    max_year_10 = [pd.Timestamp(1971,8,16,0,0,0),pd.Timestamp(1971,11,14,0,0,0)]\n",
    "    max_year_15 = [pd.Timestamp(1971,11,15,0,0,0),pd.Timestamp(1981,7,1,0,0,0)]\n",
    "    max_year_20 = [pd.Timestamp(1981,7,2,0,0,0),pd.Timestamp(1985,11,24,0,0,0)]\n",
    "    max_year_30 = [pd.Timestamp(1985,11,25,0,0,0),pd.Timestamp(2023,11,3,0,0,0)]\n",
    "    \n",
    "    GPF['sync_bond_yield_by_maturity'] = None\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Initialize variables\n",
    "        IF_has_net_interest_cost = False\n",
    "        IF_has_maturity_date = False\n",
    "        IF_has_coupon_rate = False\n",
    "        IF_has_price_or_yield = False\n",
    "        IF_has_amount_by_maturity = False\n",
    "        \n",
    "        # Determine whether to use the Thompson-researched version of data\n",
    "        if row['sale_year']<2003:\n",
    "            COL_maturity_date = 'maturity_date'\n",
    "            COL_coupon_rate = 'coupon_rate'\n",
    "            COL_price_or_yield = 'price_or_yield'\n",
    "            COL_amount_by_maturity = 'amount_by_maturity'\n",
    "        else:\n",
    "            COL_maturity_date = 'TOM_maturity_date'\n",
    "            COL_coupon_rate = 'TOM_coupon_rate'\n",
    "            COL_price_or_yield = 'TOM_price_or_yield'\n",
    "            COL_amount_by_maturity = 'TOM_amount_by_maturity'\n",
    "    \n",
    "        # Continue if coupon rate is missing\n",
    "        IF_has_coupon_rate = \\\n",
    "            row[COL_coupon_rate]!=None and \\\n",
    "            str(row[COL_coupon_rate])!='nan' \\\n",
    "            and str(row[COL_coupon_rate])!='None'\n",
    "        IF_has_maturity_date = \\\n",
    "            row[COL_maturity_date]!=None and \\\n",
    "            str(row[COL_maturity_date])!='nan' and \\\n",
    "            str(row[COL_maturity_date])!='None' and \\\n",
    "            'None' not in str(row[COL_maturity_date]) \n",
    "        IF_has_price_or_yield = \\\n",
    "            row[COL_price_or_yield]!=None and \\\n",
    "            str(row[COL_price_or_yield])!='nan' \\\n",
    "            and str(row[COL_price_or_yield])!='None'\n",
    "        IF_has_amount_by_maturity = \\\n",
    "            row[COL_amount_by_maturity]!=None and \\\n",
    "            str(row[COL_amount_by_maturity])!='nan' and \\\n",
    "            str(row[COL_amount_by_maturity])!='None' and \\\n",
    "            'None' not in str(row[COL_amount_by_maturity]) \n",
    "        if (not IF_has_coupon_rate) or (not IF_has_maturity_date) or (not IF_has_price_or_yield) or (not IF_has_amount_by_maturity):\n",
    "            continue\n",
    "    \n",
    "        # Number of entries in certain fields\n",
    "        N_coupon_rate = str(row[COL_coupon_rate]).count('\\n')+1\n",
    "        N_price_or_yield = str(row[COL_price_or_yield]).count('\\n')+1\n",
    "        N_maturity_date = str(row[COL_maturity_date]).count('\\n')+1\n",
    "        N_amount = str(row[COL_amount_by_maturity]).count('\\n')+1\n",
    "        IF_num_bonds_all_consistent = \\\n",
    "            (N_coupon_rate==N_price_or_yield) and \\\n",
    "            (N_coupon_rate==N_maturity_date) and \\\n",
    "            (N_coupon_rate==N_amount)\n",
    "    \n",
    "        # Obtain the treasury zero-coupon yield curve at the closest date\n",
    "        feds200628_copy = feds200628.copy()\n",
    "        feds200628_copy['dif_date'] = np.abs(feds200628_copy['Date']-row['sale_date'])\n",
    "        feds200628_copy = feds200628_copy.sort_values('dif_date').reset_index()\n",
    "    \n",
    "        sync_bond_yield_by_maturity = []\n",
    "    \n",
    "        # If single maturity\n",
    "        if N_coupon_rate==1 and IF_num_bonds_all_consistent:\n",
    "    \n",
    "            if row[COL_price_or_yield]>20 and row[COL_price_or_yield]<80:\n",
    "                continue\n",
    "            \n",
    "            coupon_rate = float(row[COL_coupon_rate])\n",
    "            maturity = row['all_maturity'][0]\n",
    "    \n",
    "            cf = []\n",
    "            discount_factor = []\n",
    "            N_coupons = int(np.max([1,np.around(maturity/(365/2))]))\n",
    "    \n",
    "            # Determine if synthetic bond can be constructed. Cannot do so if the length of zero-coupon yields is not long enough\n",
    "            rf_available = False\n",
    "            if row['sale_date']>max_year_7[0] and row['sale_date']<=max_year_7[1] and maturity<=7*365:\n",
    "                rf_available = True\n",
    "            if row['sale_date']>max_year_10[0] and row['sale_date']<=max_year_10[1] and maturity<=10*365:\n",
    "                rf_available = True\n",
    "            if row['sale_date']>max_year_15[0] and row['sale_date']<=max_year_15[1] and maturity<=15*365:\n",
    "                rf_available = True\n",
    "            if row['sale_date']>max_year_20[0] and row['sale_date']<=max_year_20[1] and maturity<=20*365:\n",
    "                rf_available = True\n",
    "            if row['sale_date']>max_year_30[0] and row['sale_date']<=max_year_30[1] and maturity<=30*365:\n",
    "                rf_available = True\n",
    "    \n",
    "            if rf_available:\n",
    "                # Construct a series of cash flow for each bond\n",
    "                for cf_idx in range(0,N_coupons):\n",
    "                    cf = cf+[coupon_rate/2]\n",
    "                cf[N_coupons-1] = cf[N_coupons-1]+100\n",
    "        \n",
    "                # Construct a series of discount factor for each bond\n",
    "                for cf_idx in range(0,N_coupons):\n",
    "                    if cf_idx==0:\n",
    "                        discount_factor = discount_factor+[feds200628_copy['SVENY1'][0]]\n",
    "                    elif cf_idx%2==1:\n",
    "                        discount_factor = discount_factor+[feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]]\n",
    "                    elif cf_idx%2==0:\n",
    "                        discount_factor = discount_factor+\\\n",
    "                            [(feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]\n",
    "                            +feds200628_copy['SVENY'+str(ceil(cf_idx/2)+1)][0])/2]\n",
    "                discount_factor = [(1/(1+discount_factor[disc_idx]/100))**((disc_idx+1)/2) for disc_idx in range(0,N_coupons)]\n",
    "    \n",
    "                # Bond price and yield of synthetic bond\n",
    "                sync_bond_price = np.sum(np.dot(cf,discount_factor))\n",
    "                cf = [-sync_bond_price]+cf\n",
    "                sync_bond_yield = (1+npf.irr(cf))**2-1\n",
    "                sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[sync_bond_yield]\n",
    "    \n",
    "                # Record data\n",
    "                GPF.at[idx,'sync_bond_yield_by_maturity'] = sync_bond_yield_by_maturity\n",
    "    \n",
    "    \n",
    "        # If multiple maturity, go over bond by bond\n",
    "        if N_coupon_rate>1 and IF_num_bonds_all_consistent:\n",
    "    \n",
    "            for bond_idx in range(0,N_maturity_date):\n",
    "    \n",
    "                if (float(row[COL_price_or_yield].split('\\n')[bond_idx])>20) and \\\n",
    "                    (float(row[COL_price_or_yield].split('\\n')[bond_idx])<80):\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[None]\n",
    "                    continue\n",
    "    \n",
    "                coupon_rate = float(row[COL_coupon_rate].split('\\n')[bond_idx])\n",
    "                maturity = row['all_maturity'][bond_idx]\n",
    "        \n",
    "                cf = []\n",
    "                discount_factor = []\n",
    "                N_coupons = int(np.max([1,np.around(maturity/(365/2))]))\n",
    "        \n",
    "                # Determine if synthetic bond can be constructed. Cannot do so if the length of zero-coupon yields is not long enough\n",
    "                rf_available = False\n",
    "                if row['sale_date']>max_year_7[0] and row['sale_date']<=max_year_7[1] and maturity<=7*365:\n",
    "                    rf_available = True\n",
    "                if row['sale_date']>max_year_10[0] and row['sale_date']<=max_year_10[1] and maturity<=10*365:\n",
    "                    rf_available = True\n",
    "                if row['sale_date']>max_year_15[0] and row['sale_date']<=max_year_15[1] and maturity<=15*365:\n",
    "                    rf_available = True\n",
    "                if row['sale_date']>max_year_20[0] and row['sale_date']<=max_year_20[1] and maturity<=20*365:\n",
    "                    rf_available = True\n",
    "                if row['sale_date']>max_year_30[0] and row['sale_date']<=max_year_30[1] and maturity<=30*365:\n",
    "                    rf_available = True\n",
    "        \n",
    "                if rf_available:\n",
    "                    # Construct a series of cash flow for each bond\n",
    "                    for cf_idx in range(0,N_coupons):\n",
    "                        cf = cf+[coupon_rate/2]\n",
    "                    cf[N_coupons-1] = cf[N_coupons-1]+100\n",
    "            \n",
    "                    # Construct a series of discount factor for each bond\n",
    "                    for cf_idx in range(0,N_coupons):\n",
    "                        if cf_idx==0:\n",
    "                            discount_factor = discount_factor+[feds200628_copy['SVENY1'][0]]\n",
    "                        elif cf_idx%2==1:\n",
    "                            discount_factor = discount_factor+[feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]]\n",
    "                        elif cf_idx%2==0:\n",
    "                            discount_factor = discount_factor+\\\n",
    "                                [(feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]\n",
    "                                +feds200628_copy['SVENY'+str(ceil(cf_idx/2)+1)][0])/2]\n",
    "                    discount_factor = [(1/(1+discount_factor[disc_idx]/100))**((disc_idx+1)/2) for disc_idx in range(0,N_coupons)]\n",
    "        \n",
    "                    # Bond price and yield of synthetic bond\n",
    "                    sync_bond_price = np.sum(np.dot(cf,discount_factor))\n",
    "                    cf = [-sync_bond_price]+cf\n",
    "                    sync_bond_yield = (1+npf.irr(cf))**2-1\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[sync_bond_yield]\n",
    "                else:\n",
    "                    sync_bond_yield_by_maturity = sync_bond_yield_by_maturity+[None]\n",
    "        \n",
    "            GPF.at[idx,'sync_bond_yield_by_maturity'] = sync_bond_yield_by_maturity\n",
    "\n",
    "    return GPF\n",
    "\n",
    "\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d97a9b6-7296-40d8-a4a9-d55a4df4b2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# Calculate spread #\n",
    "####################\n",
    "\n",
    "# Calculate spread. Note that I calculate spread a bit differently from Li and Zhu: Theirs is from the perspective of a taxed\n",
    "# individual, while mine is from the perspective of a non-taxed individual\n",
    "\n",
    "tax_rate = {\n",
    "    1967:0.700,1968:0.700,1969:0.700,1970:0.700,1971:0.700,1972:0.700,1973:0.700,\n",
    "    1974:0.700,1975:0.700,1976:0.700,1977:0.700,1978:0.700,1979:0.700,1980:0.700,\n",
    "    1981:0.700,1982:0.500,1983:0.500,1984:0.500,1985:0.500,1986:0.500,1987:0.385,\n",
    "    1988:0.280,1989:0.280,1990:0.280,1991:0.310,1992:0.310,1993:0.396,1994:0.396,\n",
    "    1995:0.396,1996:0.396,1997:0.396,1998:0.396,1999:0.396,2000:0.396,2001:0.391,\n",
    "    2002:0.386,2003:0.350,2004:0.350,2005:0.350,2006:0.350,2007:0.350,2008:0.350,\n",
    "    2009:0.350,2010:0.350,2011:0.350,2012:0.350,2013:0.396,2014:0.396,2015:0.396,\n",
    "    2016:0.396,2017:0.396,2018:0.370,2019:0.370,2020:0.370,2021:0.370,2022:0.370,\n",
    "    2023:0.370,\n",
    "    }\n",
    "\n",
    "GPF = GPF.reset_index(drop=True)\n",
    "GPF['spread_by_maturity'] = None\n",
    "GPF['avg_spread'] = None\n",
    "\n",
    "def proc_list(GPF):\n",
    "\n",
    "    for idx,row in GPF.iterrows():\n",
    "        \n",
    "        spread_by_maturity = []\n",
    "        if row['sync_bond_yield_by_maturity']!=None and row['yield_by_maturity']!=None:\n",
    "            for bond_idx in range(0,len(GPF.at[idx,'sync_bond_yield_by_maturity'])):\n",
    "                if row['sync_bond_yield_by_maturity'][bond_idx]==None or \\\n",
    "                    row['yield_by_maturity'][bond_idx]==None:\n",
    "                    spread_by_maturity = spread_by_maturity+[None]\n",
    "                else:\n",
    "                    # Adjust for tax here\n",
    "                    if row['taxable_code']=='E':\n",
    "                        spread_by_maturity = spread_by_maturity+\\\n",
    "                            [row['yield_by_maturity'][bond_idx]\n",
    "                            -row['sync_bond_yield_by_maturity'][bond_idx]*(1-tax_rate[row['sale_year']])]\n",
    "                    elif row['taxable_code']=='A' or 'T':\n",
    "                        spread_by_maturity = spread_by_maturity+\\\n",
    "                            [row['yield_by_maturity'][bond_idx]*(1-tax_rate[row['sale_year']])\n",
    "                            -row['sync_bond_yield_by_maturity'][bond_idx]*(1-tax_rate[row['sale_year']])]\n",
    "            GPF.at[idx,'spread_by_maturity'] = spread_by_maturity\n",
    "\n",
    "        if GPF.at[idx,'spread_by_maturity']!=None:\n",
    "            if None not in GPF.at[idx,'spread_by_maturity']:\n",
    "                GPF.loc[idx,'avg_spread'] = \\\n",
    "                    np.dot(GPF.at[idx,'spread_by_maturity'],row['all_amount'])/np.sum(row['all_amount'])\n",
    "\n",
    "    return GPF\n",
    "\n",
    "meta_columns = list(proc_list(GPF[:10]).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d836045b-f62b-4c1c-9a12-5c57a37a2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Export issue-level data #\n",
    "###########################\n",
    "\n",
    "GPF.to_csv(\"../RawData/SDC/GPF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4610cbb1-2683-499e-9f81-745a136dac7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.9 s, sys: 9.15 s, total: 39 s\n",
      "Wall time: 1h 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "###################################\n",
    "# Quantitiy of debt, county-level #\n",
    "###################################\n",
    "\n",
    "# Obtain quantity of debt at county level, by aggregate and also by \n",
    "# (1) The method of placement\n",
    "# (2) The use of proceeds\n",
    "# (3) Type of borrowing entity\n",
    "\n",
    "# Note that should consider taking log when using this variable\n",
    "\n",
    "# For speed reasons, proceed year by year\n",
    "Years = list(range(1967,2023))\n",
    "\n",
    "GPFAmount = GPF[['State','County','sale_year','amount',\n",
    "    'issuer_type_full','Bid','use_of_proceeds_BB','use_of_proceeds_general','use_of_proceeds_main']].copy()\n",
    "\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='nan']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='AS']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='DC']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='FF']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='GU']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='MR']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='PR']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='TT']\n",
    "GPFAmount = GPFAmount[GPFAmount['State']!='VI']\n",
    "\n",
    "GPFAmount = GPFAmount.reset_index(drop=True)\n",
    "GPFAmount = GPFAmount[~pd.isnull(GPFAmount['County'])]\n",
    "\n",
    "def proc_list(GPFAmount):\n",
    "    GPFAmount = GPFAmount.copy()\n",
    "    GPFAmount['County'] = GPFAmount['County'].str.replace(' AND ','/')\n",
    "    GPFAmount_New = []\n",
    "    for idx,row in GPFAmount.iterrows():\n",
    "        if '/' not in row['County']:\n",
    "            GPFAmount_New = GPFAmount_New+[dict(row)]\n",
    "        else:\n",
    "            Countys = row['County'].split('/')\n",
    "            for County in Countys:\n",
    "                row_new = dict(row)\n",
    "                row_new['County'] = County\n",
    "                row_new['amount'] = row['amount']/len(Countys)\n",
    "                GPFAmount_New = GPFAmount_New+[row_new]\n",
    "    GPFAmount_New = pd.DataFrame(GPFAmount_New)\n",
    "    GPFAmount_New['County'] = GPFAmount_New['County'].str.strip()\n",
    "    return GPFAmount_New\n",
    "\n",
    "meta_columns = list(proc_list(GPFAmount[:10]).columns)\n",
    "GPFAmount_dd = dd.from_pandas(GPFAmount, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    GPFAmount = GPFAmount_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "\n",
    "#---------------------#\n",
    "# Method of placement #\n",
    "#---------------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXBid = p.starmap(FUN_GetQ_byPlacement, input_list)\n",
    "StateXCountyXBid = pd.concat(StateXCountyXBid)\n",
    "StateXCountyXBid.to_parquet(\"../CleanData/SDC/StateXCountyXBid.parquet\")\n",
    "\n",
    "#-----------------#\n",
    "# Use of proceeds #\n",
    "#-----------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageBB = p.starmap(FUN_GetQ_byUsageBB, input_list)\n",
    "StateXCountyXUsageBB = pd.concat(StateXCountyXUsageBB)\n",
    "StateXCountyXUsageBB.to_parquet(\"../CleanData/SDC/StateXCountyXUsageBB.parquet\")\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageGeneral = p.starmap(FUN_GetQ_byUsageGeneral, input_list)\n",
    "StateXCountyXUsageGeneral = pd.concat(StateXCountyXUsageGeneral)\n",
    "StateXCountyXUsageGeneral.to_parquet(\"../CleanData/SDC/StateXCountyXUsageGeneral.parquet\")\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXUsageMain = p.starmap(FUN_GetQ_byUsageMain, input_list)\n",
    "StateXCountyXUsageMain = pd.concat(StateXCountyXUsageMain)\n",
    "StateXCountyXUsageMain.to_parquet(\"../CleanData/SDC/StateXCountyXUsageMain.parquet\")\n",
    "\n",
    "#----------------#\n",
    "# Type of issuer #\n",
    "#----------------#\n",
    "\n",
    "input_list = [(year,GPFAmount) for year in Years]\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        StateXCountyXIssuerType = p.starmap(FUN_GetQ_byIssuerType, input_list)\n",
    "StateXCountyXIssuerType = pd.concat(StateXCountyXIssuerType)\n",
    "StateXCountyXIssuerType.to_parquet(\"../CleanData/SDC/StateXCountyXIssuerType.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0d2c8-0b18-4260-bf74-705f410e04ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe02ae1-fe53-4212-8c89-b3db7193a693",
   "metadata": {},
   "source": [
    "# 2. SDC M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4a60d4c2-41e4-4bf5-9cd2-18ce09c4bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that \n",
    "# (1) In the data source I have all M&As in the financial industry since 1990. Consider expanding the dataset?\n",
    "# (2) This dataset also has withdrawn mergers, which is dropped when requiring new shares > 50%. But, maybe partial ownership can \n",
    "# affect market power as well? Sample size significantly increases when this restriction is removed\n",
    "\n",
    "MA_Fin_19800101_19861231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19800101_19861231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19870101_19891231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19870101_19891231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19900101_19951231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19900101_19951231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19960101_19981231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19960101_19981231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19990101_20011231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19990101_20011231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20020101_20041231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20020101_20041231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20050101_20061231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20050101_20061231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20070101_20091231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20070101_20091231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20100101_20121231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20100101_20121231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20130101_20151231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20130101_20151231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20160101_20181231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20160101_20181231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20190101_20211231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20190101_20211231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20220101_20230930 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20220101_20230930.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA = pd.concat([\n",
    "    MA_Fin_19800101_19861231,\n",
    "    MA_Fin_19870101_19891231,\n",
    "    MA_Fin_19900101_19951231,\n",
    "    MA_Fin_19960101_19981231,\n",
    "    MA_Fin_19990101_20011231,\n",
    "    MA_Fin_20020101_20041231,\n",
    "    MA_Fin_20050101_20061231,\n",
    "    MA_Fin_20070101_20091231,\n",
    "    MA_Fin_20100101_20121231,\n",
    "    MA_Fin_20130101_20151231,\n",
    "    MA_Fin_20160101_20181231,\n",
    "    MA_Fin_20190101_20211231,\n",
    "    MA_Fin_20220101_20230930,\n",
    "])\n",
    "\n",
    "# Rename variable\n",
    "MA = MA.rename(columns={\n",
    "    '  Date\\nAnnounced':'date_announced',\n",
    "    '  Date\\nEffective':'date_effective',\n",
    "    'Target Name':'target_raw',\n",
    "    'Acquiror Name':'acquiror_raw',\n",
    "    '  %\\nOwned\\nAfter\\nTrans-\\naction':'new_share'})\n",
    "MA = MA[['date_announced','date_effective','target_raw','acquiror_raw','new_share','Synopsis','Status']]\n",
    "# Clean names\n",
    "MA['target'] = MA['target_raw'].apply(FUN_proc_name)\n",
    "MA['acquiror'] = MA['acquiror_raw'].apply(FUN_proc_name)\n",
    "MA['sale_year'] = pd.to_datetime(MA['date_effective']).dt.year\n",
    "MA['announce_year'] = pd.to_datetime(MA['date_announced']).dt.year\n",
    "\n",
    "# Export data\n",
    "MA.to_csv(\"../RawData/SDC/MA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a71c2c-b0cf-46cc-a22e-0c3709b63691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205bbdb3-33f3-4ef9-bcc9-b2a03b9dd4d4",
   "metadata": {},
   "source": [
    "# 3. Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30adc3-3705-4733-bc21-c60e5ffba546",
   "metadata": {},
   "source": [
    "For population, use Census Bureau data for 2021-2022, but NIH data piror to that, as some files are missing from Census Bureau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "933afdc4-2e6b-49fc-9220-f3153f35f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# CSA-level population size #\n",
    "#############################\n",
    "\n",
    "# Data for 2021-2022\n",
    "CSA_POP_2022 = pd.read_csv(\"../RawData/MSA/POP/csa-est2022.csv\",encoding = \"ISO-8859-1\")\n",
    "CSA_POP_2022 = CSA_POP_2022[CSA_POP_2022['LSAD']=='Combined Statistical Area']\n",
    "CSA_POP_2022 = CSA_POP_2022[['CSA','POPESTIMATE2021','POPESTIMATE2022']].reset_index(drop=True)\n",
    "CSA_POP_2022 = CSA_POP_2022.rename(columns={'CSA':'CSA Code','POPESTIMATE2021':'pop_2021','POPESTIMATE2022':'pop_2022'})\n",
    "\n",
    "# Data prior to that\n",
    "us1969_2020 = pd.read_csv(\"../RawData/MSA/POP/us.1969_2020.19ages.adjusted.txt\",header=None)\n",
    "\n",
    "us1969_2020['year'] = us1969_2020[0].str.slice(0,4)\n",
    "us1969_2020['state'] = us1969_2020[0].str.slice(4,6)\n",
    "us1969_2020['state_FIPS'] = us1969_2020[0].str.slice(6,8)\n",
    "us1969_2020['county_FIPS'] = us1969_2020[0].str.slice(8,11)\n",
    "us1969_2020['registry'] = us1969_2020[0].str.slice(11,13)\n",
    "us1969_2020['race'] = us1969_2020[0].str.slice(13,14)\n",
    "us1969_2020['origin'] = us1969_2020[0].str.slice(14,15)\n",
    "us1969_2020['sex'] = us1969_2020[0].str.slice(15,16)\n",
    "us1969_2020['age'] = us1969_2020[0].str.slice(16,18)\n",
    "us1969_2020['pop'] = us1969_2020[0].str.slice(18,26).astype(int)\n",
    "us1969_2020 = us1969_2020.drop(columns=[0])\n",
    "\n",
    "us1969_2020 = us1969_2020.rename(columns={'state_FIPS':'FIPS State Code','county_FIPS':'FIPS County Code'})\n",
    "us1969_2020 = us1969_2020.groupby(['FIPS State Code','FIPS County Code','year']).agg({'pop':sum})\n",
    "us1969_2020 = us1969_2020.reset_index()\n",
    "\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "CBSAData['FIPS State Code'] = CBSAData['FIPS State Code'].astype(int).astype(str)\n",
    "CBSAData['FIPS County Code'] = CBSAData['FIPS County Code'].astype(int).astype(str)\n",
    "CBSAData.loc[CBSAData['FIPS State Code'].str.len()==1,'FIPS State Code'] = '0'+CBSAData['FIPS State Code'][CBSAData['FIPS State Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==1,'FIPS County Code'] = '00'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==2,'FIPS County Code'] = '0'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==2]\n",
    "\n",
    "pop_by_CSA = us1969_2020.merge(CBSAData[~pd.isnull(CBSAData['CSA Code'])][['CSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "pop_by_CSA = pop_by_CSA.groupby(['year','CSA Code']).agg({'pop':sum}).reset_index()\n",
    "\n",
    "# Combine two data sources\n",
    "pop_by_CSA_2021 = CSA_POP_2022[['CSA Code','pop_2021']].rename(columns={'pop_2021':'pop'})\n",
    "pop_by_CSA_2021['year'] = 2021\n",
    "pop_by_CSA_2022 = CSA_POP_2022[['CSA Code','pop_2022']].rename(columns={'pop_2022':'pop'})\n",
    "pop_by_CSA_2022['year'] = 2022\n",
    "# Stipulate population of 2023 to be identical as 2022\n",
    "pop_by_CSA_2023 = pop_by_CSA_2022.copy()\n",
    "pop_by_CSA_2023['year'] = 2023\n",
    "pop_by_CSA = pd.concat([pop_by_CSA,pop_by_CSA_2021,pop_by_CSA_2022,pop_by_CSA_2023])\n",
    "\n",
    "pop_by_CSA.to_csv(\"../RawData/MSA/POP/CSA_POP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b705dddf-ffbc-4766-8004-c50f36d8c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# CBSA-level population size #\n",
    "##############################\n",
    "\n",
    "# \"CSA_POP_2022\" do not have all the CBSAs. Use county level data from \"us1969_2020\" and stipulate for years after 2020\n",
    "\n",
    "# Data prior to that\n",
    "us1969_2020 = pd.read_csv(\"../RawData/MSA/POP/us.1969_2020.19ages.adjusted.txt\",header=None)\n",
    "\n",
    "us1969_2020['year'] = us1969_2020[0].str.slice(0,4)\n",
    "us1969_2020['state'] = us1969_2020[0].str.slice(4,6)\n",
    "us1969_2020['state_FIPS'] = us1969_2020[0].str.slice(6,8)\n",
    "us1969_2020['county_FIPS'] = us1969_2020[0].str.slice(8,11)\n",
    "us1969_2020['registry'] = us1969_2020[0].str.slice(11,13)\n",
    "us1969_2020['race'] = us1969_2020[0].str.slice(13,14)\n",
    "us1969_2020['origin'] = us1969_2020[0].str.slice(14,15)\n",
    "us1969_2020['sex'] = us1969_2020[0].str.slice(15,16)\n",
    "us1969_2020['age'] = us1969_2020[0].str.slice(16,18)\n",
    "us1969_2020['pop'] = us1969_2020[0].str.slice(18,26).astype(int)\n",
    "us1969_2020 = us1969_2020.drop(columns=[0])\n",
    "\n",
    "us1969_2020 = us1969_2020.rename(columns={'state_FIPS':'FIPS State Code','county_FIPS':'FIPS County Code'})\n",
    "us1969_2020 = us1969_2020.groupby(['FIPS State Code','FIPS County Code','year']).agg({'pop':sum})\n",
    "us1969_2020 = us1969_2020.reset_index()\n",
    "\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "CBSAData['FIPS State Code'] = CBSAData['FIPS State Code'].astype(int).astype(str)\n",
    "CBSAData['FIPS County Code'] = CBSAData['FIPS County Code'].astype(int).astype(str)\n",
    "CBSAData.loc[CBSAData['FIPS State Code'].str.len()==1,'FIPS State Code'] = '0'+CBSAData['FIPS State Code'][CBSAData['FIPS State Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==1,'FIPS County Code'] = '00'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==2,'FIPS County Code'] = '0'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==2]\n",
    "\n",
    "pop_by_CBSA = us1969_2020.merge(CBSAData[~pd.isnull(CBSAData['CBSA Code'])][['CBSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "pop_by_CBSA = pop_by_CBSA.groupby(['year','CBSA Code']).agg({'pop':sum}).reset_index()\n",
    "\n",
    "pop_by_CBSA['year'] = pop_by_CBSA['year'].astype(int)\n",
    "\n",
    "# Combine two data sources\n",
    "pop_by_CBSA_2021 = pop_by_CBSA[pop_by_CBSA['year']==2020].copy()\n",
    "pop_by_CBSA_2021['year'] = 2021\n",
    "pop_by_CBSA_2022 = pop_by_CBSA[pop_by_CBSA['year']==2020].copy()\n",
    "pop_by_CBSA_2022['year'] = 2022\n",
    "pop_by_CBSA_2023 = pop_by_CBSA[pop_by_CBSA['year']==2020].copy()\n",
    "pop_by_CBSA_2023['year'] = 2023\n",
    "pop_by_CBSA = pd.concat([pop_by_CBSA,pop_by_CBSA_2021,pop_by_CBSA_2022,pop_by_CBSA_2023])\n",
    "\n",
    "pop_by_CBSA.to_csv(\"../RawData/MSA/POP/CBSA_POP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd821dc4-9f9b-4e77-9e41-00b502198354",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "CBSAData['FIPS State Code'] = CBSAData['FIPS State Code'].astype(int).astype(str)\n",
    "CBSAData['FIPS County Code'] = CBSAData['FIPS County Code'].astype(int).astype(str)\n",
    "CBSAData.loc[CBSAData['FIPS State Code'].str.len()==1,'FIPS State Code'] = '0'+CBSAData['FIPS State Code'][CBSAData['FIPS State Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==1,'FIPS County Code'] = '00'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==1]\n",
    "CBSAData.loc[CBSAData['FIPS County Code'].str.len()==2,'FIPS County Code'] = '0'+CBSAData['FIPS County Code'][CBSAData['FIPS County Code'].str.len()==2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfe311-1b9c-47a2-8674-c61502125fad",
   "metadata": {},
   "source": [
    "For income, use data from BEA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c99803-190d-49a5-a7b7-4eaba333333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# CSA-level income #\n",
    "####################\n",
    "\n",
    "CAINC1__ALL_AREAS_1969_2021 = pd.read_csv(\"../RawData/MSA/CAINC1/CAINC1__ALL_AREAS_1969_2021.csv\",encoding = \"ISO-8859-1\")\n",
    "inc_by_county = CAINC1__ALL_AREAS_1969_2021[CAINC1__ALL_AREAS_1969_2021['Description']=='Per capita personal income (dollars) 2/'].copy()\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace(' ','')\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace('\"','')\n",
    "inc_by_county['FIPS State Code'] = inc_by_county['GeoFIPS'].str[:2]\n",
    "inc_by_county['FIPS County Code'] = inc_by_county['GeoFIPS'].str[2:5]\n",
    "\n",
    "CAINC1__ALL_AREAS_1969_2021 = pd.read_csv(\"../RawData/MSA/CAINC1/CAINC1__ALL_AREAS_1969_2021.csv\",encoding = \"ISO-8859-1\")\n",
    "pop_by_county = CAINC1__ALL_AREAS_1969_2021[CAINC1__ALL_AREAS_1969_2021['Description']=='Population (persons) 1/'].copy()\n",
    "pop_by_county['GeoFIPS'] = pop_by_county['GeoFIPS'].str.replace(' ','')\n",
    "pop_by_county['GeoFIPS'] = pop_by_county['GeoFIPS'].str.replace('\"','')\n",
    "pop_by_county['FIPS State Code'] = pop_by_county['GeoFIPS'].str[:2]\n",
    "pop_by_county['FIPS County Code'] = pop_by_county['GeoFIPS'].str[2:5]\n",
    "\n",
    "\n",
    "inc_by_county = inc_by_county.merge(CBSAData[~pd.isnull(CBSAData['CSA Code'])][['CSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "pop_by_county = pop_by_county.merge(CBSAData[~pd.isnull(CBSAData['CSA Code'])][['CSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "\n",
    "inc_by_CSA = pd.DataFrame()\n",
    "\n",
    "for year in range(1969,2022):\n",
    "    \n",
    "    inc_by_county_oneyear = inc_by_county[[str(year),'CSA Code','FIPS State Code','FIPS County Code']]\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.rename(columns={str(year):'inc'})\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear[inc_by_county_oneyear['inc']!='(NA)']\n",
    "    inc_by_county_oneyear['inc'] = inc_by_county_oneyear['inc'].astype(float)\n",
    "\n",
    "    pop_by_county_oneyear = pop_by_county[[str(year),'CSA Code','FIPS State Code','FIPS County Code']]\n",
    "    pop_by_county_oneyear = pop_by_county_oneyear.rename(columns={str(year):'pop'})\n",
    "    pop_by_county_oneyear = pop_by_county_oneyear[pop_by_county_oneyear['pop']!='(NA)']\n",
    "    pop_by_county_oneyear['pop'] = pop_by_county_oneyear['pop'].astype(float)\n",
    "    \n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.merge(pop_by_county_oneyear,on=['FIPS State Code','FIPS County Code','CSA Code'])\n",
    "\n",
    "    # Calculate weighted average income\n",
    "    inc_by_county_oneyear['incXpop'] = inc_by_county_oneyear['inc']*inc_by_county_oneyear['pop']\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.groupby(['CSA Code']).agg({'pop':sum,'incXpop':sum})\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.reset_index()\n",
    "    inc_by_county_oneyear['inc'] = inc_by_county_oneyear['incXpop']/inc_by_county_oneyear['pop']\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear[['inc','CSA Code']]\n",
    "    inc_by_county_oneyear['year'] = year\n",
    "\n",
    "    inc_by_CSA = pd.concat([inc_by_CSA,inc_by_county_oneyear])\n",
    "\n",
    "# Supplement with 2022 and 2023, for which I assume income to be the same as 2021\n",
    "inc_by_county_oneyear['year'] = 2022\n",
    "inc_by_CSA = pd.concat([inc_by_CSA,inc_by_county_oneyear])\n",
    "inc_by_county_oneyear['year'] = 2023\n",
    "inc_by_CSA = pd.concat([inc_by_CSA,inc_by_county_oneyear])\n",
    "\n",
    "inc_by_CSA.to_csv(\"../RawData/MSA/CAINC1/CSA_INC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edc6c6-f77c-4b12-ab4f-7e81f78e807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# CBSA-level income #\n",
    "#####################\n",
    "\n",
    "CAINC1__ALL_AREAS_1969_2021 = pd.read_csv(\"../RawData/MSA/CAINC1/CAINC1__ALL_AREAS_1969_2021.csv\",encoding = \"ISO-8859-1\")\n",
    "inc_by_county = CAINC1__ALL_AREAS_1969_2021[CAINC1__ALL_AREAS_1969_2021['Description']=='Per capita personal income (dollars) 2/'].copy()\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace(' ','')\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace('\"','')\n",
    "inc_by_county['FIPS State Code'] = inc_by_county['GeoFIPS'].str[:2]\n",
    "inc_by_county['FIPS County Code'] = inc_by_county['GeoFIPS'].str[2:5]\n",
    "\n",
    "CAINC1__ALL_AREAS_1969_2021 = pd.read_csv(\"../RawData/MSA/CAINC1/CAINC1__ALL_AREAS_1969_2021.csv\",encoding = \"ISO-8859-1\")\n",
    "pop_by_county = CAINC1__ALL_AREAS_1969_2021[CAINC1__ALL_AREAS_1969_2021['Description']=='Population (persons) 1/'].copy()\n",
    "pop_by_county['GeoFIPS'] = pop_by_county['GeoFIPS'].str.replace(' ','')\n",
    "pop_by_county['GeoFIPS'] = pop_by_county['GeoFIPS'].str.replace('\"','')\n",
    "pop_by_county['FIPS State Code'] = pop_by_county['GeoFIPS'].str[:2]\n",
    "pop_by_county['FIPS County Code'] = pop_by_county['GeoFIPS'].str[2:5]\n",
    "\n",
    "\n",
    "inc_by_county = inc_by_county.merge(CBSAData[~pd.isnull(CBSAData['CBSA Code'])][['CBSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "pop_by_county = pop_by_county.merge(CBSAData[~pd.isnull(CBSAData['CBSA Code'])][['CBSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['FIPS State Code','FIPS County Code'])\n",
    "\n",
    "inc_by_CBSA = pd.DataFrame()\n",
    "\n",
    "for year in range(1969,2022):\n",
    "    \n",
    "    inc_by_county_oneyear = inc_by_county[[str(year),'CBSA Code','FIPS State Code','FIPS County Code']]\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.rename(columns={str(year):'inc'})\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear[inc_by_county_oneyear['inc']!='(NA)']\n",
    "    inc_by_county_oneyear['inc'] = inc_by_county_oneyear['inc'].astype(float)\n",
    "\n",
    "    pop_by_county_oneyear = pop_by_county[[str(year),'CBSA Code','FIPS State Code','FIPS County Code']]\n",
    "    pop_by_county_oneyear = pop_by_county_oneyear.rename(columns={str(year):'pop'})\n",
    "    pop_by_county_oneyear = pop_by_county_oneyear[pop_by_county_oneyear['pop']!='(NA)']\n",
    "    pop_by_county_oneyear['pop'] = pop_by_county_oneyear['pop'].astype(float)\n",
    "    \n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.merge(pop_by_county_oneyear,on=['FIPS State Code','FIPS County Code','CBSA Code'])\n",
    "\n",
    "    # Calculate weighted average income\n",
    "    inc_by_county_oneyear['incXpop'] = inc_by_county_oneyear['inc']*inc_by_county_oneyear['pop']\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.groupby(['CBSA Code']).agg({'pop':sum,'incXpop':sum})\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear.reset_index()\n",
    "    inc_by_county_oneyear['inc'] = inc_by_county_oneyear['incXpop']/inc_by_county_oneyear['pop']\n",
    "    inc_by_county_oneyear = inc_by_county_oneyear[['inc','CBSA Code']]\n",
    "    inc_by_county_oneyear['year'] = year\n",
    "\n",
    "    inc_by_CBSA = pd.concat([inc_by_CBSA,inc_by_county_oneyear])\n",
    "\n",
    "# Supplement with 2022 and 2023, for which I assume income to be the same as 2021\n",
    "inc_by_county_oneyear['year'] = 2022\n",
    "inc_by_CBSA = pd.concat([inc_by_CBSA,inc_by_county_oneyear])\n",
    "inc_by_county_oneyear['year'] = 2023\n",
    "inc_by_CBSA = pd.concat([inc_by_CBSA,inc_by_county_oneyear])\n",
    "\n",
    "inc_by_CBSA.to_csv(\"../RawData/MSA/CAINC1/CBSA_INC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "95cd427c-a976-4541-a324-7be50c6f5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# County-level data, including black ratio #\n",
    "############################################\n",
    "\n",
    "# Data prior to that\n",
    "us1969_2020 = pd.read_csv(\"../RawData/MSA/POP/us.1969_2020.19ages.adjusted.txt\",header=None)\n",
    "\n",
    "us1969_2020['year'] = us1969_2020[0].str.slice(0,4)\n",
    "us1969_2020['state'] = us1969_2020[0].str.slice(4,6)\n",
    "us1969_2020['state_FIPS'] = us1969_2020[0].str.slice(6,8)\n",
    "us1969_2020['county_FIPS'] = us1969_2020[0].str.slice(8,11)\n",
    "us1969_2020['registry'] = us1969_2020[0].str.slice(11,13)\n",
    "us1969_2020['race'] = us1969_2020[0].str.slice(13,14)\n",
    "us1969_2020['origin'] = us1969_2020[0].str.slice(14,15)\n",
    "us1969_2020['sex'] = us1969_2020[0].str.slice(15,16)\n",
    "us1969_2020['age'] = us1969_2020[0].str.slice(16,18)\n",
    "us1969_2020['pop'] = us1969_2020[0].str.slice(18,26).astype(int)\n",
    "us1969_2020 = us1969_2020.drop(columns=[0])\n",
    "\n",
    "us1969_2020 = us1969_2020.rename(columns={'state_FIPS':'FIPS State Code','county_FIPS':'FIPS County Code'})\n",
    "\n",
    "# Calculate black ratio\n",
    "black_pop = us1969_2020[us1969_2020['race']=='2'][['year','FIPS State Code','FIPS County Code','pop']]\n",
    "black_pop = black_pop.groupby(['FIPS State Code','FIPS County Code','year']).agg({'pop':sum})\n",
    "black_pop = black_pop.rename(columns={'pop':'black_pop'})\n",
    "\n",
    "county_pop = us1969_2020.groupby(['FIPS State Code','FIPS County Code','year']).agg({'pop':sum})\n",
    "county_pop = county_pop.reset_index()\n",
    "\n",
    "black_pop = black_pop.merge(county_pop,on=['FIPS State Code','FIPS County Code','year'])\n",
    "black_pop['black_ratio'] = black_pop['black_pop']/black_pop['pop']\n",
    "\n",
    "# Income by county, including those not in CBSA\n",
    "CAINC1__ALL_AREAS_1969_2021 = pd.read_csv(\"../RawData/MSA/CAINC1/CAINC1__ALL_AREAS_1969_2021.csv\",encoding = \"ISO-8859-1\")\n",
    "inc_by_county = CAINC1__ALL_AREAS_1969_2021[CAINC1__ALL_AREAS_1969_2021['Description']=='Per capita personal income (dollars) 2/'].copy()\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace(' ','')\n",
    "inc_by_county['GeoFIPS'] = inc_by_county['GeoFIPS'].str.replace('\"','')\n",
    "inc_by_county['FIPS State Code'] = inc_by_county['GeoFIPS'].str[:2]\n",
    "inc_by_county['FIPS County Code'] = inc_by_county['GeoFIPS'].str[2:5]\n",
    "\n",
    "inc = pd.DataFrame()\n",
    "for year in range(1969,2021):\n",
    "    inc_oneyear = inc_by_county[['FIPS State Code','FIPS County Code',str(year)]].copy()\n",
    "    inc_oneyear = inc_oneyear.rename(columns={str(year):'inc'})\n",
    "    inc_oneyear['year'] = year\n",
    "    inc = pd.concat([inc,inc_oneyear])\n",
    "\n",
    "# Note that CBSA should not be used below: It is not a complete list of all counties in the US\n",
    "\n",
    "# Complete list of counties, including those not part of CSA \n",
    "all_counties = pd.read_csv(\"../RawData/MSA/fips-by-state.csv\",sep=',',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "all_counties = all_counties.rename(columns={'name':'County','state':'State'})\n",
    "all_counties['County'] = all_counties['County'].str.upper()\n",
    "all_counties['County'] = all_counties['County'].str.replace(' COUNTY','')\n",
    "all_counties['County'] = all_counties['County'].str.replace(' AND ',' & ')\n",
    "all_counties['County'] = all_counties['County'].str.replace('.','',regex=False)\n",
    "\n",
    "all_counties['fips'] = all_counties['fips'].astype(str)\n",
    "all_counties.loc[all_counties['fips'].str.len()==4,'fips'] = '0'+all_counties['fips'][all_counties['fips'].str.len()==4]\n",
    "all_counties['FIPS State Code'] = all_counties['fips'].str.slice(0,2)\n",
    "all_counties['FIPS County Code'] = all_counties['fips'].str.slice(2,5)\n",
    "\n",
    "all_counties_Exploded = pd.DataFrame()\n",
    "for year in range(1969,2021):\n",
    "    all_counties['year'] = year\n",
    "    all_counties_Exploded = pd.concat([all_counties_Exploded,all_counties])\n",
    "all_counties_Exploded['year'] = all_counties_Exploded['year'].astype(str)\n",
    "\n",
    "# Merge with county data\n",
    "black_pop = all_counties_Exploded.merge(black_pop,on=['FIPS State Code','FIPS County Code','year'])\n",
    "black_pop['year'] = black_pop['year'].astype(int)\n",
    "black_pop = black_pop.merge(inc,on=['FIPS State Code','FIPS County Code','year'])\n",
    "black_pop.loc[black_pop['inc']=='(NA)','inc'] = None\n",
    "black_pop['inc'] = black_pop['inc'].astype(float)\n",
    "\n",
    "# Export data\n",
    "black_pop['year'] = black_pop['year'].astype(int)\n",
    "black_pop_2021 = black_pop[black_pop['year']==2020].copy()\n",
    "black_pop_2021['year'] = 2021\n",
    "black_pop_2022 = black_pop[black_pop['year']==2020].copy()\n",
    "black_pop_2022['year'] = 2022\n",
    "black_pop_2023 = black_pop[black_pop['year']==2020].copy()\n",
    "black_pop_2023['year'] = 2023\n",
    "black_pop = pd.concat([black_pop,black_pop_2021,black_pop_2022,black_pop_2023])\n",
    "black_pop.to_csv(\"../RawData/MSA/POP/black_pop.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49236f86-34d8-4e5d-b182-324fb1304f05",
   "metadata": {},
   "source": [
    "# 4. Geographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259d9e5-3663-4600-8b9d-efd02e9b4028",
   "metadata": {},
   "source": [
    "- Get a list of MSAs that has overlap in terms of being in the same state. Use these to rule out unreasonable control groups that might also get affected by the treatment. Note that just by matching using the population and average income will result in some treated-control pairs that are very geographically approximate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd73b5c-8dd2-4cb3-acea-befff268e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"CSA\" is for metropolitan and \"CBSAData\" includes also those micropolitan\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "%run -i SCRIPT_us_states.py\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSAData = CBSAData.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSAData = CBSAData.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSAData = CBSAData[CBSAData['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSAData['County'] = CBSAData['County'].str.upper().str.replace(' COUNTY','')\n",
    "\n",
    "\n",
    "#-----------#\n",
    "# CSA pairs #\n",
    "#-----------#\n",
    "\n",
    "Same_State_CSA_pairs = []\n",
    "CSAs = list(CBSAData['CSA Code'].unique())\n",
    "CSAs = [item for item in CSAs if item!=None and str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "    # A list of CSAs in the same state\n",
    "    States = list(CBSAData[CBSAData['CSA Code']==CSA]['FIPS State Code'].unique())\n",
    "    for State in States:\n",
    "        CSAs_same_state = list(CBSAData[CBSAData['FIPS State Code']==State]['CSA Code'].unique())\n",
    "        CSAs_same_state = [item for item in CSAs_same_state if item!=None and str(item)!='nan' and item!=CSA]\n",
    "        for item in CSAs_same_state:\n",
    "            Same_State_CSA_pairs = Same_State_CSA_pairs+[{'CSA_1':CSA,'CSA_2':item}]\n",
    "            Same_State_CSA_pairs = Same_State_CSA_pairs+[{'CSA_1':item,'CSA_2':CSA}]\n",
    "\n",
    "# Pairs of CSAs in the same state\n",
    "Title_Code = CBSAData[['CSA Code','CSA Title']].drop_duplicates()\n",
    "Title_Code = Title_Code[~pd.isnull(Title_Code['CSA Code'])]\n",
    "Same_State_CSA_pairs = pd.DataFrame(Same_State_CSA_pairs)\n",
    "Same_State_CSA_pairs = Same_State_CSA_pairs.merge(Title_Code.rename(columns={'CSA Code':'CSA_1','CSA Title':'Title_1'}),on='CSA_1')\n",
    "Same_State_CSA_pairs = Same_State_CSA_pairs.merge(Title_Code.rename(columns={'CSA Code':'CSA_2','CSA Title':'Title_2'}),on='CSA_2')\n",
    "\n",
    "Same_State_CSA_pairs.to_csv(\"../RawData/MSA/CAINC1/Same_State_CSA_pairs.csv\")\n",
    "\n",
    "#------------#\n",
    "# CBSA pairs #\n",
    "#------------#\n",
    "\n",
    "Same_State_CBSA_pairs = []\n",
    "CBSAs = list(CBSAData['CBSA Code'].unique())\n",
    "CBSAs = [item for item in CBSAs if item!=None and str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "    # A list of CBSAs in the same state\n",
    "    States = list(CBSAData[CBSAData['CBSA Code']==CBSA]['FIPS State Code'].unique())\n",
    "    for State in States:\n",
    "        CBSAs_same_state = list(CBSAData[CBSAData['FIPS State Code']==State]['CBSA Code'].unique())\n",
    "        CBSAs_same_state = [item for item in CBSAs_same_state if item!=None and str(item)!='nan' and item!=CBSA]\n",
    "        for item in CBSAs_same_state:\n",
    "            Same_State_CBSA_pairs = Same_State_CBSA_pairs+[{'CBSA_1':CBSA,'CBSA_2':item}]\n",
    "            Same_State_CBSA_pairs = Same_State_CBSA_pairs+[{'CBSA_1':item,'CBSA_2':CBSA}]\n",
    "\n",
    "# Pairs of CBSAs in the same state\n",
    "Title_Code = CBSAData[['CBSA Code','CBSA Title']].drop_duplicates()\n",
    "Title_Code = Title_Code[~pd.isnull(Title_Code['CBSA Code'])]\n",
    "Same_State_CBSA_pairs = pd.DataFrame(Same_State_CBSA_pairs)\n",
    "Same_State_CBSA_pairs = Same_State_CBSA_pairs.merge(Title_Code.rename(columns={'CBSA Code':'CBSA_1','CBSA Title':'Title_1'}),on='CBSA_1')\n",
    "Same_State_CBSA_pairs = Same_State_CBSA_pairs.merge(Title_Code.rename(columns={'CBSA Code':'CBSA_2','CBSA Title':'Title_2'}),on='CBSA_2')\n",
    "\n",
    "Same_State_CBSA_pairs.to_csv(\"../RawData/MSA/CAINC1/Same_State_CBSA_pairs.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8e6ac-eb02-454d-a740-11eec38a5e3c",
   "metadata": {},
   "source": [
    "# 5. Texas & California Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf4cfc06-160d-494a-9a57-a34182043539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Texas, fee data is available for 2022 only\n",
    "Local_Issuance_Fee = pd.read_csv('../RawData/Texas/Local_Issuance_Fee.csv')\n",
    "Local_Issuance = pd.read_csv('../RawData/Texas/Local_Issuance.csv')\n",
    "Local_Issuance_Fee.value_counts('FiscalYearIssuance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bacd695-2e53-41e4-9fcb-af2efa735ac3",
   "metadata": {},
   "source": [
    "Estimate a statistical model of issuance fees (advisor fee, insurance premium, ratings fee) as a ratio of amount:\n",
    "$$\\text{Cost}=\n",
    "\\gamma_1\\frac{\\text{County Income}}{\\text{National Average Income}}+\\gamma_2\\frac{\\text{County Population}}{\\text{National Average County Population}}+\n",
    "\\delta_{\\text{maturity bracket}}+\\delta_{\\text{amount bracket}}+\n",
    "\\theta_{\\text{method of sales}}+\\theta_{\\text{tax status}}+\\theta_{\\text{source of repayment}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "153c3e4f-c2ca-4e51-b080-cb971b483be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# Import california fee data #\n",
    "##############################\n",
    "\n",
    "CDA_All_Data = pd.read_csv('../RawData/California/CDA_All_Data.csv')\n",
    "\n",
    "CDA_All_Data['County'] = CDA_All_Data['Issuer County'].str.upper()\n",
    "CDA_All_Data['State'] = 'CA'\n",
    "\n",
    "CDA_All_Data['AdvisorRatio'] = CDA_All_Data['Financial Advisor Fee']/CDA_All_Data['Principal Amount']\n",
    "CDA_All_Data['CRRatio'] = CDA_All_Data['Rating Agency Fee']/CDA_All_Data['Principal Amount']\n",
    "CDA_All_Data['InsureRatio'] = CDA_All_Data['Credit Enhancement Fee']/CDA_All_Data['Principal Amount']\n",
    "\n",
    "# Winsorize data\n",
    "upper_limit = np.percentile(CDA_All_Data['AdvisorRatio'][np.logical_not(np.isnan(CDA_All_Data['AdvisorRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data['AdvisorRatio'][np.logical_not(np.isnan(CDA_All_Data['AdvisorRatio']))],1)\n",
    "CDA_All_Data.loc[(CDA_All_Data['AdvisorRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data['AdvisorRatio']))),'AdvisorRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data.loc[(CDA_All_Data['AdvisorRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data['AdvisorRatio']))),'AdvisorRatio'] = \\\n",
    "    lower_limit\n",
    "upper_limit = np.percentile(CDA_All_Data['CRRatio'][np.logical_not(np.isnan(CDA_All_Data['CRRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data['CRRatio'][np.logical_not(np.isnan(CDA_All_Data['CRRatio']))],1)\n",
    "CDA_All_Data.loc[(CDA_All_Data['CRRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data['CRRatio']))),'CRRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data.loc[(CDA_All_Data['CRRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data['CRRatio']))),'CRRatio'] = \\\n",
    "    lower_limit\n",
    "upper_limit = np.percentile(CDA_All_Data['InsureRatio'][np.logical_not(np.isnan(CDA_All_Data['InsureRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data['InsureRatio'][np.logical_not(np.isnan(CDA_All_Data['InsureRatio']))],1)\n",
    "CDA_All_Data.loc[(CDA_All_Data['InsureRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data['InsureRatio']))),'InsureRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data.loc[(CDA_All_Data['InsureRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data['InsureRatio']))),'InsureRatio'] = \\\n",
    "    lower_limit\n",
    "\n",
    "CDA_All_Data['Sale Date'] = pd.to_datetime(CDA_All_Data['Sale Date'])\n",
    "CDA_All_Data['year'] = CDA_All_Data['Sale Date'].dt.year\n",
    "CDA_All_Data['Final Maturity Date'] = pd.to_datetime(CDA_All_Data['Final Maturity Date'])\n",
    "CDA_All_Data['year_maturity'] = CDA_All_Data['Final Maturity Date'].dt.year\n",
    "CDA_All_Data['maturity_in_years'] = CDA_All_Data['year_maturity']-CDA_All_Data['year']\n",
    "\n",
    "\n",
    "######################################\n",
    "# Merge in county level demographics #\n",
    "######################################\n",
    "\n",
    "black_pop = pd.read_csv(\"../RawData/MSA/POP/black_pop.csv\")\n",
    "\n",
    "# Get national average county-level income and \n",
    "black_pop_yearlyavg = black_pop.groupby('year').agg({'pop':'mean','inc':'mean'})\n",
    "black_pop_yearlyavg = black_pop_yearlyavg.rename(columns={'pop':'pop_yearlyavg','inc':'inc_yearlyavg'})\n",
    "black_pop = black_pop.merge(black_pop_yearlyavg,on=['year'])\n",
    "black_pop['pop_to_avg'] = black_pop['pop']/black_pop['pop_yearlyavg']\n",
    "black_pop['inc_to_avg'] = black_pop['inc']/black_pop['inc_yearlyavg']\n",
    "\n",
    "CDA_All_Data = CDA_All_Data.merge(black_pop[['State','County','year','pop_to_avg','inc_to_avg']],on=['State','County','year'])\n",
    "\n",
    "CDA_All_Data = CDA_All_Data[[\n",
    "    'Issuer','Issuer Group','Issuer Type','County','State','year',\n",
    "    'Principal Amount','maturity_in_years','Debt Type','Federally Taxable','Sale Type (Comp/Neg)',\n",
    "    'AdvisorRatio','CRRatio','InsureRatio',\n",
    "    'pop_to_avg','inc_to_avg']]\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "# Recreate variables to be consistent with GPF #\n",
    "################################################\n",
    "\n",
    "CDA_All_Data['GPF_security_type'] = None\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Tax and Revenue Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='General Obligation Bond','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Special Assessment Bond','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Certificate of Participation/Leases','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Limited Tax Obligation Bond (Special Tax Bonds)','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Conduit Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Public Enterprise Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Tax Allocation Bond','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Public Lease Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Marks-Roos Loan','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Commercial Paper','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Conduit Revenue Note or Loan (Private Obligor)','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Bond Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Capital Lease','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Pension Obligation Bonds','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Other Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Sales Tax Revenue Bond','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Tax Allocation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Revenue Anticipation Note','GPF_security_type'] = 'RV'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Other Bond','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='State Agency Loan','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Loan from bank/other institution','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Tax Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Grant Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Other Debt','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Promissory Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='General Obligation Note','GPF_security_type'] = 'GO'\n",
    "CDA_All_Data.loc[CDA_All_Data['Debt Type']=='Revenue Anticipation Warrant','GPF_security_type'] = 'RV'\n",
    "\n",
    "CDA_All_Data['GPF_taxable_code'] = None\n",
    "CDA_All_Data.loc[CDA_All_Data['Federally Taxable']=='Federal Tax Flag: E','GPF_taxable_code'] = 'E'\n",
    "CDA_All_Data.loc[CDA_All_Data['Federally Taxable']=='Federal Tax Flag: T','GPF_taxable_code'] = 'T'\n",
    "CDA_All_Data.loc[CDA_All_Data['Federally Taxable']=='Subject to Alternative Minimum Tax','GPF_taxable_code'] = 'A'\n",
    "CDA_All_Data.loc[CDA_All_Data['Federally Taxable']=='Federal Tax Flag: ET','GPF_taxable_code'] = 'E'\n",
    "CDA_All_Data.loc[CDA_All_Data['Federally Taxable']=='Federal Tax Flag: TE','GPF_taxable_code'] = 'T'\n",
    "\n",
    "CDA_All_Data['GPF_Bid'] = None\n",
    "CDA_All_Data.loc[CDA_All_Data['Sale Type (Comp/Neg)']=='Neg','GPF_Bid'] = 'N'\n",
    "CDA_All_Data.loc[CDA_All_Data['Sale Type (Comp/Neg)']=='Comp','GPF_Bid'] = 'C'\n",
    "\n",
    "# Adjust inflation\n",
    "FPCPITOTLZGUSA = pd.read_csv(\"../RawData/StLouisFed/FPCPITOTLZGUSA.csv\")\n",
    "FPCPITOTLZGUSA['year'] = FPCPITOTLZGUSA['DATE'].str[:4].astype(int)\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA.sort_values('year',ascending=False).reset_index(drop=True)\n",
    "scaler = 1\n",
    "FPCPITOTLZGUSA['scaler'] = None\n",
    "for idx,row in FPCPITOTLZGUSA.iterrows():\n",
    "    if idx==0:\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = 1\n",
    "    else:\n",
    "        scaler = scaler*(FPCPITOTLZGUSA.at[idx-1,'FPCPITOTLZGUSA']/100+1)\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = scaler\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA[['scaler','year']]\n",
    "\n",
    "CDA_All_Data = CDA_All_Data.merge(FPCPITOTLZGUSA,on=['year'])\n",
    "CDA_All_Data['GPF_amount_inf_adjusted'] = CDA_All_Data['Principal Amount']*CDA_All_Data['scaler']\n",
    "\n",
    "CDA_All_Data['GPF_amount_bracket'] = None\n",
    "CDA_All_Data.loc[CDA_All_Data['GPF_amount_inf_adjusted']<=1*1000000,\n",
    "    'GPF_amount_bracket'] = 'Less than 1M'\n",
    "CDA_All_Data.loc[(CDA_All_Data['GPF_amount_inf_adjusted']>1*1000000)&(CDA_All_Data['GPF_amount_inf_adjusted']<=5*1000000),\n",
    "    'GPF_amount_bracket'] = '1M to 5M'\n",
    "CDA_All_Data.loc[(CDA_All_Data['GPF_amount_inf_adjusted']>5*1000000)&(CDA_All_Data['GPF_amount_inf_adjusted']<=10*1000000),\n",
    "    'GPF_amount_bracket'] = '5M to 10M'\n",
    "CDA_All_Data.loc[(CDA_All_Data['GPF_amount_inf_adjusted']>10*1000000)&(CDA_All_Data['GPF_amount_inf_adjusted']<=50*1000000),\n",
    "    'GPF_amount_bracket'] = '10M to 50M'\n",
    "CDA_All_Data.loc[(CDA_All_Data['GPF_amount_inf_adjusted']>50*1000000)&(CDA_All_Data['GPF_amount_inf_adjusted']<=100*1000000),\n",
    "    'GPF_amount_bracket'] = '50M to 100M'\n",
    "CDA_All_Data.loc[CDA_All_Data['GPF_amount_inf_adjusted']>100*1000000,\n",
    "    'GPF_amount_bracket'] = 'Greater than 100M'\n",
    "\n",
    "CDA_All_Data['GPF_maturity_bracket'] = None\n",
    "CDA_All_Data.loc[CDA_All_Data['maturity_in_years']<=2,'GPF_maturity_bracket'] = 'Less then 2y'\n",
    "CDA_All_Data.loc[(CDA_All_Data['maturity_in_years']>2)&(CDA_All_Data['maturity_in_years']<=5),'GPF_maturity_bracket'] = '2y to 5y'\n",
    "CDA_All_Data.loc[(CDA_All_Data['maturity_in_years']>5)&(CDA_All_Data['maturity_in_years']<=10),'GPF_maturity_bracket'] = '5y to 10y'\n",
    "CDA_All_Data.loc[(CDA_All_Data['maturity_in_years']>10)&(CDA_All_Data['maturity_in_years']<=20),'GPF_maturity_bracket'] = '10y to 20y'\n",
    "CDA_All_Data.loc[(CDA_All_Data['maturity_in_years']>20)&(CDA_All_Data['maturity_in_years']<=30),'GPF_maturity_bracket'] = '20y to 30y'\n",
    "CDA_All_Data.loc[(CDA_All_Data['maturity_in_years']>30)&(CDA_All_Data['maturity_in_years']<=40),'GPF_maturity_bracket'] = '30y to 40y'\n",
    "CDA_All_Data.loc[CDA_All_Data['maturity_in_years']>40,'GPF_maturity_bracket'] = 'Greater than 40y'\n",
    "\n",
    "CDA_All_Data.to_csv('../RawData/California/SumStats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "bfb9a823-17d7-4343-b40c-13498f836319",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Import GPF #\n",
    "##############\n",
    "\n",
    "GPF = pd.read_csv(\"../RawData/SDC/GPF.csv\",low_memory=False)\n",
    "\n",
    "GPF['is_security_type_GO'] = GPF['security_type']=='GO'\n",
    "GPF['is_security_type_RV'] = GPF['security_type']=='RV'\n",
    "GPF['is_Bid_C'] = GPF['Bid']=='C'\n",
    "GPF['is_Bid_N'] = GPF['Bid']=='N'\n",
    "GPF['is_taxable_code_A'] = GPF['taxable_code']=='A'\n",
    "GPF['is_taxable_code_E'] = GPF['taxable_code']=='E'\n",
    "GPF['is_taxable_code_T'] = GPF['taxable_code']=='T'\n",
    "\n",
    "GPF = GPF.merge(FPCPITOTLZGUSA.rename(columns={'year':'sale_year'}),on=['sale_year'],how='outer',indicator=True)\n",
    "GPF = GPF[GPF['_merge']!='right_only']\n",
    "GPF = GPF.drop(columns=['_merge'])\n",
    "GPF['amount_inf_adjusted'] = GPF['amount']*GPF['scaler']*1000000\n",
    "\n",
    "GPF['is_amount_Less_than_1M'] = GPF['amount_inf_adjusted']<1*1000000\n",
    "GPF['is_amount_1M_to_5M'] = (GPF['amount_inf_adjusted']>1*1000000)&(GPF['amount_inf_adjusted']<=5*1000000)\n",
    "GPF['is_amount_5M_to_10M'] = (GPF['amount_inf_adjusted']>5*1000000)&(GPF['amount_inf_adjusted']<=10*1000000)\n",
    "GPF['is_amount_10M_to_50M'] = (GPF['amount_inf_adjusted']>10*1000000)&(GPF['amount_inf_adjusted']<=50*1000000)\n",
    "GPF['is_amount_50M_to_100M'] = (GPF['amount_inf_adjusted']>50*1000000)&(GPF['amount_inf_adjusted']<=100*1000000)\n",
    "GPF['is_amount_Greater_than_100M'] = GPF['amount_inf_adjusted']>100*1000000\n",
    "\n",
    "GPF['maturity_in_years'] = np.round(GPF['avg_maturity']/365)\n",
    "\n",
    "GPF['is_maturity_Less_than_2y'] = GPF['maturity_in_years']<2\n",
    "GPF['is_maturity_2y_to_5y'] = (GPF['maturity_in_years']>2)&(GPF['maturity_in_years']<=5)\n",
    "GPF['is_maturity_5y_to_10y'] = (GPF['maturity_in_years']>5)&(GPF['maturity_in_years']<=10)\n",
    "GPF['is_maturity_10y_to_20y'] = (GPF['maturity_in_years']>10)&(GPF['maturity_in_years']<=20)\n",
    "GPF['is_maturity_20y_to_30y'] = (GPF['maturity_in_years']>20)&(GPF['maturity_in_years']<=30)\n",
    "GPF['is_maturity_30y_to_40y'] = (GPF['maturity_in_years']>30)&(GPF['maturity_in_years']<=40)\n",
    "GPF['is_maturity_Greater_than_40y'] = GPF['maturity_in_years']>40\n",
    "\n",
    "GPF = GPF.merge(black_pop[['State','County','year','pop_to_avg','inc_to_avg']]\\\n",
    "    .rename(columns={'year':'sale_year'}),on=['State','County','sale_year'],how='outer',indicator=True)\n",
    "GPF = GPF[GPF['_merge']!='right_only']\n",
    "GPF = GPF.drop(columns=['_merge'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "da69227a-e14e-4d97-9826-1647250fd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict would-be cost of advisors, credit ratings, and insurance\n",
    "\n",
    "################\n",
    "# Advisors fee #\n",
    "################\n",
    "\n",
    "# Run regression in the California sample\n",
    "# To make sure that order of first category is fixed\n",
    "CDA_All_Data = CDA_All_Data.sort_values(['GPF_maturity_bracket','GPF_amount_bracket','GPF_Bid','GPF_security_type','GPF_taxable_code'])\n",
    "maturity_bracket = pd.get_dummies(CDA_All_Data['GPF_maturity_bracket'], drop_first=True)\n",
    "amount_bracket = pd.get_dummies(CDA_All_Data['GPF_amount_bracket'], drop_first=True)\n",
    "Bid = pd.get_dummies(CDA_All_Data['GPF_Bid'], drop_first=True)\n",
    "security_type = pd.get_dummies(CDA_All_Data['GPF_security_type'], drop_first=True)\n",
    "taxable_code = pd.get_dummies(CDA_All_Data['GPF_taxable_code'], drop_first=True)\n",
    "\n",
    "CDA_All_Data_RegData = pd.concat([CDA_All_Data,maturity_bracket,amount_bracket,Bid,security_type,taxable_code],axis=1)\n",
    "CDA_All_Data_RegData = CDA_All_Data_RegData.dropna(subset=['AdvisorRatio'])\n",
    "CDA_All_Data_RegData['AdvisorRatio'] = CDA_All_Data_RegData['AdvisorRatio']*10000\n",
    "\n",
    "upper_limit = np.percentile(CDA_All_Data_RegData['AdvisorRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['AdvisorRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data_RegData['AdvisorRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['AdvisorRatio']))],1)\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['AdvisorRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['AdvisorRatio']))),'AdvisorRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['AdvisorRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['AdvisorRatio']))),'AdvisorRatio'] = \\\n",
    "    lower_limit\n",
    "\n",
    "X = CDA_All_Data_RegData[['pop_to_avg','inc_to_avg']\n",
    "    +list(maturity_bracket.columns)+list(amount_bracket.columns)+list(Bid.columns)+list(security_type.columns)+list(taxable_code.columns)]\n",
    "y = CDA_All_Data_RegData['AdvisorRatio']\n",
    "\n",
    "CDA_All_Data_RegData.to_csv(\"../RawData/California/CDA_All_Data_RegData_AdvisorRatio.csv\")\n",
    "\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "# Predict would-be costs\n",
    "GPF['AdvisorRatio_hat'] = result.params['const']+\\\n",
    "    result.params['pop_to_avg']*GPF['pop_to_avg']+\\\n",
    "    result.params['inc_to_avg']*GPF['inc_to_avg']+\\\n",
    "    result.params['20y to 30y']*GPF['is_maturity_20y_to_30y']+\\\n",
    "    result.params['2y to 5y']*GPF['is_maturity_2y_to_5y']+\\\n",
    "    result.params['30y to 40y']*GPF['is_maturity_30y_to_40y']+\\\n",
    "    result.params['5y to 10y']*GPF['is_maturity_5y_to_10y']+\\\n",
    "    result.params['Greater than 40y']*GPF['is_maturity_Greater_than_40y']+\\\n",
    "    result.params['Less then 2y']*GPF['is_maturity_Less_than_2y']+\\\n",
    "    result.params['1M to 5M']*GPF['is_amount_1M_to_5M']+\\\n",
    "    result.params['50M to 100M']*GPF['is_amount_50M_to_100M']+\\\n",
    "    result.params['5M to 10M']*GPF['is_amount_5M_to_10M']+\\\n",
    "    result.params['Greater than 100M']*GPF['is_amount_Greater_than_100M']+\\\n",
    "    result.params['Less than 1M']*GPF['is_amount_Less_than_1M']+\\\n",
    "    result.params['N']*GPF['is_Bid_N']+\\\n",
    "    result.params['RV']*GPF['is_security_type_RV']+\\\n",
    "    result.params['E']*GPF['is_taxable_code_E']+\\\n",
    "    result.params['T']*GPF['is_taxable_code_T']\n",
    "\n",
    "GPF.loc[GPF['AdvisorRatio_hat']<0,'AdvisorRatio_hat'] = 0\n",
    "GPF['AdvisorRatio_hat'] = GPF['AdvisorRatio_hat'].astype(float)\n",
    "\n",
    "upper_limit = np.percentile(GPF['AdvisorRatio_hat'][np.logical_not(np.isnan(GPF['AdvisorRatio_hat']))],99)\n",
    "lower_limit = np.percentile(GPF['AdvisorRatio_hat'][np.logical_not(np.isnan(GPF['AdvisorRatio_hat']))],1)\n",
    "GPF.loc[(GPF['AdvisorRatio_hat']>upper_limit)&(np.logical_not(np.isnan(GPF['AdvisorRatio_hat']))),'AdvisorRatio_hat'] = \\\n",
    "    upper_limit\n",
    "GPF.loc[(GPF['AdvisorRatio_hat']<lower_limit)&(np.logical_not(np.isnan(GPF['AdvisorRatio_hat']))),'AdvisorRatio_hat'] = \\\n",
    "    lower_limit\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "# Credit rating fee #\n",
    "#####################\n",
    "\n",
    "# Run regression in the California sample\n",
    "# To make sure that order of first category is fixed\n",
    "CDA_All_Data = CDA_All_Data.sort_values(['GPF_maturity_bracket','GPF_amount_bracket','GPF_Bid','GPF_security_type','GPF_taxable_code'])\n",
    "maturity_bracket = pd.get_dummies(CDA_All_Data['GPF_maturity_bracket'], drop_first=True)\n",
    "amount_bracket = pd.get_dummies(CDA_All_Data['GPF_amount_bracket'], drop_first=True)\n",
    "Bid = pd.get_dummies(CDA_All_Data['GPF_Bid'], drop_first=True)\n",
    "security_type = pd.get_dummies(CDA_All_Data['GPF_security_type'], drop_first=True)\n",
    "taxable_code = pd.get_dummies(CDA_All_Data['GPF_taxable_code'], drop_first=True)\n",
    "\n",
    "CDA_All_Data_RegData = pd.concat([CDA_All_Data,maturity_bracket,amount_bracket,Bid,security_type,taxable_code],axis=1)\n",
    "CDA_All_Data_RegData = CDA_All_Data_RegData.dropna(subset=['CRRatio'])\n",
    "CDA_All_Data_RegData['CRRatio'] = CDA_All_Data_RegData['CRRatio']*10000\n",
    "\n",
    "upper_limit = np.percentile(CDA_All_Data_RegData['CRRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['CRRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data_RegData['CRRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['CRRatio']))],1)\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['CRRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['CRRatio']))),'CRRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['CRRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['CRRatio']))),'CRRatio'] = \\\n",
    "    lower_limit\n",
    "\n",
    "X = CDA_All_Data_RegData[['pop_to_avg','inc_to_avg']\n",
    "    +list(maturity_bracket.columns)+list(amount_bracket.columns)+list(Bid.columns)+list(security_type.columns)+list(taxable_code.columns)]\n",
    "y = CDA_All_Data_RegData['CRRatio']\n",
    "\n",
    "CDA_All_Data_RegData.to_csv(\"../RawData/California/CDA_All_Data_RegData_CRRatio.csv\")\n",
    "\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "# Predict would-be costs\n",
    "GPF['CRRatio_hat'] = result.params['const']+\\\n",
    "    result.params['pop_to_avg']*GPF['pop_to_avg']+\\\n",
    "    result.params['inc_to_avg']*GPF['inc_to_avg']+\\\n",
    "    result.params['20y to 30y']*GPF['is_maturity_20y_to_30y']+\\\n",
    "    result.params['2y to 5y']*GPF['is_maturity_2y_to_5y']+\\\n",
    "    result.params['30y to 40y']*GPF['is_maturity_30y_to_40y']+\\\n",
    "    result.params['5y to 10y']*GPF['is_maturity_5y_to_10y']+\\\n",
    "    result.params['Greater than 40y']*GPF['is_maturity_Greater_than_40y']+\\\n",
    "    result.params['Less then 2y']*GPF['is_maturity_Less_than_2y']+\\\n",
    "    result.params['1M to 5M']*GPF['is_amount_1M_to_5M']+\\\n",
    "    result.params['50M to 100M']*GPF['is_amount_50M_to_100M']+\\\n",
    "    result.params['5M to 10M']*GPF['is_amount_5M_to_10M']+\\\n",
    "    result.params['Greater than 100M']*GPF['is_amount_Greater_than_100M']+\\\n",
    "    result.params['Less than 1M']*GPF['is_amount_Less_than_1M']+\\\n",
    "    result.params['N']*GPF['is_Bid_N']+\\\n",
    "    result.params['RV']*GPF['is_security_type_RV']+\\\n",
    "    result.params['E']*GPF['is_taxable_code_E']+\\\n",
    "    result.params['T']*GPF['is_taxable_code_T']\n",
    "\n",
    "GPF.loc[GPF['CRRatio_hat']<0,'CRRatio_hat'] = 0\n",
    "GPF['CRRatio_hat'] = GPF['CRRatio_hat'].astype(float)\n",
    "\n",
    "upper_limit = np.percentile(GPF['CRRatio_hat'][np.logical_not(np.isnan(GPF['CRRatio_hat']))],99)\n",
    "lower_limit = np.percentile(GPF['CRRatio_hat'][np.logical_not(np.isnan(GPF['CRRatio_hat']))],1)\n",
    "GPF.loc[(GPF['CRRatio_hat']>upper_limit)&(np.logical_not(np.isnan(GPF['CRRatio_hat']))),'CRRatio_hat'] = \\\n",
    "    upper_limit\n",
    "GPF.loc[(GPF['CRRatio_hat']<lower_limit)&(np.logical_not(np.isnan(GPF['CRRatio_hat']))),'CRRatio_hat'] = \\\n",
    "    lower_limit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "# Insurance fee #\n",
    "#################\n",
    "\n",
    "# Run regression in the California sample\n",
    "# To make sure that order of first category is fixed\n",
    "CDA_All_Data = CDA_All_Data.sort_values(['GPF_maturity_bracket','GPF_amount_bracket','GPF_Bid','GPF_security_type','GPF_taxable_code'])\n",
    "maturity_bracket = pd.get_dummies(CDA_All_Data['GPF_maturity_bracket'], drop_first=True)\n",
    "amount_bracket = pd.get_dummies(CDA_All_Data['GPF_amount_bracket'], drop_first=True)\n",
    "Bid = pd.get_dummies(CDA_All_Data['GPF_Bid'], drop_first=True)\n",
    "security_type = pd.get_dummies(CDA_All_Data['GPF_security_type'], drop_first=True)\n",
    "taxable_code = pd.get_dummies(CDA_All_Data['GPF_taxable_code'], drop_first=True)\n",
    "\n",
    "CDA_All_Data_RegData = pd.concat([CDA_All_Data,maturity_bracket,amount_bracket,Bid,security_type,taxable_code],axis=1)\n",
    "CDA_All_Data_RegData = CDA_All_Data_RegData.dropna(subset=['InsureRatio'])\n",
    "CDA_All_Data_RegData['InsureRatio'] = CDA_All_Data_RegData['InsureRatio']*10000\n",
    "\n",
    "upper_limit = np.percentile(CDA_All_Data_RegData['InsureRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['InsureRatio']))],99)\n",
    "lower_limit = np.percentile(CDA_All_Data_RegData['InsureRatio'][np.logical_not(np.isnan(CDA_All_Data_RegData['InsureRatio']))],1)\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['InsureRatio']>upper_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['InsureRatio']))),'InsureRatio'] = \\\n",
    "    upper_limit\n",
    "CDA_All_Data_RegData.loc[(CDA_All_Data_RegData['InsureRatio']<lower_limit)&(np.logical_not(np.isnan(CDA_All_Data_RegData['InsureRatio']))),'InsureRatio'] = \\\n",
    "    lower_limit\n",
    "\n",
    "X = CDA_All_Data_RegData[['pop_to_avg','inc_to_avg']\n",
    "    +list(maturity_bracket.columns)+list(amount_bracket.columns)+list(Bid.columns)+list(security_type.columns)+list(taxable_code.columns)]\n",
    "y = CDA_All_Data_RegData['InsureRatio']\n",
    "\n",
    "CDA_All_Data_RegData.to_csv(\"../RawData/California/CDA_All_Data_RegData_InsureRatio.csv\")\n",
    "\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "# Predict would-be costs\n",
    "GPF['InsureRatio_hat'] = result.params['const']+\\\n",
    "    result.params['pop_to_avg']*GPF['pop_to_avg']+\\\n",
    "    result.params['inc_to_avg']*GPF['inc_to_avg']+\\\n",
    "    result.params['20y to 30y']*GPF['is_maturity_20y_to_30y']+\\\n",
    "    result.params['2y to 5y']*GPF['is_maturity_2y_to_5y']+\\\n",
    "    result.params['30y to 40y']*GPF['is_maturity_30y_to_40y']+\\\n",
    "    result.params['5y to 10y']*GPF['is_maturity_5y_to_10y']+\\\n",
    "    result.params['Greater than 40y']*GPF['is_maturity_Greater_than_40y']+\\\n",
    "    result.params['Less then 2y']*GPF['is_maturity_Less_than_2y']+\\\n",
    "    result.params['1M to 5M']*GPF['is_amount_1M_to_5M']+\\\n",
    "    result.params['50M to 100M']*GPF['is_amount_50M_to_100M']+\\\n",
    "    result.params['5M to 10M']*GPF['is_amount_5M_to_10M']+\\\n",
    "    result.params['Greater than 100M']*GPF['is_amount_Greater_than_100M']+\\\n",
    "    result.params['Less than 1M']*GPF['is_amount_Less_than_1M']+\\\n",
    "    result.params['N']*GPF['is_Bid_N']+\\\n",
    "    result.params['RV']*GPF['is_security_type_RV']+\\\n",
    "    result.params['E']*GPF['is_taxable_code_E']+\\\n",
    "    result.params['T']*GPF['is_taxable_code_T']\n",
    "\n",
    "GPF.loc[GPF['InsureRatio_hat']<0,'InsureRatio_hat'] = 0\n",
    "GPF['InsureRatio_hat'] = GPF['InsureRatio_hat'].astype(float)\n",
    "\n",
    "upper_limit = np.percentile(GPF['InsureRatio_hat'][np.logical_not(np.isnan(GPF['InsureRatio_hat']))],99)\n",
    "lower_limit = np.percentile(GPF['InsureRatio_hat'][np.logical_not(np.isnan(GPF['InsureRatio_hat']))],1)\n",
    "GPF.loc[(GPF['InsureRatio_hat']>upper_limit)&(np.logical_not(np.isnan(GPF['InsureRatio_hat']))),'InsureRatio_hat'] = \\\n",
    "    upper_limit\n",
    "GPF.loc[(GPF['InsureRatio_hat']<lower_limit)&(np.logical_not(np.isnan(GPF['InsureRatio_hat']))),'InsureRatio_hat'] = \\\n",
    "    lower_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161aa9f-155d-4fd5-a548-408f36e629f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------#\n",
    "# Export a version of GPF with imputed other costs #\n",
    "#--------------------------------------------------#\n",
    "\n",
    "GPF = GPF.drop(columns=[\n",
    "    'is_security_type_GO', 'is_security_type_RV', 'is_Bid_C', 'is_Bid_N',\n",
    "    'is_taxable_code_A', 'is_taxable_code_E', 'is_taxable_code_T', 'scaler',\n",
    "    'amount_inf_adjusted', 'is_amount_Less_than_1M', 'is_amount_1M_to_5M',\n",
    "    'is_amount_5M_to_10M', 'is_amount_10M_to_50M', 'is_amount_50M_to_100M',\n",
    "    'is_amount_Greater_than_100M', 'maturity_in_years',\n",
    "    'is_maturity_Less_than_2y', 'is_maturity_2y_to_5y',\n",
    "    'is_maturity_5y_to_10y', 'is_maturity_10y_to_20y',\n",
    "    'is_maturity_20y_to_30y', 'is_maturity_30y_to_40y',\n",
    "    'is_maturity_Greater_than_40y', 'pop_to_avg', 'inc_to_avg',\n",
    "    ])\n",
    "GPF.to_csv(\"../RawData/SDC/GPF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b438d-f179-472e-b436-2a00cc93ab02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
