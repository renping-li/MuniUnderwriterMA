{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874c45d1-6e1d-4cbd-b2bb-632deda4d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "try:\n",
    "    del(FUN_proc_name)\n",
    "except:\n",
    "    pass\n",
    "import FUN_proc_name\n",
    "importlib.reload(FUN_proc_name)\n",
    "from FUN_proc_name import FUN_proc_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb9ad0-82dc-4de2-843c-bcf72f93abba",
   "metadata": {},
   "source": [
    "# 1. Clean SDC M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936774d3-c2c3-4deb-bc96-2c005f521834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MA_Fin_19800101_19861231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19800101_19861231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19870101_19891231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19870101_19891231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19900101_19951231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19900101_19951231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19960101_19981231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19960101_19981231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_19990101_20011231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_19990101_20011231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20020101_20041231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20020101_20041231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20050101_20061231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20050101_20061231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20070101_20091231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20070101_20091231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20100101_20121231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20100101_20121231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20130101_20151231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20130101_20151231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20160101_20181231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20160101_20181231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20190101_20211231 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20190101_20211231.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA_Fin_20220101_20230930 = pd.read_excel(\"../RawData/SDC/M&A_Fin_20220101_20230930.xlsx\",skiprows=[0],\n",
    "    parse_dates=[' Rank Date','  Date\\nAnnounced','  Date\\nEffective','Date\\nEffective/\\nUnconditional','  Date\\nWithdrawn'],\n",
    "    dtype={'Acquiror Name':str,'Target Name':str})\n",
    "MA = pd.concat([\n",
    "    MA_Fin_19800101_19861231,\n",
    "    MA_Fin_19870101_19891231,\n",
    "    MA_Fin_19900101_19951231,\n",
    "    MA_Fin_19960101_19981231,\n",
    "    MA_Fin_19990101_20011231,\n",
    "    MA_Fin_20020101_20041231,\n",
    "    MA_Fin_20050101_20061231,\n",
    "    MA_Fin_20070101_20091231,\n",
    "    MA_Fin_20100101_20121231,\n",
    "    MA_Fin_20130101_20151231,\n",
    "    MA_Fin_20160101_20181231,\n",
    "    MA_Fin_20190101_20211231,\n",
    "    MA_Fin_20220101_20230930,\n",
    "])\n",
    "\n",
    "# Rename variable\n",
    "MA = MA.rename(columns={\n",
    "    '  Date\\nAnnounced':'date_announced',\n",
    "    '  Date\\nEffective':'date_effective',\n",
    "    'Target Name':'target_raw',\n",
    "    'Acquiror Name':'acquiror_raw',\n",
    "    '  %\\nOwned\\nAfter\\nTrans-\\naction':'new_share'})\n",
    "MA = MA[['date_announced','date_effective','target_raw','acquiror_raw','new_share','Synopsis','Status']]\n",
    "# Clean names\n",
    "MA['target'] = MA['target_raw'].apply(FUN_proc_name)\n",
    "MA['acquiror'] = MA['acquiror_raw'].apply(FUN_proc_name)\n",
    "MA['sale_year'] = pd.to_datetime(MA['date_effective']).dt.year\n",
    "MA['announce_year'] = pd.to_datetime(MA['date_announced']).dt.year\n",
    "\n",
    "# Export data\n",
    "MA.to_csv(\"../CleanData/SDC/0B_MA_SDC.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ec6ad-e04f-41a0-8be1-889f8f420089",
   "metadata": {},
   "source": [
    "# 2. Obtain M&A Deals from SDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb0c3a4-ae23-4568-a1ef-ecf248a7ecd6",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- This section does not export datafiles. Instead, I will handcheck all the deals that I obtain from SDC, and record them in a separate file which will be read in as one part of all M&As.\n",
    "- SDC M&A database fails to capture most of the deals actually. Therefore, I will mostly use hand-collected deals and supplement by examining SDC deals and record those reasonable ones. NIC data is used as sometimes I match GPF underwriters to SDC via parent names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eed600-ddfa-4157-a241-ebef1fabb5c0",
   "metadata": {},
   "source": [
    "## 2.1. Bank info from NIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44390a54-f2a1-4d83-9f1d-662582cd1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Import data\n",
    "ATTRIBUTES_ACTIVE = pd.read_csv(\"../RawData/NIC/CSV_ATTRIBUTES_ACTIVE.CSV\",\n",
    "    dtype={35:'str',45:'str',47:'str',63:'str'})\n",
    "ATTRIBUTES_ACTIVE['status'] = 'Active as of Sep 2023'\n",
    "ATTRIBUTES_CLOSED = pd.read_csv(\"../RawData/NIC/CSV_ATTRIBUTES_CLOSED.CSV\",\n",
    "    dtype={35:'str',38:'str',45:'str',47:'str',63:'str',71:'str'})\n",
    "ATTRIBUTES_CLOSED['status'] = 'Closed as of Sep 2023'\n",
    "# No overlap in \"#ID_RSSD\" between active and closed banks\n",
    "ATTRIBUTES_ALL = pd.concat([ATTRIBUTES_ACTIVE,ATTRIBUTES_CLOSED])\n",
    "\n",
    "# Clean names in NIC\n",
    "ATTRIBUTES_ALL = ATTRIBUTES_ALL[['#ID_RSSD','NM_LGL','status','STATE_ABBR_NM','STREET_LINE1']]\n",
    "ATTRIBUTES_ALL = ATTRIBUTES_ALL.rename(columns={'NM_LGL':'raw_name_NIC'})\n",
    "ATTRIBUTES_ALL['name_NIC'] = ATTRIBUTES_ALL['raw_name_NIC'].apply(FUN_proc_name)\n",
    "\n",
    "# Merge NIC banks and SDC municipal bond underwriters, which will be used for (1) constructing ownership relationship and\n",
    "# (2) obtain merger events based on NIC in the future\n",
    "\n",
    "Merged = GPF_names.\\\n",
    "    merge(ATTRIBUTES_ALL,left_on='name_GPF',right_on='name_NIC',how='outer',indicator=True)\n",
    "Merged = Merged[(Merged['_merge']=='left_only')|(Merged['_merge']=='both')]\n",
    "Merged = Merged.sort_values(['name_GPF','sale_year'])\n",
    "print(Merged.value_counts('_merge'))\n",
    "\n",
    "# Process SDC underwriter names manually for those not merged, then merge again with NIC\n",
    "New_Merged = Merged[Merged['_merge']=='left_only'].drop(columns=['_merge']).\\\n",
    "    merge(ATTRIBUTES_ALL,left_on='name_GPF',right_on='name_NIC',how='outer',indicator=True)\n",
    "New_Merged = New_Merged[(New_Merged['_merge']=='left_only')|(New_Merged['_merge']=='both')]\n",
    "Merged = pd.concat([Merged[Merged['_merge']=='both'],New_Merged])\n",
    "Merged = Merged.sort_values(['name_GPF','sale_year'])\n",
    "print(Merged.value_counts('_merge'))\n",
    "\n",
    "# A mapping from names to GPF to names & IDs in NIC. Note that one name in GPF can correspond to multiple names and IDs in NIC.\n",
    "# This is not very concerning as ultimately I care about names of banks rather than their codes, which is more often unique\n",
    "GPF_to_NIC = Merged[Merged['_merge']=='both'][['raw_name_GPF','name_GPF','sale_year','#ID_RSSD','raw_name_NIC','name_NIC']]\n",
    "GPF_to_NIC = GPF_to_NIC.groupby(['raw_name_GPF','name_GPF','sale_year'])\\\n",
    "    [['#ID_RSSD','raw_name_NIC','name_NIC']].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b158f87-2e25-4bfa-b938-656079556706",
   "metadata": {},
   "source": [
    "## 3.2. Bank ownership from NIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1f36a-5b34-404f-9bb7-9814f3b8d74a",
   "metadata": {},
   "source": [
    "Construct a mapping that links each firm to a unique parent (if it has one). E.g., the parent of Stifel, Nicolaus & Company, Incorporated is Stifel Financial Corp. This will be used for matching with merger information. For example, in SDC, the acquisitions made by Stifel are all recorded under the name Stifel Financial Corp.\n",
    "\n",
    "This relationship needs to be time-specific: sometimes an investment bank gets acquired by another but continue to operate under the original name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b8c519-6846-486f-88d1-06236cb45033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Import data\n",
    "RELATIONSHIPS = pd.read_csv(\"../RawData/NIC/CSV_RELATIONSHIPS.CSV\")\n",
    "# Restrict to scenarios where parent controls subsidiary\n",
    "RELATIONSHIPS = RELATIONSHIPS[RELATIONSHIPS['CTRL_IND']==1]\n",
    "# Convert \"RELATIONSHIPS\"\n",
    "RELATIONSHIPS['RELN_EST_year'] = pd.to_datetime(RELATIONSHIPS['DT_RELN_EST'],format='%Y%m%d').dt.year\n",
    "# For places where end date is indefinite (most likely relationship still valid), stipulate 2030 as end year\n",
    "RELATIONSHIPS.loc[RELATIONSHIPS['DT_END']==99991231,'DT_END'] = 20301231\n",
    "RELATIONSHIPS['END_year'] = pd.to_datetime(RELATIONSHIPS['DT_END'],format='%Y%m%d').dt.year\n",
    "\n",
    "# Relationship data has beginning and end dates, so I convert data to yearly based on that\n",
    "def proc_list(RELATIONSHIPS):\n",
    "    RELATIONSHIPS_byyear = []\n",
    "    for idx,row in RELATIONSHIPS.iterrows():\n",
    "        for year in range(row['RELN_EST_year'],row['END_year']+1):\n",
    "            RELATIONSHIPS_byyear = RELATIONSHIPS_byyear+[{\n",
    "                '#ID_RSSD_PARENT':row['#ID_RSSD_PARENT'],\n",
    "                'ID_RSSD_OFFSPRING':row['ID_RSSD_OFFSPRING'],\n",
    "                'year':year\n",
    "                }]\n",
    "    RELATIONSHIPS_byyear = pd.DataFrame(RELATIONSHIPS_byyear)\n",
    "    return RELATIONSHIPS_byyear\n",
    "\n",
    "RELATIONSHIPS_dd = dd.from_pandas(RELATIONSHIPS,npartitions=50)\n",
    "with dask.config.set(scheduler='processes',num_workers=10):\n",
    "    RELATIONSHIPS_byyear = RELATIONSHIPS_dd.map_partitions(proc_list).compute()\n",
    "\n",
    "# Note that this ownership hierachy can be multi-layered, so I collapse it and obtain the ultimate owner of each firm. There might be \n",
    "# cases where A is a subsidiary of B and B is a subsidiary of A, so the algorithm sets maximum iteration to break that\n",
    "\n",
    "RELATIONSHIPS_allyears = pd.DataFrame()\n",
    "for year in range(1980,2024):\n",
    "    RELATIONSHIPS_oneyear = RELATIONSHIPS_byyear[RELATIONSHIPS_byyear['year']==year]\n",
    "    RELATIONSHIPS_oneyear = RELATIONSHIPS_oneyear.drop_duplicates()\n",
    "    # Iteratively repeat the process\n",
    "    for i in range(1,10):\n",
    "        # Create a copy for merging\n",
    "        RELATIONSHIPS_oneyear_dup = RELATIONSHIPS_byyear[RELATIONSHIPS_byyear['year']==year].drop_duplicates().\\\n",
    "            rename(columns={'ID_RSSD_OFFSPRING':'#ID_RSSD_PARENT','#ID_RSSD_PARENT':'ult_parent'}).\\\n",
    "            drop(columns=['year'])\n",
    "        RELATIONSHIPS_oneyear = RELATIONSHIPS_oneyear.merge(RELATIONSHIPS_oneyear_dup,on='#ID_RSSD_PARENT',how='outer',indicator=True)\n",
    "        RELATIONSHIPS_oneyear = RELATIONSHIPS_oneyear[RELATIONSHIPS_oneyear['_merge']!='right_only']\n",
    "        # Modify data in cases where a parent can be found\n",
    "        RELATIONSHIPS_oneyear.loc[~pd.isnull(RELATIONSHIPS_oneyear['ult_parent']),'#ID_RSSD_PARENT'] = \\\n",
    "            RELATIONSHIPS_oneyear[~pd.isnull(RELATIONSHIPS_oneyear['ult_parent'])]['ult_parent']\n",
    "        RELATIONSHIPS_oneyear = RELATIONSHIPS_oneyear[['#ID_RSSD_PARENT','ID_RSSD_OFFSPRING','year']]\n",
    "    RELATIONSHIPS_oneyear = RELATIONSHIPS_oneyear.drop_duplicates(subset=['ID_RSSD_OFFSPRING'])\n",
    "    RELATIONSHIPS_allyears = pd.concat([RELATIONSHIPS_allyears,RELATIONSHIPS_oneyear])\n",
    "\n",
    "# Parent-subsidiary relationship is only interesting for those IDs that can be matched to GPF underwriters\n",
    "IDs_in_GPF_names = []\n",
    "for idx,row in GPF_to_NIC.iterrows():\n",
    "    IDs_in_GPF_names = IDs_in_GPF_names+row['#ID_RSSD']\n",
    "IDs_in_GPF_names = list(set(IDs_in_GPF_names))\n",
    "\n",
    "RELATIONSHIPS_allyears = RELATIONSHIPS_allyears.merge(\n",
    "    pd.DataFrame(IDs_in_GPF_names,columns=['ID_RSSD_OFFSPRING']),\n",
    "    on='ID_RSSD_OFFSPRING',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54c8d23-97d4-4ee5-a279-51ab472b2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Obtain the parent name based on \"RELATIONSHIPS\". Note that I have handled the cases where the parent-subsidiary relationship \n",
    "# can be multi-layered\n",
    "\n",
    "def proc_list(GPF_to_NIC):\n",
    "    GPF_to_NIC['parent_raw_name_NIC'] = None\n",
    "    GPF_to_NIC['parent_name_NIC'] = None\n",
    "    GPF_to_NIC['parent_ID_RSSD'] = None\n",
    "    # Go over each row and obtain the name of parent according to the \"RELATIONSHIP\" file\n",
    "    for idx,row in GPF_to_NIC.iterrows():\n",
    "        parent_raw_name_NICs = []\n",
    "        parent_name_NICs = []\n",
    "        for item in row['#ID_RSSD']:\n",
    "            relation_frag = RELATIONSHIPS_allyears[(RELATIONSHIPS_allyears['year']==row['sale_year'])&\n",
    "                (RELATIONSHIPS_allyears['ID_RSSD_OFFSPRING']==item)]\n",
    "            # Check if there is relationship data for a SDC underwriter\n",
    "            if len(relation_frag)>0:\n",
    "                parent_id = relation_frag['#ID_RSSD_PARENT'].values[0]\n",
    "                attribute_frag = ATTRIBUTES_ALL[ATTRIBUTES_ALL['#ID_RSSD']==parent_id]\n",
    "                parent_raw_name_NICs = parent_raw_name_NICs+list(attribute_frag['raw_name_NIC'].values)\n",
    "                parent_name_NICs = parent_name_NICs+list(attribute_frag['name_NIC'].values)\n",
    "        # Remove duplicates\n",
    "        parent_raw_name_NICs = list(set(parent_raw_name_NICs))\n",
    "        parent_name_NICs = list(set(parent_name_NICs))\n",
    "        GPF_to_NIC.at[idx,'parent_raw_name_NIC'] = parent_raw_name_NICs\n",
    "        GPF_to_NIC.at[idx,'parent_name_NIC'] = parent_name_NICs\n",
    "        GPF_to_NIC.at[idx,'parent_ID_RSSD'] = row['#ID_RSSD']\n",
    "    return GPF_to_NIC\n",
    "\n",
    "GPF_to_NIC_dd = dd.from_pandas(GPF_to_NIC,npartitions=50)\n",
    "with dask.config.set(scheduler='processes',num_workers=10):\n",
    "    GPF_to_NIC = GPF_to_NIC_dd.map_partitions(proc_list).compute()\n",
    "\n",
    "# There should not be any duplicates where one subsidiary is assigned to multiple parents. Check manually here and correct such cases.\n",
    "# Such cases are usually some banks that are common entities that share the same name (e.g, \"Community Bank\", \"Farmer's Bank\", \"People's\n",
    "# Bank\"), which are irrelevant as they have no underwriting business\n",
    "for idx,row in GPF_to_NIC.iterrows():\n",
    "    if len(GPF_to_NIC['parent_raw_name_NIC'][idx])==0:\n",
    "        GPF_to_NIC.at[idx,'parent_raw_name_NIC'] = None\n",
    "        GPF_to_NIC.at[idx,'parent_name_NIC'] = None\n",
    "    else:\n",
    "        GPF_to_NIC.at[idx,'parent_raw_name_NIC'] = GPF_to_NIC['parent_raw_name_NIC'][idx][0]\n",
    "        GPF_to_NIC.at[idx,'parent_name_NIC'] = FUN_proc_name(GPF_to_NIC.at[idx,'parent_raw_name_NIC'])\n",
    "\n",
    "# # No need to keep observations without parent data\n",
    "# GPF_to_NIC = GPF_to_NIC[~pd.isnull(GPF_to_NIC['parent_raw_name_NIC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b21736-1ad8-4a66-864f-5f54b426e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Edit GPF names to incorporate hand-correction results so it is aligned with firm names in NIC\n",
    "# Similarly, names in M&A data will be made to be aligned with NIC too\n",
    "GPF_names['parent_raw_name_NIC'] = None\n",
    "GPF_names['parent_name_NIC'] = None\n",
    "GPF_names = GPF_names.reset_index(drop=True)\n",
    "\n",
    "# Edit the list of GPF underwriters by augmenting parent info\n",
    "for idx,row in GPF_names.iterrows():\n",
    "    GPF_to_NIC_frag = GPF_to_NIC[(GPF_to_NIC['name_GPF']==row['name_GPF'])&(GPF_to_NIC['sale_year']==row['sale_year'])]\n",
    "    if len(GPF_to_NIC_frag)>0:\n",
    "        GPF_to_NIC_frag = GPF_to_NIC_frag.reset_index()\n",
    "        if GPF_to_NIC_frag['parent_raw_name_NIC'][0]!=None:\n",
    "            GPF_names.at[idx,'parent_raw_name_NIC'] = GPF_to_NIC_frag['parent_raw_name_NIC'][0]\n",
    "        if GPF_to_NIC_frag['parent_name_NIC'][0]!=None:\n",
    "            GPF_names.at[idx,'parent_name_NIC'] = GPF_to_NIC_frag['parent_name_NIC'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01a13b-2c61-4503-b9e4-b5bcf508d388",
   "metadata": {},
   "source": [
    "## 3.3. SDC M&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409d5bd-5e32-4d17-adeb-9f590a7660e8",
   "metadata": {},
   "source": [
    "Assemble two datasets:\n",
    "1) A list of M&A events. Such M&A events can be:\n",
    "    1) a subsidiary gets aquired by another firm, in which case its parent changes;\n",
    "    2) a parent gets aquired by another firm, in which case the parent of itself and all of its subsidiaries changes;\n",
    "    3) a merger occurs, in which case the merged entity takes a new name.\n",
    "2) Update the parent-subsidiary-by-time relationship based on the M&A that occurs.\n",
    "\n",
    "When merging SDC with GPF underwriter names, note that four kinds of situations will arise:\n",
    "1) A standalone/subsidiary acquires another standalone/subsidiary;\n",
    "2) A parent acquires a standalone/subsidiary;\n",
    "3) A standalone/subsidiary acquires a parent;\n",
    "4) A parent acquires a parent.\n",
    "\n",
    "Steps are:\n",
    "1) Do the exact matches for all four situations. Arrive at four sets of events each indicating which kind of merge it is.\n",
    "2) Get deals without an exact match in any four situations. Do the fuzzy matches for all four situations.\n",
    "3) Hand check results of fuzzy matches and obtain four datasets indicating what kind of fuzzy match it is.\n",
    "4) Handle overlapping cases based on reasonable rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0582e2-6f42-4eeb-b596-7f5edf21e181",
   "metadata": {},
   "source": [
    "**Exact match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f19cfe9-77cf-47f7-b2dd-3457906789d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Import M&A events\n",
    "MA = pd.read_csv(\"../CleanData/SDC/0B_MA_SDC.csv\",low_memory=False)\n",
    "MA = MA[MA['new_share']>50]\n",
    "MA = MA[~pd.isnull(MA['date_effective'])]\n",
    "MA = MA.drop(columns='Unnamed: 0')\n",
    "\n",
    "# Case 1: A standalone/subsidiary acquires another standalone/subsidiary\n",
    "\n",
    "# A version of \"GPF_names\" that removes the year dimension\n",
    "GPF_names_noyear = GPF_names[['name_GPF']].drop_duplicates()\n",
    "\n",
    "# Merge SDC with names in GPF, first by subsidiary name\n",
    "Merged = MA.merge(GPF_names_noyear.rename(columns={'name_GPF':'target','raw_name_GPF':'raw_name_GPF_for_target'}),\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_target'})\n",
    "\n",
    "# Merge again with parent name\n",
    "Merged = Merged.merge(GPF_names_noyear.rename(columns={'name_GPF':'acquiror','raw_name_GPF':'raw_name_GPF_for_acquiror'}),\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_acquiror'})\n",
    "\n",
    "# Retain those perfectly matched ones. Note that the matches will be much less if I require that, in the year of merger, both the\n",
    "# parent and the subsidiary have active municipal bond underwriting business\n",
    "Merged_Perfect_BothSub = Merged[(Merged['_merge_by_target']=='both')&(Merged['_merge_by_acquiror']=='both')]\n",
    "\n",
    "# Case 2: A parent acquires a standalone/subsidiary\n",
    "\n",
    "# Note that the prominent Stifel cases are captured in this step\n",
    "\n",
    "GPF_parent_names_noyear = GPF_names[['parent_name_NIC']].drop_duplicates()\n",
    "GPF_parent_names_noyear = GPF_parent_names_noyear.rename(columns={'parent_name_NIC':'acquiror'})\n",
    "\n",
    "# Merge SDC with names in GPF, first by subsidiary name\n",
    "Merged = MA.merge(GPF_names_noyear.rename(columns={'name_GPF':'target','raw_name_GPF':'raw_name_GPF_for_target'}),\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_target'})\n",
    "\n",
    "# Merge again with parent name\n",
    "Merged = Merged.merge(GPF_parent_names_noyear,\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_acquiror'})\n",
    "\n",
    "# Retain those perfectly matched ones. Note that the matches will be much less if I require that, in the year of merger, both the\n",
    "# parent and the subsidiary have active municipal bond underwriting business\n",
    "Merged_Perfect_AcqIsPar = Merged[(Merged['_merge_by_target']=='both')&(Merged['_merge_by_acquiror']=='both')]\n",
    "\n",
    "# Case 3: A standalone/subsidiary acquires a parent\n",
    "\n",
    "GPF_parent_names_noyear = GPF_names[['parent_name_NIC']].drop_duplicates()\n",
    "GPF_parent_names_noyear = GPF_parent_names_noyear.rename(columns={'parent_name_NIC':'target'})\n",
    "\n",
    "# Merge SDC with names in GPF, first by subsidiary name\n",
    "Merged = MA.merge(GPF_parent_names_noyear,\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_target'})\n",
    "\n",
    "# Merge again with parent name\n",
    "Merged = Merged.merge(GPF_names_noyear.rename(columns={'name_GPF':'acquiror','raw_name_GPF':'raw_name_GPF_for_acquiror'}),\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_acquiror'})\n",
    "\n",
    "# Retain those perfectly matched ones. Note that the matches will be much less if I require that, in the year of merger, both the\n",
    "# parent and the subsidiary have active municipal bond underwriting business\n",
    "Merged_Perfect_TargetIsPar = Merged[(Merged['_merge_by_target']=='both')&(Merged['_merge_by_acquiror']=='both')]\n",
    "\n",
    "# Case 4: A parent acquires a parent\n",
    "\n",
    "# Merge SDC with names in GPF, first by subsidiary name\n",
    "GPF_parent_names_noyear = GPF_names[['parent_name_NIC']].drop_duplicates()\n",
    "GPF_parent_names_noyear = GPF_parent_names_noyear.rename(columns={'parent_name_NIC':'target'})\n",
    "Merged = MA.merge(GPF_parent_names_noyear,\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_target'})\n",
    "\n",
    "# Merge again with parent name\n",
    "GPF_parent_names_noyear = GPF_names[['parent_name_NIC']].drop_duplicates()\n",
    "GPF_parent_names_noyear = GPF_parent_names_noyear.rename(columns={'parent_name_NIC':'acquiror'})\n",
    "Merged = Merged.merge(GPF_parent_names_noyear,\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "Merged = Merged[Merged['_merge']!='right_only'].rename(columns={'_merge':'_merge_by_acquiror'})\n",
    "\n",
    "# Retain those perfectly matched ones. Note that the matches will be much less if I require that, in the year of merger, both the\n",
    "# parent and the subsidiary have active municipal bond underwriting business\n",
    "Merged_Perfect_BothPar = Merged[(Merged['_merge_by_target']=='both')&(Merged['_merge_by_acquiror']=='both')]\n",
    "\n",
    "Merged_Perfect = pd.concat([\n",
    "    Merged_Perfect_BothSub,\n",
    "    Merged_Perfect_AcqIsPar,\n",
    "    Merged_Perfect_TargetIsPar,\n",
    "    Merged_Perfect_BothPar,\n",
    "    ])\n",
    "Merged_Perfect = Merged_Perfect.drop_duplicates(['acquiror','target','sale_year'])\n",
    "\n",
    "MA_Unmatched = MA.merge(Merged_Perfect[['acquiror','target','sale_year']],on=['acquiror','target','sale_year'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']=='left_only']\n",
    "\n",
    "# Extract unmatched SDC names of subsidiaries\n",
    "SDC_Unmatched_target = MA_Unmatched[['target']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Extract unmatched SDC names of parents\n",
    "SDC_Unmatched_acquiror = MA_Unmatched[['acquiror']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa580eea-40c0-43b3-8c61-f6b0da7b17c2",
   "metadata": {},
   "source": [
    "### Fuzzy match block - Start\n",
    "\n",
    "**Fuzzy match**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fb6413-c3bf-4679-b904-3ff9523ea5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# For the fuzzy match results, only those for which both parent and subsidiary have potential fuzzy OR exact matches are of interest\n",
    "\n",
    "def proc_list(SDC_Unmatched_target):\n",
    "\n",
    "    fuzzy_matches_targets = []\n",
    "    for idx,row in SDC_Unmatched_target.iterrows():\n",
    "        ratio = process.extract(row['target'],GPF_names_noyear['name_GPF'],limit=3,scorer=fuzz.partial_ratio)\n",
    "        for item in ratio:\n",
    "            if item[1]>=90:\n",
    "                fuzzy_matches_targets = fuzzy_matches_targets+[{\n",
    "                    'target':row['target'],\n",
    "                    'name_GPF':item[0],\n",
    "                    'score':item[1],\n",
    "                }]\n",
    "    fuzzy_matches_targets = pd.DataFrame(fuzzy_matches_targets)\n",
    "    \n",
    "    return fuzzy_matches_targets\n",
    "\n",
    "SDC_Unmatched_target_dd = dd.from_pandas(SDC_Unmatched_target, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    fuzzy_matches_targets = SDC_Unmatched_target_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=['target','name_GPF','score'])).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ede48f8-0107-4cfa-ab8d-cf151fbeea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "def proc_list(SDC_Unmatched_acquiror):\n",
    "\n",
    "    fuzzy_matches_acquirors = []\n",
    "    for idx,row in SDC_Unmatched_acquiror.iterrows():\n",
    "        ratio = process.extract(row['acquiror'],GPF_names_noyear['name_GPF'],limit=3,scorer=fuzz.partial_ratio)\n",
    "        for item in ratio:\n",
    "            if item[1]>=90:\n",
    "                fuzzy_matches_acquirors = fuzzy_matches_acquirors+[{\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'name_GPF':item[0],\n",
    "                    'score':item[1],\n",
    "                }]\n",
    "    fuzzy_matches_acquirors = pd.DataFrame(fuzzy_matches_acquirors)\n",
    "    \n",
    "    return fuzzy_matches_acquirors\n",
    "\n",
    "SDC_Unmatched_acquiror_dd = dd.from_pandas(SDC_Unmatched_acquiror, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    fuzzy_matches_acquirors = SDC_Unmatched_acquiror_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=['acquiror','name_GPF','score'])).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01b949-08c9-4b3c-a163-10a6ed0eb0ce",
   "metadata": {},
   "source": [
    "**Process unmatched deals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adbdb0e6-fa1c-4ff3-849e-c49a03290340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Format the dataframe for next step's matching\n",
    "\n",
    "# Try to find an exact match for either parent for subsidiary\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether target has an exact match with a standalone/subsidiary in GPF\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['name_GPF','sale_year']].rename(columns={'name_GPF':'target'}),\n",
    "    on=['target','sale_year'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_target_exact'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether acquirer has an exact match with a standalone/subsidiary in GPF\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['name_GPF','sale_year']].rename(columns={'name_GPF':'acquiror'}),\n",
    "    on=['acquiror','sale_year'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_acquiror_exact'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether target has an exact match with a parent\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['parent_name_NIC','sale_year']].rename(columns={'parent_name_NIC':'target'}).drop_duplicates(),\n",
    "    on=['target','sale_year'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_target_exact_to_parent'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether acquirer has an exact match with a parent\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['parent_name_NIC','sale_year']].rename(columns={'parent_name_NIC':'acquiror'}).drop_duplicates(),\n",
    "    on=['acquiror','sale_year'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_acquiror_exact_to_parent'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# Note that for unmatched deals (\"MA_Unmatched\"), it could be that one side actually has a perfect match to either a standalone firm/\n",
    "# subsidiary or to a parent. I try to look for a match of either kind. Those deals for which either (1) parent has no fuzzy or exact\n",
    "# match or (2) subsidiary has no fuzzy or exact match will be dropped\n",
    "\n",
    "MA_Unmatched = MA_Unmatched.merge(fuzzy_matches_acquirors[['acquiror','name_GPF','score']].\\\n",
    "    rename(columns={'name_GPF':'name_GPF_acquiror_fuzzy','score':'score_acquiror'}),\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[(MA_Unmatched['_merge']=='both')|\n",
    "    (MA_Unmatched['if_acquiror_exact'])|(MA_Unmatched['if_acquiror_exact_to_parent'])]\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "# Keep if there is a fuzzy match OR an exact match obtained previously\n",
    "MA_Unmatched = MA_Unmatched.merge(fuzzy_matches_targets[['target','name_GPF','score']].\\\n",
    "    rename(columns={'name_GPF':'name_GPF_target_fuzzy','score':'score_target'}),\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[(MA_Unmatched['_merge']=='both')|\n",
    "    (MA_Unmatched['if_target_exact'])|(MA_Unmatched['if_target_exact_to_parent'])]\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "# In 'Unmerged_Deals', 'name_GPF_subsidiary_fuzzy'/'name_GPF_parent_fuzzy' are fuzzy matches\n",
    "\n",
    "# Note that some cases the fuzzy match is exact but 'if_acquiror_exact' and 'if_target_exact' is false, which is only because that\n",
    "# merging requires also the time dimension\n",
    "\n",
    "# Sort columns for visual inspection\n",
    "first_columns = ['sale_year',\n",
    "    'acquiror','name_GPF_acquiror_fuzzy','if_acquiror_exact','if_acquiror_exact_to_parent',\n",
    "    'target','name_GPF_target_fuzzy','if_target_exact','if_target_exact_to_parent']\n",
    "other_columns = [item for item in MA_Unmatched.columns if item not in first_columns]\n",
    "MA_Unmatched = MA_Unmatched[first_columns+other_columns]\n",
    "# Note that, based on the way \"SDC_Unmatched_acquiror\" and \"SDC_Unmatched_target\" are constructed, it is possible they have an exact match, as\n",
    "# it is sent to fuzzy matching as long as one side does not have an exact match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59680b-e2e9-4532-93e5-b5678d53b985",
   "metadata": {},
   "source": [
    "### Fuzzy match block - End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4a2e0b-803d-468e-a520-1628bd9dc8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# For those unmatched deals where both target and acquiror can be fuzzy/exact matched, I handcheck them and record how the target/acquiror\n",
    "# name can be edited so that they can be exact-matched to underwriter names in GPF, and recorded the edits in \"SCRIPT_clean_M&A.py\"\n",
    "\n",
    "fieldname = 'acquiror'\n",
    "%run -i SCRIPT_0B_clean_M&A.py\n",
    "fieldname = 'target'\n",
    "%run -i SCRIPT_0B_clean_M&A.py\n",
    "\n",
    "# Now underwriter names in M&A dataset has been hand-cleaned, many more exact matches can be found\n",
    "\n",
    "# Find an exact match for either parent for subsidiary\n",
    "# If fuzzy match is run\n",
    "if IF_fuzzy_match:\n",
    "    MA_Unmatched = MA_Unmatched.drop(columns=[\n",
    "        'name_GPF_acquiror_fuzzy','if_acquiror_exact','if_acquiror_exact_to_parent',\n",
    "        'name_GPF_target_fuzzy','if_target_exact','if_target_exact_to_parent',\n",
    "        'score_acquiror','score_target'])\n",
    "else:\n",
    "    MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether target has an exact match with a standalone/subsidiary in GPF\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['name_GPF']].drop_duplicates().rename(columns={'name_GPF':'target'}),\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_target_exact'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether acquirer has an exact match with a standalone/subsidiary in GPF\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['name_GPF']].drop_duplicates().rename(columns={'name_GPF':'acquiror'}),\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_acquiror_exact'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether target has an exact match with a parent\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['parent_name_NIC']].drop_duplicates().rename(columns={'parent_name_NIC':'target'}).drop_duplicates(),\n",
    "    on=['target'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_target_exact_to_parent'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "# An indicator for whether acquirer has an exact match with a parent\n",
    "MA_Unmatched = MA_Unmatched.merge(GPF_names[['parent_name_NIC']].drop_duplicates().rename(columns={'parent_name_NIC':'acquiror'}).drop_duplicates(),\n",
    "    on=['acquiror'],how='outer',indicator=True)\n",
    "MA_Unmatched = MA_Unmatched[MA_Unmatched['_merge']!='right_only']\n",
    "MA_Unmatched['if_acquiror_exact_to_parent'] = MA_Unmatched['_merge']=='both'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['_merge'])\n",
    "\n",
    "MA_Unmatched = MA_Unmatched[\n",
    "    (MA_Unmatched['if_target_exact']|MA_Unmatched['if_target_exact_to_parent'])&\n",
    "    (MA_Unmatched['if_acquiror_exact']|MA_Unmatched['if_acquiror_exact_to_parent'])]\n",
    "\n",
    "# Reorder columns\n",
    "MA_Unmatched['tag'] = None\n",
    "MA_Unmatched.loc[MA_Unmatched['if_acquiror_exact']&MA_Unmatched['if_target_exact'],'tag'] = 'Merged_Perfect_BothSub'\n",
    "MA_Unmatched.loc[MA_Unmatched['if_acquiror_exact']&MA_Unmatched['if_target_exact_to_parent'],'tag'] = 'Merged_Perfect_TargetIsPar'\n",
    "MA_Unmatched.loc[MA_Unmatched['if_acquiror_exact_to_parent']&MA_Unmatched['if_target_exact'],'tag'] = 'Merged_Perfect_AcqIsPar'\n",
    "MA_Unmatched.loc[MA_Unmatched['if_acquiror_exact_to_parent']&MA_Unmatched['if_target_exact_to_parent'],'tag'] = 'Merged_Perfect_BothPar'\n",
    "MA_Unmatched = MA_Unmatched.drop(columns=['if_acquiror_exact','if_target_exact','if_acquiror_exact_to_parent','if_target_exact_to_parent'])\n",
    "MA_Fuzzy = MA_Unmatched\n",
    "\n",
    "# Combine exact and fuzzy matches results\n",
    "    \n",
    "# Note that I still need to\n",
    "# (1) Handle fuzzy matches, especially noting cases where one side has an exact match\n",
    "# (2) Check if those included in the news articles are in sample\n",
    "# (3) Check S&P mergers\n",
    "# and these steps will generate much more cases of M&A\n",
    "\n",
    "Merged_Perfect_BothSub = Merged_Perfect_BothSub.copy()\n",
    "Merged_Perfect_BothSub['tag'] = None\n",
    "Merged_Perfect_BothSub['tag'] = 'Merged_Perfect_BothSub'\n",
    "Merged_Perfect_AcqIsPar = Merged_Perfect_AcqIsPar.copy()\n",
    "Merged_Perfect_AcqIsPar['tag'] = None\n",
    "Merged_Perfect_AcqIsPar['tag'] = 'Merged_Perfect_AcqIsPar'\n",
    "Merged_Perfect_TargetIsPar = Merged_Perfect_TargetIsPar.copy()\n",
    "Merged_Perfect_TargetIsPar['tag'] = None\n",
    "Merged_Perfect_TargetIsPar['tag'] = 'Merged_Perfect_TargetIsPar'\n",
    "Merged_Perfect_BothPar = Merged_Perfect_BothPar.copy()\n",
    "Merged_Perfect_BothPar['tag'] = None\n",
    "Merged_Perfect_BothPar['tag'] = 'Merged_Perfect_BothPar'\n",
    "Merged_Perfect = pd.concat([\n",
    "    Merged_Perfect_BothSub,\n",
    "    Merged_Perfect_AcqIsPar,\n",
    "    Merged_Perfect_TargetIsPar,\n",
    "    Merged_Perfect_BothPar,\n",
    "    ])\n",
    "\n",
    "Merged_Perfect = Merged_Perfect.reset_index(drop=True).sort_values('sale_year')\n",
    "# Remove cases where acquirer and target are the same firm\n",
    "Merged_Perfect = Merged_Perfect[Merged_Perfect['acquiror']!=Merged_Perfect['target']]\n",
    "\n",
    "MA_from_SDC = pd.concat([MA_Fuzzy,Merged_Perfect])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd548456-f998-4ad2-bd2c-b32e881553f9",
   "metadata": {},
   "source": [
    "# 3. Create Investment Bank M&A Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c352522-1ba7-4fd2-a1b8-ed22425c3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those obtained via manual searching\n",
    "MA_handsearch = pd.read_csv('SCRIPT_hand_search_M&A.csv')\n",
    "# Those obtained from SDC\n",
    "MA_SDC = pd.read_csv('SCRIPT_SDC_deals_cleaned.csv')\n",
    "MA = pd.concat([MA_handsearch,MA_SDC])\n",
    "MA = MA.reset_index(drop=True)\n",
    "MA.to_parquet('../CleanData/SDC/0B_M&A.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
