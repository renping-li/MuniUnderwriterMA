{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a72d83-43f3-4727-a6fb-b5b9315a3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d53dfd3-e972-4437-9e20-c6ebfa389753",
   "metadata": {},
   "source": [
    "# 1. Clean Mergent Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4d944-110f-4c0b-bd7d-35a7136c86e2",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Gao, Murphy, and Qi (2019) has step by step guidance on how to calculate call-adjusted yield **spread** from Mergent data, which seems to be an ideal reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a509a98-4d38-40d2-8295-8cc747e464fb",
   "metadata": {},
   "source": [
    "## 1.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e9c128-dc72-4ca4-9625-c96dbe43a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# (1) \"coupon_code_c\" = FXD, ODF, OID, OIP (& zero-coupon bond) correspond to fixed rate bonds\n",
    "\n",
    "# Import data\n",
    "bondinfo = pd.read_csv('../RawData/Mergent/bondinfo.csv',on_bad_lines='skip',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "\n",
    "issueinfo = pd.read_csv('../RawData/Mergent/issueinfo.csv',on_bad_lines='skip',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "# In cases such as \"LEXINGTON-FAYETTE\", actually two cities/counties are referred to. Replace '-' to facilitate later parsing\n",
    "issueinfo['issuer_long_name_c'] = issueinfo['issuer_long_name_c'].str.replace('-',' ')\n",
    "issueinfo['issuer_short_name_c'] = issueinfo['issuer_short_name_c'].str.replace('-',' ')\n",
    "\n",
    "# Clean \"offering type\"\n",
    "issueinfo.loc[issueinfo['offering_type_c']==' ','offering_type_c'] = 'MISSING'\n",
    "issueinfo.loc[issueinfo['offering_type_c']=='LTD','offering_type_c'] = 'MISSING'\n",
    "issueinfo.loc[issueinfo['offering_type_c']=='*','offering_type_c'] = 'MISSING'\n",
    "issueinfo.loc[issueinfo['offering_type_c']=='18980000.00000','offering_type_c'] = 'MISSING'\n",
    "\n",
    "# Clean \"issue description\"\n",
    "issueinfo['issue_description_Processed'] = issueinfo['issue_description_c'].str.strip()\n",
    "issueinfo['issue_description_Processed'] = issueinfo['issue_description_Processed'].str.upper()\n",
    "issueinfo.loc[issueinfo['issue_description_Processed'].str.contains('GENERAL OBLIGATION'),\n",
    "    'issue_description_Processed'] = \"GENERAL OBLIGATION\"\n",
    "issueinfo.loc[issueinfo['issue_description_Processed'].str.contains('REVENUE'),\n",
    "    'issue_description_Processed'] = \"REVENUE\"\n",
    "issueinfo.loc[issueinfo['issue_description_Processed'].str.contains('UNLIMITED TAX'),\n",
    "    'issue_description_Processed'] = \"REVENUE\"\n",
    "issueinfo.loc[issueinfo['issue_description_Processed'].str.contains('SCHOOL'),\n",
    "    'issue_description_Processed'] = \"REVENUE\"\n",
    "issueinfo.loc[(issueinfo['issue_description_Processed']!=\"REVENUE\")&(issueinfo['issue_description_Processed']!=\"GENERAL OBLIGATION\"),\n",
    "    'issue_description_Processed'] = \"OTHERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45882df5-8a85-4701-8cdd-6f0b1bcedaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# (1) Some counties, including those that issue municipal bonds, are not part of any CBSA. See Harlan County as in\n",
    "# https://en.wikipedia.org/wiki/Kentucky_statistical_areas.\n",
    "# (2) To avoid issues from counties with \"&\" in between be identified as issues from multiple counties, and also to facilitate \n",
    "# parsing of cities, I pre-process both county/city data and issuer data by (1) Adding \"_\"s for any county/city names with more\n",
    "# than one words, and (2) Replacing phrases of names in issuer data with versions that have different words connected with \"_\" \n",
    "# in between.\n",
    "\n",
    "###################\n",
    "# List of issuers #\n",
    "###################\n",
    "\n",
    "# Match each issue to a county (if possible) by name\n",
    "issuers = pd.DataFrame(issueinfo.value_counts(['issuer_long_name_c','issuer_short_name_c','state_c']))\n",
    "issuers = issuers.reset_index()\n",
    "issuers = issuers.rename(columns={0:'n_issues'})\n",
    "# Create a unique ID for each issuer\n",
    "issuers['issuer_id'] = range(0,len(issuers))\n",
    "issueinfo = issueinfo.merge(issuers,on=['issuer_long_name_c','issuer_short_name_c','state_c'])\n",
    "\n",
    "%run -i SCRIPT_us_states.py\n",
    "\n",
    "###############\n",
    "# Import CBSA #\n",
    "###############\n",
    "\n",
    "# \"CSA\" is for metropolitan and \"CBSA\" includes also those micropolitan\n",
    "CBSA = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSA = CBSA[~pd.isnull(CBSA['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSA = CBSA.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSA = CBSA.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSA = CBSA[CBSA['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSA['County'] = CBSA['County'].str.upper()\n",
    "CBSA['County'] = CBSA['County'].str.replace(' COUNTY','')\n",
    "# Facilitate matching in scenarios like \"Lewis and Clark County\"\n",
    "CBSA['County'] = CBSA['County'].str.replace(' AND ',' & ')\n",
    "# Add \"_\" to county names with spaces\n",
    "CBSA['County'] = CBSA['County'].str.replace(' ','_')\n",
    "CBSA['County'] = CBSA['County'].str.replace('.','',regex=False)\n",
    "\n",
    "###################\n",
    "# Import counties #\n",
    "###################\n",
    "\n",
    "# Complete list of counties, including those not part of CSA \n",
    "all_counties = pd.read_csv(\"../RawData/MSA/fips-by-state.csv\",sep=',',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "all_counties = all_counties.rename(columns={'name':'County','state':'State'})\n",
    "all_counties['County'] = all_counties['County'].str.upper()\n",
    "all_counties['County'] = all_counties['County'].str.replace(' COUNTY','')\n",
    "all_counties['County'] = all_counties['County'].str.replace(' AND ',' & ')\n",
    "all_counties['County'] = all_counties['County'].str.replace(' ','_')\n",
    "all_counties['County'] = all_counties['County'].str.replace('.','',regex=False)\n",
    "\n",
    "#######################################\n",
    "# Import cities and city equivalences #\n",
    "#######################################\n",
    "\n",
    "# (https://github.com/grammakov/USA-cities-and-states/blob/master/us_cities_states_counties.csv)\n",
    "all_cities = pd.read_csv(\"../RawData/MSA/us_cities_states_counties.csv\",sep='|',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "all_cities_alias = all_cities.drop(columns=['City']).rename(columns={'City alias':'City'})\n",
    "all_cities = pd.concat([all_cities.drop(columns=['City alias']),all_cities_alias])\n",
    "all_cities = all_cities.drop_duplicates()\n",
    "all_cities['City'] = all_cities['City'].str.upper()\n",
    "# Facilitate matching in scenarios like \"Lewis and Clark County\"\n",
    "all_cities['County'] = all_cities['County'].str.replace(' AND ',' & ')\n",
    "all_cities['City'] = all_cities['City'].str.replace(' AND ',' & ')\n",
    "# Add \"_\" to county names with spaces\n",
    "all_cities['County'] = all_cities['County'].str.replace(' ','_')\n",
    "all_cities['City'] = all_cities['City'].str.replace(' ','_')\n",
    "all_cities['County'] = all_cities['County'].str.replace('.','',regex=False)\n",
    "all_cities['City'] = all_cities['City'].str.replace('.','',regex=False)\n",
    "all_cities = all_cities.rename(columns={'State short':'State'})\n",
    "\n",
    "##########################\n",
    "# Import school district #\n",
    "##########################\n",
    "\n",
    "all_schooldistrcts = pd.read_csv(\"../RawData/MSA/school-districts_lea_directory.csv\",low_memory=False)\n",
    "all_schooldistrcts = all_schooldistrcts[['lea_name','state_mailing','county_name']]\n",
    "all_schooldistrcts = all_schooldistrcts.drop_duplicates()\n",
    "all_schooldistrcts = all_schooldistrcts.rename(\n",
    "    columns={'lea_name':'SchoolDistrict','state_mailing':'State','county_name':'County'})\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.upper()\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' COUNTY','')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' AND ',' & ')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' ','_')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace('.','',regex=False)\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.upper()\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.replace(' AND ',' & ')\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.replace('.','',regex=False)\n",
    "\n",
    "all_schooldistrcts = all_schooldistrcts[~pd.isnull(all_schooldistrcts['SchoolDistrict'])]\n",
    "all_schooldistrcts[all_schooldistrcts['County']!='-1']\n",
    "all_schooldistrcts[all_schooldistrcts['County']!='-2']\n",
    "\n",
    "# A version of school district name that has subfixes removed, which will be used to merge with issuer data\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict']\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace('-',' ')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' INC','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CORP','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DISTRICT','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' DISTRICT','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' LOCAL SCHOOL DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DIS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCH DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCH DIS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEMENTARY','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEM','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ISD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' DISTRIC','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' LOCAL SD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CSD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' AREA SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' MUNICIPAL SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' PUBLIC SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' TOWNSHIP','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CHARTER SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' HIGH SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' PREPARATORY SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEMENTARY SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04639501-cc70-44c4-980d-4bebf1509d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When maching with school districts:\n",
    "# (1) Clean numbers from both \"all_schooldistrcts\" and \"issuer\" names, including those that start with \"#\".\n",
    "# (2) Clean state names, usually with three characters, from \"issuer\" names\n",
    "all_schooldistrcts[all_schooldistrcts['SchoolDistrict_NoSubfix'].str.contains('RAYMOND')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66515aa1-4a4e-4f42-ae73-31a3822e808f",
   "metadata": {},
   "source": [
    "## 1.2 Match Mergent issuer with county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e23d1-79c7-490e-a4a3-6a980f4a97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Round 1: Match by county or city names #\n",
    "##########################################\n",
    "\n",
    "def proc_list(issuers):\n",
    "\n",
    "    issuers = issuers.copy()\n",
    "    issuers['issuer_long_name_c_Processed'] = issuers['issuer_long_name_c']\n",
    "    \n",
    "    # Pre-process issuer data and replace phrases of names in issuer data with versions that have different words connected with\n",
    "    # \"_\" in between. This facilitates parsing and matching. Note that in this step I add \"_\" to distinguish blocks in issuer names\n",
    "    # only if the specific issuer is in the same state as the entity for which the name has multiple words. This addresses cases like\n",
    "    # \"Thief River Falls\" and \"River Falls\" which are in different states\n",
    "    for idx,row in all_counties[all_counties['County'].str.contains('_')].iterrows():\n",
    "        issuers.loc[issuers['state_c']==row['State'],'issuer_long_name_c_Processed'] = \\\n",
    "            issuers[issuers['state_c']==row['State']]['issuer_long_name_c_Processed'].str.replace(\n",
    "            row['County'].replace('_',' '),row['County'],regex=False)\n",
    "    for idx,row in all_cities[all_cities['City'].str.contains('_')].iterrows():\n",
    "        issuers.loc[issuers['state_c']==row['State'],'issuer_long_name_c_Processed'] = \\\n",
    "            issuers[issuers['state_c']==row['State']]['issuer_long_name_c_Processed'].str.replace(\n",
    "            row['City'].replace('_',' '),row['City'],regex=False)\n",
    "\n",
    "    # Initialize fields\n",
    "    issuers['County'] = None\n",
    "    issuers['County_2'] = None\n",
    "    issuers['County_3'] = None\n",
    "    issuers['County_4'] = None\n",
    "    issuers['County_5'] = None\n",
    "    issuers['City'] = None\n",
    "\n",
    "    # Check if it is a county\n",
    "    for idx,row in issuers.iterrows():\n",
    "    \n",
    "        ###########################################################################################\n",
    "        # Handle those clean cases where county names and the key word \"CNTY\" are in issuer names #\n",
    "        ###########################################################################################\n",
    "\n",
    "        # If issue is by one single county\n",
    "        if ' CNTY ' in row['issuer_long_name_c_Processed']:\n",
    "            name_county = row['issuer_long_name_c_Processed'].split(' CNTY ')[0]\n",
    "            all_counties_frag = all_counties[(all_counties['County']==name_county)&(all_counties['State']==row['state_c'])]\\\n",
    "                .reset_index()\n",
    "            if len(all_counties_frag)==1:\n",
    "                issuers.at[idx,'County'] = name_county\n",
    "        # If issue is by multiple counties\n",
    "        elif ' CNTYS ' in row['issuer_long_name_c_Processed']:\n",
    "            name_county = row['issuer_long_name_c_Processed'].split(' CNTYS ')[0]\n",
    "            name_county = name_county.replace(' & ',' ')\n",
    "            name_counties = name_county.split(' ')\n",
    "            all_counties_frag = all_counties[(all_counties['County']==name_counties[0])&(all_counties['State']==row['state_c'])]\\\n",
    "                .reset_index()\n",
    "            if len(all_counties_frag)==1:\n",
    "                issuers.at[idx,'County'] = name_counties[0]\n",
    "            if len(name_counties)>=2:\n",
    "                all_counties_frag = all_counties[(all_counties['County']==name_counties[1])&(all_counties['State']==row['state_c'])]\\\n",
    "                    .reset_index()\n",
    "                if len(all_counties_frag)==1:\n",
    "                    issuers.at[idx,'County_2'] = name_counties[1]\n",
    "            if len(name_counties)>=3:\n",
    "                all_counties_frag = all_counties[(all_counties['County']==name_counties[2])&(all_counties['State']==row['state_c'])]\\\n",
    "                    .reset_index()\n",
    "                if len(all_counties_frag)==1:\n",
    "                    issuers.at[idx,'County_3'] = name_counties[2]\n",
    "            if len(name_counties)>=4:\n",
    "                all_counties_frag = all_counties[(all_counties['County']==name_counties[3])&(all_counties['State']==row['state_c'])]\\\n",
    "                    .reset_index()\n",
    "                if len(all_counties_frag)==1:\n",
    "                    issuers.at[idx,'County_4'] = name_counties[3]\n",
    "            if len(name_counties)>=5:\n",
    "                all_counties_frag = all_counties[(all_counties['County']==name_counties[4])&(all_counties['State']==row['state_c'])]\\\n",
    "                    .reset_index()\n",
    "                if len(all_counties_frag)==1:\n",
    "                    issuers.at[idx,'County_5'] = name_counties[4]\n",
    "\n",
    "        ##############################################################################\n",
    "        # Match by county or city or city equivalence or school district or hospital #\n",
    "        ##############################################################################\n",
    "\n",
    "        # Conduct this as long as a match by county has not been identifed. Also note that I handle prefixes (NEWS) in this step\n",
    "\n",
    "        # Match by county again. This addresses cases where key words \"CNTY\" are not in issuer names\n",
    "        if issuers.at[idx,'County']==None:\n",
    "            issuer_long_name = row['issuer_long_name_c_Processed']\n",
    "            issuer_long_name_noloc = issuer_long_name[10:] if issuer_long_name[:9]=='NORTHEAST' else issuer_long_name\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHEAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='NORTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='EAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='WEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='NORTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='SOUTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[8:] if issuer_long_name_noloc[:7]=='CENTRAL' else issuer_long_name_noloc\n",
    "            name_county = issuer_long_name.split(' ')[0]\n",
    "            name_county_noloc = issuer_long_name_noloc.split(' ')[0]\n",
    "            all_counties_frag = all_counties[\n",
    "                ((all_counties['County']==name_county)|(all_counties['County']==name_county_noloc))\n",
    "                &(all_counties['State']==row['state_c'])].reset_index()\n",
    "            if len(all_counties_frag)==1:\n",
    "                issuers.at[idx,'County'] = all_counties_frag['County'][0]\n",
    "\n",
    "        # Match by city\n",
    "        if issuers.at[idx,'County']==None:\n",
    "            issuer_long_name = row['issuer_long_name_c_Processed']\n",
    "            issuer_long_name_noloc = issuer_long_name[10:] if issuer_long_name[:9]=='NORTHEAST' else issuer_long_name\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHEAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='NORTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='EAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='WEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='NORTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='SOUTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[8:] if issuer_long_name_noloc[:7]=='CENTRAL' else issuer_long_name_noloc\n",
    "            name_city = issuer_long_name.split(' ')[0]\n",
    "            name_city_noloc = issuer_long_name_noloc.split(' ')[0]\n",
    "            all_cities_frag = all_cities[\n",
    "                ((all_cities['City']==name_city)|(all_cities['City']==name_city_noloc))\n",
    "                &(all_cities['State']==row['state_c'])].reset_index()\n",
    "            # Note that there could be duplicate entries in \"all_cities\"\n",
    "            if len(all_cities_frag)>=1:\n",
    "                issuers.at[idx,'City'] = name_city\n",
    "                issuers.at[idx,'County'] = all_cities_frag['County'][0]\n",
    "\n",
    "    return issuers\n",
    "\n",
    "meta_columns = list(proc_list(issuers[:10]).columns)\n",
    "issuers_dd = dd.from_pandas(issuers, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    issuers = issuers_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6faf7-d1d8-45a5-b798-a1b338c219d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# Round 2: Match by school district or hospital names or universities or Community Develop Destrict (in Florida) #\n",
    "##################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f197f-8ed9-4fd6-937d-b747f37daf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CSA information\n",
    "\n",
    "def proc_list(issuers):\n",
    "\n",
    "    issuers = issuers.copy()\n",
    "    issuers['CSA Code'] = None\n",
    "    issuers['CSA Title'] = None\n",
    "    issuers['CBSA Code'] = None\n",
    "    issuers['CBSA Title'] = None\n",
    "    \n",
    "    for idx,row in issuers.iterrows():\n",
    "        CBSA_frag = CBSA[(CBSA['County']==row['County'])&(CBSA['State']==row['state_c'])]\\\n",
    "            .reset_index()\n",
    "        if len(CBSA_frag)==1:\n",
    "            issuers.at[idx,'CSA Code'] = CBSA_frag['CSA Code'][0]\n",
    "            issuers.at[idx,'CSA Title'] = CBSA_frag['CSA Title'][0]\n",
    "            issuers.at[idx,'CBSA Code'] = CBSA_frag['CBSA Code'][0]\n",
    "            issuers.at[idx,'CBSA Title'] = CBSA_frag['CBSA Title'][0]\n",
    "\n",
    "    return issuers\n",
    "\n",
    "meta_columns = list(proc_list(issuers[:10]).columns)\n",
    "issuers_dd = dd.from_pandas(issuers, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    issuers = issuers_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725b442-35c2-4f3f-90d5-c1d1d7f1a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in issuer infor into issues dataset\n",
    "issueinfo = issueinfo.merge(issuers[['issuer_id','County','City','CSA Code','CBSA Code']],on=['issuer_id'])\n",
    "\n",
    "bondinfo['maturity_date_d'] = pd.to_datetime(bondinfo['maturity_date_d'],errors='coerce')\n",
    "bondinfo['settlement_date_d'] = pd.to_datetime(bondinfo['settlement_date_d'],errors='coerce')\n",
    "bondinfo['dif_dates'] = bondinfo['maturity_date_d']-bondinfo['settlement_date_d']\n",
    "bondinfo['dif_dates'] = bondinfo['dif_dates'].dt.days\n",
    "bondinfo['issue_id_l'] = pd.to_numeric(bondinfo['issue_id_l'],errors='coerce')\n",
    "bondinfo = bondinfo[~pd.isnull(bondinfo['issue_id_l'])].copy()\n",
    "bondinfo['issue_id_l'] = bondinfo['issue_id_l'].astype(int)\n",
    "bondinfo = bondinfo[[\n",
    "    'issue_id_l','maturity_id_l',\n",
    "    'offering_yield_f','coupon_f','coupon_code_c','offering_price_f',\n",
    "    'total_maturity_offering_amt_f',\n",
    "    'maturity_date_d','dif_dates',\n",
    "    'tax_code_c']]\n",
    "\n",
    "issueinfo_nodup = issueinfo[[\n",
    "    'issuer_id','issue_id_l',\n",
    "    'offering_date_d',\n",
    "    'County','City','CSA Code','CBSA Code',\n",
    "    'offering_type_c','issue_description_c','issue_description_Processed'\n",
    "    ]].drop_duplicates()\n",
    "Mergent = bondinfo.merge(issueinfo_nodup,on='issue_id_l')\n",
    "Mergent['sale_year'] = pd.to_datetime(Mergent['offering_date_d']).dt.year\n",
    "\n",
    "# Export data\n",
    "Mergent.to_parquet('../RawData/Mergent/Mergent_bondlevel.parque')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0429a3a-5f4e-4f04-a2ba-56d65dec4c36",
   "metadata": {},
   "source": [
    "## 1.3 Calculate yield of synthetic risk-free bond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00699e60-07fa-4b20-a824-85b92ac795cc",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Without coupon rate, it is impossible to calculate the price (yield) of the synthetic treasury bond. For example, consider  Bond A: Pays \\\\$5 one year from now, and \\\\$5 two years from now, and Bond B: Pays $100*(1+5%)^2 two years from now. These two have the same yield. Suppose that treasury yield in one year is 0% and in two years is 20%. Then price of first synthetic bond is higher than the second, and the yield of the first synthetic bond is lower than the second. In other words, without coupon rate, I do not know when the cash flow is going to come, so I do not know what is the component of risk-free rate that I should tease out from the return of the municipal bond yield. An INACCURATE approximation can be simply using yield of municipal bond minus  that of treasury bond, but it is erraneous to do so.\n",
    "\n",
    "Luckily except for later parts of the sample, coupon rate is usually available.\n",
    "\n",
    "- I only handle fixed rate bonds, and (for the moment) skip zero-coupon bonds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab436537-698e-45ae-9dbd-d3180c706d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Mergent = Mergent.copy()\n",
    "\n",
    "# Treasury yield\n",
    "feds200628 = pd.read_csv(\"../RawData/FedBOG/feds200628.csv\", header=9)\n",
    "feds200628 = feds200628[~pd.isnull(feds200628['SVENY01'])]\n",
    "columns = ['Date']+ \\\n",
    "    ['SVENY0'+str(i) for i in range(1,10)]+ \\\n",
    "    ['SVENY'+str(i) for i in range(10,31)]\n",
    "feds200628 = feds200628[columns]\n",
    "new_columns = ['Date']+ \\\n",
    "    ['SVENY'+str(i) for i in range(1,10)]+ \\\n",
    "    ['SVENY'+str(i) for i in range(10,31)]\n",
    "feds200628.columns = new_columns\n",
    "feds200628['Date'] = pd.to_datetime(feds200628['Date'])\n",
    "threshold_date = pd.to_datetime('2050-01-01')\n",
    "feds200628['Date'] = feds200628['Date'].apply(lambda x: x - pd.DateOffset(years=100) if x > threshold_date else x)\n",
    "\n",
    "max_year_7 = [pd.Timestamp(1961,6,14,0,0,0),pd.Timestamp(1971,8,15,0,0,0)]\n",
    "max_year_10 = [pd.Timestamp(1971,8,16,0,0,0),pd.Timestamp(1971,11,14,0,0,0)]\n",
    "max_year_15 = [pd.Timestamp(1971,11,15,0,0,0),pd.Timestamp(1981,7,1,0,0,0)]\n",
    "max_year_20 = [pd.Timestamp(1981,7,2,0,0,0),pd.Timestamp(1985,11,24,0,0,0)]\n",
    "max_year_30 = [pd.Timestamp(1985,11,25,0,0,0),pd.Timestamp(2023,11,3,0,0,0)]\n",
    "\n",
    "Mergent['sync_bond_yield'] = None\n",
    "\n",
    "def proc_list(Mergent_part):\n",
    "\n",
    "    Mergent_part = Mergent_part.copy()\n",
    "\n",
    "    for idx,row in Mergent_part.iterrows():\n",
    "    \n",
    "        # Only handle if bond is fixed rate\n",
    "        if not ((row['coupon_code_c']=='OIP') or (row['coupon_code_c']=='FXD') or (row['coupon_code_c']=='OID')):\n",
    "            continue\n",
    "    \n",
    "        # Skip if key info is missing\n",
    "        if str(row['coupon_f'])=='nan' or str(row['dif_dates'])=='nan':\n",
    "            continue\n",
    "        \n",
    "        offering_date = datetime.strptime(row['offering_date_d'],\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Obtain the treasury zero-coupon yield curve at the closest date\n",
    "        feds200628_copy = feds200628.copy()\n",
    "        feds200628_copy['dif_date'] = np.abs(feds200628_copy['Date']-offering_date)\n",
    "        feds200628_copy = feds200628_copy.sort_values('dif_date').reset_index()\n",
    "    \n",
    "        coupon_rate = float(row['coupon_f'])\n",
    "        maturity = row['dif_dates']\n",
    "    \n",
    "        cf = []\n",
    "        discount_factor = []\n",
    "        N_coupons = int(np.max([1,np.around(maturity/(365/2))]))\n",
    "    \n",
    "        # Determine if synthetic bond can be constructed. Cannot do so if the length of zero-coupon yields is not long enough\n",
    "        rf_available = False\n",
    "        if offering_date>max_year_7[0] and offering_date<=max_year_7[1] and maturity<=7*365:\n",
    "            rf_available = True\n",
    "        if offering_date>max_year_10[0] and offering_date<=max_year_10[1] and maturity<=10*365:\n",
    "            rf_available = True\n",
    "        if offering_date>max_year_15[0] and offering_date<=max_year_15[1] and maturity<=15*365:\n",
    "            rf_available = True\n",
    "        if offering_date>max_year_20[0] and offering_date<=max_year_20[1] and maturity<=20*365:\n",
    "            rf_available = True\n",
    "        if offering_date>max_year_30[0] and offering_date<=max_year_30[1] and maturity<=30*365:\n",
    "            rf_available = True\n",
    "    \n",
    "        if rf_available:\n",
    "            # Construct a series of cash flow for each bond\n",
    "            for cf_idx in range(0,N_coupons):\n",
    "                cf = cf+[coupon_rate/2]\n",
    "            cf[N_coupons-1] = cf[N_coupons-1]+100\n",
    "    \n",
    "            # Construct a series of discount factor for each bond\n",
    "            for cf_idx in range(0,N_coupons):\n",
    "                if cf_idx==0:\n",
    "                    discount_factor = discount_factor+[feds200628_copy['SVENY1'][0]]\n",
    "                elif cf_idx%2==1:\n",
    "                    discount_factor = discount_factor+[feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]]\n",
    "                elif cf_idx%2==0:\n",
    "                    discount_factor = discount_factor+\\\n",
    "                        [(feds200628_copy['SVENY'+str(ceil(cf_idx/2))][0]+feds200628_copy['SVENY'+str(ceil(cf_idx/2)+1)][0])/2]\n",
    "            discount_factor = [(1/(1+discount_factor[disc_idx]/100))**((disc_idx+1)/2) for disc_idx in range(0,N_coupons)]\n",
    "    \n",
    "            # Bond price and yield of synthetic bond\n",
    "            sync_bond_price = np.sum(np.dot(cf,discount_factor))\n",
    "            cf = [-sync_bond_price]+cf\n",
    "            sync_bond_yield = (1+npf.irr(cf))**2-1\n",
    "    \n",
    "            # Record data\n",
    "            Mergent_part.at[idx,'sync_bond_yield'] = sync_bond_yield\n",
    "\n",
    "    return Mergent_part\n",
    "\n",
    "meta_columns = list(proc_list(Mergent[:10]).columns)\n",
    "Mergent_dd = dd.from_pandas(Mergent,npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    Mergent = Mergent_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7f95e-1ca3-4bea-acae-be065281c9b5",
   "metadata": {},
   "source": [
    "## 1.4 Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802ca09-f78e-457a-a43f-930db3c014b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mergent.to_parquet('../CleanData/Mergent/0E_Mergent_bondlevel.parque')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
