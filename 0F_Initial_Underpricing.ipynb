{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04e8b42-07b5-4cdb-aac0-641e31360d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "import pyreadstat\n",
    "import bisect\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620c346-288e-437b-a55f-9c135bc6cee0",
   "metadata": {},
   "source": [
    "# 1. Calculate Bond-Level Underpricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b43c8c-6eff-4b83-ae54-5dfacce42672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2005\n",
      "Processed year 2006\n",
      "Processed year 2007\n",
      "Processed year 2008\n",
      "Processed year 2009\n",
      "Processed year 2010\n",
      "Processed year 2011\n",
      "Processed year 2012\n",
      "Processed year 2013\n",
      "Processed year 2014\n",
      "Processed year 2015\n",
      "Processed year 2016\n",
      "Processed year 2017\n",
      "Processed year 2018\n",
      "Processed year 2019\n",
      "Processed year 2020\n",
      "Processed year 2021\n",
      "Processed year 2022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed year 2022\n"
     ]
    }
   ],
   "source": [
    "initial_underpricing_allyears = pd.DataFrame()\n",
    "\n",
    "for year in range(2005,2023):\n",
    "\n",
    "    # Process every two adjacent years' data, to make sure 30 days post offering are within the sample\n",
    "\n",
    "    # For unknown reasons, cannot download data as \"dta\" with correct dates in Feb 2024\n",
    "    if year not in [2016,2022,2023]:\n",
    "        part1,meta1 = pyreadstat.read_dta('../RawData/MSRB/MSRB_'+str(year)+'.dta')\n",
    "    else:\n",
    "        part1 = pd.read_csv('../RawData/MSRB/MSRB_'+str(year)+'.csv',low_memory=False)\n",
    "        part1['trade_date'] = pd.to_datetime(part1['trade_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['dated_date'] = pd.to_datetime(part1['dated_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['maturity_date'] = pd.to_datetime(part1['maturity_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['settlement_date'] = pd.to_datetime(part1['settlement_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['rtrs_publish_date'] = pd.to_datetime(part1['rtrs_publish_date'], format='%d/%m/%Y',errors='coerce')\n",
    "    if year+1 not in [2016,2022,2023]:\n",
    "        part2,meta2 = pyreadstat.read_dta('../RawData/MSRB/MSRB_'+str(year+1)+'.dta')\n",
    "    else:\n",
    "        part2 = pd.read_csv('../RawData/MSRB/MSRB_'+str(year+1)+'.csv',low_memory=False)\n",
    "        part2['trade_date'] = pd.to_datetime(part2['trade_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['dated_date'] = pd.to_datetime(part2['dated_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['maturity_date'] = pd.to_datetime(part2['maturity_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['settlement_date'] = pd.to_datetime(part2['settlement_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['rtrs_publish_date'] = pd.to_datetime(part2['rtrs_publish_date'], format='%d/%m/%Y',errors='coerce')\n",
    "    TradeData = pd.concat([part1,part2])\n",
    "\n",
    "    print('Processed year '+str(year))\n",
    "\n",
    "    # Identify new offerings in the first year\n",
    "    \n",
    "    start_datetime = pd.to_datetime(str(year)+'-01-01 00:00:00')\n",
    "    end_datetime = pd.to_datetime(str(year)+'-12-31 23:59:59')\n",
    "    TradeData = TradeData[~pd.isnull(TradeData['dated_date'])]\n",
    "    TradeData = TradeData[TradeData['dated_date']!=None]\n",
    "    TradeData = TradeData[(TradeData['dated_date']>=start_datetime)&(TradeData['dated_date']<=end_datetime)]\n",
    "    if TradeData['par_traded'].astype(str).str.contains('[a-zA-Z]').sum()>0:\n",
    "        TradeData['par_traded'] = TradeData['par_traded'].str.replace('1MM+','1000000',regex=False)\n",
    "    TradeData['par_traded'] = TradeData['par_traded'].astype('float')\n",
    "    \n",
    "    # Calculate initial underpricing for each new offering\n",
    "    \n",
    "    def proc_list(TradeData):\n",
    "    \n",
    "        TradeData = TradeData.reset_index()\n",
    "        TradeData_gb = TradeData.groupby('cusip')\n",
    "    \n",
    "        new_offerings = list(TradeData['cusip'].unique())\n",
    "        new_offerings = pd.DataFrame(new_offerings,columns=['cusip'])\n",
    "    \n",
    "        initial_underpricing = []\n",
    "        \n",
    "        for idx,row in new_offerings.iterrows():\n",
    "        \n",
    "            TradeData_onecusip = TradeData_gb.get_group(row['cusip'])\n",
    "            \n",
    "            # If \"offer_price_takedown_indicator\" is available, use the first \"trade_date\" within those with \"Y\" as initial day\n",
    "            TradeData_onecusip_initial = TradeData_onecusip[TradeData_onecusip['offer_price_takedown_indicator']=='Y']\n",
    "            if len(TradeData_onecusip_initial)>0:\n",
    "                TradeData_onecusip_initial = TradeData_onecusip_initial.sort_values('trade_date',ascending=True)\n",
    "                TradeData_onecusip_initial = TradeData_onecusip_initial.reset_index(drop=True)\n",
    "                offering_date = TradeData_onecusip_initial['trade_date'][0]\n",
    "            # Otherwise, use the first \"trade_date\"\n",
    "            else:\n",
    "                TradeData_onecusip = TradeData_onecusip.sort_values('trade_date',ascending=True)\n",
    "                TradeData_onecusip = TradeData_onecusip.reset_index(drop=True)\n",
    "                offering_date = TradeData_onecusip['trade_date'][0]\n",
    "            \n",
    "            # Obtain initial trading day price\n",
    "            TradeData_onecusip_initial = TradeData_onecusip[TradeData_onecusip['trade_date']==offering_date]\n",
    "            offering_date_price = np.dot(TradeData_onecusip_initial['par_traded'],TradeData_onecusip_initial['dollar_price'])/\\\n",
    "                np.sum(TradeData_onecusip_initial['par_traded'])\n",
    "            \n",
    "            # Obtain average trading price in the [15,60] window\n",
    "            start_date = offering_date + pd.Timedelta(days=15)\n",
    "            end_date = offering_date + pd.Timedelta(days=60)\n",
    "            TradeData_onecusip_15to60 = TradeData_onecusip[(TradeData_onecusip['trade_date']>=start_date)&\n",
    "                (TradeData_onecusip['trade_date']<=end_date)]\n",
    "            if len(TradeData_onecusip_15to60)>0:\n",
    "                date_15to60_price = np.dot(TradeData_onecusip_15to60['par_traded'],TradeData_onecusip_15to60['dollar_price'])/\\\n",
    "                    np.sum(TradeData_onecusip_15to60['par_traded'])\n",
    "            else:\n",
    "                date_15to60_price = None\n",
    "\n",
    "            # Obtain average trading price in the [15,30] window\n",
    "            start_date = offering_date + pd.Timedelta(days=15)\n",
    "            end_date = offering_date + pd.Timedelta(days=30)\n",
    "            TradeData_onecusip_15to30 = TradeData_onecusip[(TradeData_onecusip['trade_date']>=start_date)&\n",
    "                (TradeData_onecusip['trade_date']<=end_date)]\n",
    "            if len(TradeData_onecusip_15to30)>0:\n",
    "                date_15to30_price = np.dot(TradeData_onecusip_15to30['par_traded'],TradeData_onecusip_15to30['dollar_price'])/\\\n",
    "                    np.sum(TradeData_onecusip_15to30['par_traded'])\n",
    "            else:\n",
    "                date_15to30_price = None\n",
    "\n",
    "            initial_underpricing = initial_underpricing+[{'cusip':row['cusip'],'offering_date':offering_date,\n",
    "                'offering_date_price':offering_date_price,'date_15to60_price':date_15to60_price,'date_15to30_price':date_15to30_price}]\n",
    "        \n",
    "        initial_underpricing = pd.DataFrame(initial_underpricing)\n",
    "        return initial_underpricing\n",
    "    \n",
    "    TradeData = TradeData.set_index('cusip')\n",
    "    TradeData_dd = dd.from_pandas(TradeData, npartitions=20)\n",
    "    with dask.config.set(scheduler='processes',num_workers=20):\n",
    "        initial_underpricing = TradeData_dd.map_partitions(proc_list, \\\n",
    "            meta=pd.DataFrame(columns=['cusip','offering_date','offering_date_price','date_15to60_price','date_15to30_price'])).compute()\n",
    "    TradeData = TradeData.reset_index()\n",
    "\n",
    "    initial_underpricing_allyears = pd.concat([initial_underpricing_allyears,initial_underpricing])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302de0fb-5a47-4319-ba76-c58bcb986adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_underpricing_allyears.to_parquet('../CleanData/MSRB/0F_initial_underpricing_allyears.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598c22c-2d52-485a-8de2-0a241ff443c1",
   "metadata": {},
   "source": [
    "# 2. Add Underpricing to GPF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43215f3-0c5c-4060-a0f0-5abd02c007ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_underpricing_allyears = pd.read_parquet('../CleanData/MSRB/0F_initial_underpricing_allyears.parquet')\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e2c17c-f5d8-4c2a-a30a-6b4b4b741606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary\n",
    "initial_underpricing_allyears['underpricing_15to60'] = \\\n",
    "    initial_underpricing_allyears['date_15to60_price']-initial_underpricing_allyears['offering_date_price']\n",
    "initial_underpricing_allyears['underpricing_15to30'] = \\\n",
    "    initial_underpricing_allyears['date_15to30_price']-initial_underpricing_allyears['offering_date_price']\n",
    "initial_underpricing_allyears['underpricing_15to60'] = \\\n",
    "    winsor2(initial_underpricing_allyears['underpricing_15to60'],cutoffs=[0.01,0.01])\n",
    "initial_underpricing_allyears['underpricing_15to30'] = \\\n",
    "    winsor2(initial_underpricing_allyears['underpricing_15to30'],cutoffs=[0.01,0.01])\n",
    "\n",
    "dict_underpricing_15to60 = initial_underpricing_allyears.set_index('cusip').to_dict(orient='dict')['underpricing_15to60']\n",
    "dict_underpricing_15to30 = initial_underpricing_allyears.set_index('cusip').to_dict(orient='dict')['underpricing_15to30']\n",
    "\n",
    "cusips_in_MSRB = list(dict_underpricing_15to60.keys())\n",
    "cusips_in_MSRB.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d5d36c-8587-4c2c-9e45-fcd3ec47a71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 s, sys: 5.81 s, total: 30.7 s\n",
      "Wall time: 3h 18min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def proc_list(GPF):\n",
    "    \n",
    "    GPF = GPF.copy()\n",
    "    GPF['underpricing_15to60'] = None\n",
    "    GPF['underpricing_15to30'] = None\n",
    "    \n",
    "    for idx,row in GPF.iterrows():\n",
    "    \n",
    "        cusip = str(row['cusip'])\n",
    "        underpricing_15to60 = None\n",
    "        underpricing_15to30 = None\n",
    "    \n",
    "        if cusip=='nan' or cusip=='None':\n",
    "            continue\n",
    "        \n",
    "        if '\\n' not in cusip:\n",
    "        \n",
    "            if cusip in cusips_in_MSRB:\n",
    "                underpricing_15to60 = dict_underpricing_15to60[cusip]\n",
    "                underpricing_15to30 = dict_underpricing_15to30[cusip]\n",
    "                if str(underpricing_15to60)=='nan':\n",
    "                    underpricing_15to60 = None\n",
    "                if str(underpricing_15to30)=='nan':\n",
    "                    underpricing_15to30 = None\n",
    "    \n",
    "        else:\n",
    "\n",
    "            cusip = cusip.split('\\n')\n",
    "\n",
    "            underpricing_15to60_list = []\n",
    "            underpricing_15to30_list = []\n",
    "            for cusip_onebond in cusip:\n",
    "                index = bisect.bisect_left(cusips_in_MSRB,cusip_onebond)\n",
    "                is_in_list = index<len(cusips_in_MSRB) and cusips_in_MSRB[index]==cusip_onebond\n",
    "                if is_in_list:\n",
    "                    underpricing_15to60_list = underpricing_15to60_list+[dict_underpricing_15to60[cusip_onebond]]\n",
    "                    underpricing_15to30_list = underpricing_15to30_list+[dict_underpricing_15to30[cusip_onebond]]\n",
    "            underpricing_15to60_list = [item for item in underpricing_15to60_list if str(item)!='nan']\n",
    "            underpricing_15to30_list = [item for item in underpricing_15to30_list if str(item)!='nan']    \n",
    "\n",
    "            # Take simple average. Most of the time number of items is not aligned in CUSIP and in amount by maturity\n",
    "            if len(underpricing_15to60_list)>0:\n",
    "                underpricing_15to60 = np.mean(underpricing_15to60_list)\n",
    "            else:\n",
    "                underpricing_15to60 = None\n",
    "            if len(underpricing_15to30_list)>0:\n",
    "                underpricing_15to30 = np.mean(underpricing_15to30_list)\n",
    "            else:\n",
    "                underpricing_15to30 = None\n",
    "    \n",
    "        GPF.at[idx,'underpricing_15to60'] = underpricing_15to60\n",
    "        GPF.at[idx,'underpricing_15to30'] = underpricing_15to30\n",
    "\n",
    "    return GPF\n",
    "\n",
    "output_columns = proc_list(GPF[:10]).columns # Process one year to get columns\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list, \\\n",
    "        meta=pd.DataFrame(columns=output_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1d8e4-833f-46b9-90aa-ca11cca93505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb892cf-2c6e-4806-a4bf-892787ce03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPF.to_csv(\"../CleanData/SDC/0A_GPF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ce682-8d40-4819-b2f2-a098675a73c9",
   "metadata": {},
   "source": [
    "# 3. Construct trading yield at the bond (CUSIP) X calendar month level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bd2b6-d4df-4445-b709-347684be58db",
   "metadata": {},
   "source": [
    "Put CSA and \"sale_year\" (year of underwriting) into the sample, which will be used for constructing an event study sample. In the event study sample, treated and control is defined with the year of underwriting. Post indicates whether the year of underwriting is post consolidation, rather than based on the month of trading. Market-level factor is controlled with time fixed effects of the month of trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68009fd-9881-4052-afc8-f7e16a6a61be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD!!\n",
    "\n",
    "initial_underpricing_allyears = pd.DataFrame()\n",
    "\n",
    "for year in range(2005,2023):\n",
    "\n",
    "    # Process every two adjacent years' data, to make sure 30 days post offering are within the sample\n",
    "\n",
    "    # For unknown reasons, cannot download data as \"dta\" with correct dates in Feb 2024\n",
    "    if year not in [2016,2022,2023]:\n",
    "        part1,meta1 = pyreadstat.read_dta('../RawData/MSRB/MSRB_'+str(year)+'.dta')\n",
    "    else:\n",
    "        part1 = pd.read_csv('../RawData/MSRB/MSRB_'+str(year)+'.csv',low_memory=False)\n",
    "        part1['trade_date'] = pd.to_datetime(part1['trade_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['dated_date'] = pd.to_datetime(part1['dated_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['maturity_date'] = pd.to_datetime(part1['maturity_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['settlement_date'] = pd.to_datetime(part1['settlement_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part1['rtrs_publish_date'] = pd.to_datetime(part1['rtrs_publish_date'], format='%d/%m/%Y',errors='coerce')\n",
    "    if year+1 not in [2016,2022,2023]:\n",
    "        part2,meta2 = pyreadstat.read_dta('../RawData/MSRB/MSRB_'+str(year+1)+'.dta')\n",
    "    else:\n",
    "        part2 = pd.read_csv('../RawData/MSRB/MSRB_'+str(year+1)+'.csv',low_memory=False)\n",
    "        part2['trade_date'] = pd.to_datetime(part2['trade_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['dated_date'] = pd.to_datetime(part2['dated_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['maturity_date'] = pd.to_datetime(part2['maturity_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['settlement_date'] = pd.to_datetime(part2['settlement_date'], format='%d/%m/%Y',errors='coerce')\n",
    "        part2['rtrs_publish_date'] = pd.to_datetime(part2['rtrs_publish_date'], format='%d/%m/%Y',errors='coerce')\n",
    "    TradeData = pd.concat([part1,part2])\n",
    "\n",
    "    print('Processed year '+str(year))\n",
    "\n",
    "    # Identify new offerings in the first year\n",
    "    \n",
    "    start_datetime = pd.to_datetime(str(year)+'-01-01 00:00:00')\n",
    "    end_datetime = pd.to_datetime(str(year)+'-12-31 23:59:59')\n",
    "    TradeData = TradeData[~pd.isnull(TradeData['dated_date'])]\n",
    "    TradeData = TradeData[TradeData['dated_date']!=None]\n",
    "    if TradeData['par_traded'].astype(str).str.contains('[a-zA-Z]').sum()>0:\n",
    "        TradeData['par_traded'] = TradeData['par_traded'].str.replace('1MM+','1000000',regex=False)\n",
    "    TradeData['par_traded'] = TradeData['par_traded'].astype('float')\n",
    "    \n",
    "    # Calculate initial underpricing for each new offering\n",
    "    \n",
    "    def proc_list(TradeData):\n",
    "    \n",
    "        TradeData = TradeData.reset_index()\n",
    "        TradeData_gb = TradeData.groupby('cusip')\n",
    "    \n",
    "        new_offerings = list(TradeData['cusip'].unique())\n",
    "        new_offerings = pd.DataFrame(new_offerings,columns=['cusip'])\n",
    "    \n",
    "        initial_underpricing = []\n",
    "        \n",
    "        for idx,row in new_offerings.iterrows():\n",
    "        \n",
    "            TradeData_onecusip = TradeData_gb.get_group(row['cusip'])\n",
    "            \n",
    "            # If \"offer_price_takedown_indicator\" is available, use the first \"trade_date\" within those with \"Y\" as initial day\n",
    "            TradeData_onecusip_initial = TradeData_onecusip[TradeData_onecusip['offer_price_takedown_indicator']=='Y']\n",
    "            if len(TradeData_onecusip_initial)>0:\n",
    "                TradeData_onecusip_initial = TradeData_onecusip_initial.sort_values('trade_date',ascending=True)\n",
    "                TradeData_onecusip_initial = TradeData_onecusip_initial.reset_index(drop=True)\n",
    "                offering_date = TradeData_onecusip_initial['trade_date'][0]\n",
    "            # Otherwise, use the first \"trade_date\"\n",
    "            else:\n",
    "                TradeData_onecusip = TradeData_onecusip.sort_values('trade_date',ascending=True)\n",
    "                TradeData_onecusip = TradeData_onecusip.reset_index(drop=True)\n",
    "                offering_date = TradeData_onecusip['trade_date'][0]\n",
    "            \n",
    "            # Obtain initial trading day price\n",
    "            TradeData_onecusip_initial = TradeData_onecusip[TradeData_onecusip['trade_date']==offering_date]\n",
    "            offering_date_price = np.dot(TradeData_onecusip_initial['par_traded'],TradeData_onecusip_initial['dollar_price'])/\\\n",
    "                np.sum(TradeData_onecusip_initial['par_traded'])\n",
    "            \n",
    "            # Obtain average trading price in the [15,60] window\n",
    "            start_date = offering_date + pd.Timedelta(days=15)\n",
    "            end_date = offering_date + pd.Timedelta(days=60)\n",
    "            TradeData_onecusip_15to60 = TradeData_onecusip[(TradeData_onecusip['trade_date']>=start_date)&\n",
    "                (TradeData_onecusip['trade_date']<=end_date)]\n",
    "            if len(TradeData_onecusip_15to60)>0:\n",
    "                date_15to60_price = np.dot(TradeData_onecusip_15to60['par_traded'],TradeData_onecusip_15to60['dollar_price'])/\\\n",
    "                    np.sum(TradeData_onecusip_15to60['par_traded'])\n",
    "            else:\n",
    "                date_15to60_price = None\n",
    "\n",
    "            # Obtain average trading price in the [15,30] window\n",
    "            start_date = offering_date + pd.Timedelta(days=15)\n",
    "            end_date = offering_date + pd.Timedelta(days=30)\n",
    "            TradeData_onecusip_15to30 = TradeData_onecusip[(TradeData_onecusip['trade_date']>=start_date)&\n",
    "                (TradeData_onecusip['trade_date']<=end_date)]\n",
    "            if len(TradeData_onecusip_15to30)>0:\n",
    "                date_15to30_price = np.dot(TradeData_onecusip_15to30['par_traded'],TradeData_onecusip_15to30['dollar_price'])/\\\n",
    "                    np.sum(TradeData_onecusip_15to30['par_traded'])\n",
    "            else:\n",
    "                date_15to30_price = None\n",
    "\n",
    "            initial_underpricing = initial_underpricing+[{'cusip':row['cusip'],'offering_date':offering_date,\n",
    "                'offering_date_price':offering_date_price,'date_15to60_price':date_15to60_price,'date_15to30_price':date_15to30_price}]\n",
    "        \n",
    "        initial_underpricing = pd.DataFrame(initial_underpricing)\n",
    "        return initial_underpricing\n",
    "    \n",
    "    TradeData = TradeData.set_index('cusip')\n",
    "    TradeData_dd = dd.from_pandas(TradeData, npartitions=20)\n",
    "    with dask.config.set(scheduler='processes',num_workers=20):\n",
    "        initial_underpricing = TradeData_dd.map_partitions(proc_list, \\\n",
    "            meta=pd.DataFrame(columns=['cusip','offering_date','offering_date_price','date_15to60_price','date_15to30_price'])).compute()\n",
    "    TradeData = TradeData.reset_index()\n",
    "\n",
    "    initial_underpricing_allyears = pd.concat([initial_underpricing_allyears,initial_underpricing])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f61560-b1cd-41a9-9bb7-abd57681fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
