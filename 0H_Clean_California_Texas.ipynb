{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271ed1c7-ecc9-4bfb-bf4b-99681f23fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f9556-b1ca-4b79-a2f5-b2885d1267dd",
   "metadata": {},
   "source": [
    "Estimate a statistical model of issuance fees (advisor fee, insurance premium, ratings fee) as a ratio of amount:\n",
    "$$\\text{Cost}=\n",
    "\\gamma_1\\frac{\\text{County Income}}{\\text{National Average Income}}+\\gamma_2\\frac{\\text{County Population}}{\\text{National Average County Population}}+\n",
    "\\delta_{\\text{maturity bracket}}+\\delta_{\\text{amount bracket}}+\n",
    "$$\n",
    "$$\n",
    "\\theta_{\\text{method of sales}}+\\theta_{\\text{tax status}}+\\theta_{\\text{source of repayment}}+\n",
    "\\beta_{\\text{if prior relationship}}+\\beta_{\\text{if refunding issue}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911389f-5710-4318-bf75-cac8861266e3",
   "metadata": {},
   "source": [
    "# 1. California"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60f2e4-f5fa-4e47-a23e-c05b75fe76f8",
   "metadata": {},
   "source": [
    "## 1.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1a4fbc-07e7-40a7-a730-2dc6c775b294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------#\n",
    "# Import california fee data #\n",
    "#----------------------------#\n",
    "\n",
    "CaliforniaData = pd.read_csv('../RawData/California/CDA_All_Data.csv')\n",
    "\n",
    "CaliforniaData['County'] = CaliforniaData['Issuer County'].str.upper()\n",
    "CaliforniaData['State'] = 'CA'\n",
    "\n",
    "#------------#\n",
    "# Fee ratios #\n",
    "#------------#\n",
    "\n",
    "CaliforniaData['AdvisorFeeRatio'] = CaliforniaData['Financial Advisor Fee']/CaliforniaData['Principal Amount']\n",
    "CaliforniaData['CRFeeRatio'] = CaliforniaData['Rating Agency Fee']/CaliforniaData['Principal Amount']\n",
    "CaliforniaData['InsureFeeRatio'] = CaliforniaData['Credit Enhancement Fee']/CaliforniaData['Principal Amount']\n",
    "\n",
    "# Winsorize data\n",
    "upper_limit = np.percentile(CaliforniaData['AdvisorFeeRatio'][np.logical_not(np.isnan(CaliforniaData['AdvisorFeeRatio']))],99)\n",
    "lower_limit = np.percentile(CaliforniaData['AdvisorFeeRatio'][np.logical_not(np.isnan(CaliforniaData['AdvisorFeeRatio']))],1)\n",
    "CaliforniaData.loc[(CaliforniaData['AdvisorFeeRatio']>upper_limit)&(np.logical_not(np.isnan(CaliforniaData['AdvisorFeeRatio']))),\n",
    "    'AdvisorFeeRatio'] = upper_limit\n",
    "CaliforniaData.loc[(CaliforniaData['AdvisorFeeRatio']<lower_limit)&(np.logical_not(np.isnan(CaliforniaData['AdvisorFeeRatio']))),\n",
    "    'AdvisorFeeRatio'] = lower_limit\n",
    "upper_limit = np.percentile(CaliforniaData['CRFeeRatio'][np.logical_not(np.isnan(CaliforniaData['CRFeeRatio']))],99)\n",
    "lower_limit = np.percentile(CaliforniaData['CRFeeRatio'][np.logical_not(np.isnan(CaliforniaData['CRFeeRatio']))],1)\n",
    "CaliforniaData.loc[(CaliforniaData['CRFeeRatio']>upper_limit)&(np.logical_not(np.isnan(CaliforniaData['CRFeeRatio']))),\n",
    "    'CRFeeRatio'] = upper_limit\n",
    "CaliforniaData.loc[(CaliforniaData['CRFeeRatio']<lower_limit)&(np.logical_not(np.isnan(CaliforniaData['CRFeeRatio']))),\n",
    "    'CRFeeRatio'] = lower_limit\n",
    "upper_limit = np.percentile(CaliforniaData['InsureFeeRatio'][np.logical_not(np.isnan(CaliforniaData['InsureFeeRatio']))],99)\n",
    "lower_limit = np.percentile(CaliforniaData['InsureFeeRatio'][np.logical_not(np.isnan(CaliforniaData['InsureFeeRatio']))],1)\n",
    "CaliforniaData.loc[(CaliforniaData['InsureFeeRatio']>upper_limit)&(np.logical_not(np.isnan(CaliforniaData['InsureFeeRatio']))),\n",
    "    'InsureFeeRatio'] = upper_limit\n",
    "CaliforniaData.loc[(CaliforniaData['InsureFeeRatio']<lower_limit)&(np.logical_not(np.isnan(CaliforniaData['InsureFeeRatio']))),\n",
    "    'InsureFeeRatio'] = lower_limit\n",
    "\n",
    "CaliforniaData['Sale Date'] = pd.to_datetime(CaliforniaData['Sale Date'])\n",
    "CaliforniaData['year'] = CaliforniaData['Sale Date'].dt.year\n",
    "CaliforniaData['Final Maturity Date'] = pd.to_datetime(CaliforniaData['Final Maturity Date'])\n",
    "CaliforniaData['year_maturity'] = CaliforniaData['Final Maturity Date'].dt.year\n",
    "CaliforniaData['maturity_in_years'] = CaliforniaData['year_maturity']-CaliforniaData['year']\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "# Whether the issuer has worked with the CRA/financial advisor/insurer in prior years (to capture frequent issuer discount) #\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "CaliforniaData_gb = CaliforniaData.groupby('Issuer')\n",
    "\n",
    "CaliforniaData['GPF_if_prior_insurer'] = None\n",
    "CaliforniaData['GPF_if_prior_advisor'] = None\n",
    "CaliforniaData['GPF_if_prior_rater'] = None\n",
    "\n",
    "for idx,row in CaliforniaData.iterrows():\n",
    "    \n",
    "    CaliforniaData_oneissuer = CaliforniaData_gb.get_group(row['Issuer'])\n",
    "    CaliforniaData_oneissuer = CaliforniaData_oneissuer[CaliforniaData_oneissuer['year']<row['year']]\n",
    "\n",
    "    prior_insurers = list(CaliforniaData_oneissuer['Guarantor'].unique())\n",
    "    prior_insurers = [item for item in prior_insurers if item!=None]\n",
    "    prior_insurers = [item for item in prior_insurers if str(item)!='nan']\n",
    "    prior_insurers = [item for item in prior_insurers if str(item)!='None']\n",
    "    if_prior_insurer = row['Guarantor'] in prior_insurers\n",
    "    CaliforniaData.at[idx,'GPF_if_prior_insurer'] = if_prior_insurer\n",
    "\n",
    "    prior_advisors = list(CaliforniaData_oneissuer['Financial Advisor'].unique())\n",
    "    prior_advisors = [item for item in prior_advisors if item!=None]\n",
    "    prior_advisors = [item for item in prior_advisors if str(item)!='nan']\n",
    "    prior_advisors = [item for item in prior_advisors if str(item)!='None']\n",
    "    if_prior_advisor = row['Financial Advisor'] in prior_advisors\n",
    "    CaliforniaData.at[idx,'GPF_if_prior_advisor'] = if_prior_advisor\n",
    "\n",
    "    if_prior_rater = False\n",
    "    prior_ratings = list(CaliforniaData_oneissuer['S and P Rating'].unique())\n",
    "    prior_ratings = [item for item in prior_ratings if item!=None]\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='nan']\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='None']\n",
    "    prior_ratings = [item for item in prior_ratings if item!='NOT RATED']\n",
    "    if len(prior_ratings)>0 and row['S and P Rating']!='NOT RATED' and \\\n",
    "        row['S and P Rating']!=None and str(row['S and P Rating'])!='nan':\n",
    "        if_prior_rater = True\n",
    "    prior_ratings = list(CaliforniaData_oneissuer['Moody Rating'].unique())\n",
    "    prior_ratings = [item for item in prior_ratings if item!=None]\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='nan']\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='None']\n",
    "    prior_ratings = [item for item in prior_ratings if item!='NOT RATED']\n",
    "    if len(prior_ratings)>0 and row['Moody Rating']!='NOT RATED' and \\\n",
    "        row['Moody Rating']!=None and str(row['Moody Rating'])!='nan':\n",
    "        if_prior_rater = True\n",
    "    prior_ratings = list(CaliforniaData_oneissuer['Fitch Rating'].unique())\n",
    "    prior_ratings = [item for item in prior_ratings if item!=None]\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='nan']\n",
    "    prior_ratings = [item for item in prior_ratings if str(item)!='None']\n",
    "    prior_ratings = [item for item in prior_ratings if item!='NOT RATED']\n",
    "    if len(prior_ratings)>0 and row['Fitch Rating']!='NOT RATED' and \\\n",
    "        row['Fitch Rating']!=None and str(row['Fitch Rating'])!='nan':\n",
    "        if_prior_rater = True\n",
    "    CaliforniaData.at[idx,'GPF_if_prior_rater'] = if_prior_rater\n",
    "\n",
    "\n",
    "#----------------------------------------------------#\n",
    "# Recreate other variables to be consistent with GPF #\n",
    "#----------------------------------------------------#\n",
    "\n",
    "CaliforniaData['GPF_security_type'] = None\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Tax and Revenue Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='General Obligation Bond','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Special Assessment Bond','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Certificate of Participation/Leases','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Limited Tax Obligation Bond (Special Tax Bonds)','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Conduit Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Public Enterprise Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Tax Allocation Bond','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Public Lease Revenue Bond','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Marks-Roos Loan','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Commercial Paper','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Conduit Revenue Note or Loan (Private Obligor)','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Bond Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Capital Lease','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Pension Obligation Bonds','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Other Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Sales Tax Revenue Bond','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Tax Allocation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Revenue Anticipation Note','GPF_security_type'] = 'RV'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Other Bond','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='State Agency Loan','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Loan from bank/other institution','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Tax Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Grant Anticipation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Other Debt','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Promissory Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='General Obligation Note','GPF_security_type'] = 'GO'\n",
    "CaliforniaData.loc[CaliforniaData['Debt Type']=='Revenue Anticipation Warrant','GPF_security_type'] = 'RV'\n",
    "\n",
    "CaliforniaData['GPF_taxable_code'] = None\n",
    "CaliforniaData.loc[CaliforniaData['Federally Taxable']=='Federal Tax Flag: E','GPF_taxable_code'] = 'E'\n",
    "CaliforniaData.loc[CaliforniaData['Federally Taxable']=='Federal Tax Flag: T','GPF_taxable_code'] = 'T'\n",
    "CaliforniaData.loc[CaliforniaData['Federally Taxable']=='Subject to Alternative Minimum Tax','GPF_taxable_code'] = 'A'\n",
    "CaliforniaData.loc[CaliforniaData['Federally Taxable']=='Federal Tax Flag: ET','GPF_taxable_code'] = 'E'\n",
    "CaliforniaData.loc[CaliforniaData['Federally Taxable']=='Federal Tax Flag: TE','GPF_taxable_code'] = 'T'\n",
    "\n",
    "CaliforniaData['GPF_Bid'] = None\n",
    "CaliforniaData.loc[CaliforniaData['Sale Type (Comp/Neg)']=='Neg','GPF_Bid'] = 'N'\n",
    "CaliforniaData.loc[CaliforniaData['Sale Type (Comp/Neg)']=='Comp','GPF_Bid'] = 'C'\n",
    "\n",
    "# Adjust inflation\n",
    "FPCPITOTLZGUSA = pd.read_csv(\"../RawData/StLouisFed/FPCPITOTLZGUSA.csv\")\n",
    "FPCPITOTLZGUSA['year'] = FPCPITOTLZGUSA['DATE'].str[:4].astype(int)\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA.sort_values('year',ascending=False).reset_index(drop=True)\n",
    "scaler = 1\n",
    "FPCPITOTLZGUSA['scaler'] = None\n",
    "for idx,row in FPCPITOTLZGUSA.iterrows():\n",
    "    if idx==0:\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = 1\n",
    "    else:\n",
    "        scaler = scaler*(FPCPITOTLZGUSA.at[idx-1,'FPCPITOTLZGUSA']/100+1)\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = scaler\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA[['scaler','year']]\n",
    "\n",
    "CaliforniaData = CaliforniaData.merge(FPCPITOTLZGUSA,on=['year'])\n",
    "CaliforniaData['GPF_amount_inf_adjusted'] = CaliforniaData['Principal Amount']*CaliforniaData['scaler']\n",
    "\n",
    "CaliforniaData['GPF_amount_bracket'] = None\n",
    "CaliforniaData.loc[CaliforniaData['GPF_amount_inf_adjusted']<=1*1000000,\n",
    "    'GPF_amount_bracket'] = 'Less than 1M'\n",
    "CaliforniaData.loc[(CaliforniaData['GPF_amount_inf_adjusted']>1*1000000)&(CaliforniaData['GPF_amount_inf_adjusted']<=5*1000000),\n",
    "    'GPF_amount_bracket'] = '1M to 5M'\n",
    "CaliforniaData.loc[(CaliforniaData['GPF_amount_inf_adjusted']>5*1000000)&(CaliforniaData['GPF_amount_inf_adjusted']<=10*1000000),\n",
    "    'GPF_amount_bracket'] = '5M to 10M'\n",
    "CaliforniaData.loc[(CaliforniaData['GPF_amount_inf_adjusted']>10*1000000)&(CaliforniaData['GPF_amount_inf_adjusted']<=50*1000000),\n",
    "    'GPF_amount_bracket'] = '10M to 50M'\n",
    "CaliforniaData.loc[(CaliforniaData['GPF_amount_inf_adjusted']>50*1000000)&(CaliforniaData['GPF_amount_inf_adjusted']<=100*1000000),\n",
    "    'GPF_amount_bracket'] = '50M to 100M'\n",
    "CaliforniaData.loc[CaliforniaData['GPF_amount_inf_adjusted']>100*1000000,\n",
    "    'GPF_amount_bracket'] = 'Greater than 100M'\n",
    "\n",
    "CaliforniaData['GPF_maturity_bracket'] = None\n",
    "CaliforniaData.loc[CaliforniaData['maturity_in_years']<=2,\n",
    "    'GPF_maturity_bracket'] = 'Less then 2y'\n",
    "CaliforniaData.loc[(CaliforniaData['maturity_in_years']>2)&(CaliforniaData['maturity_in_years']<=5),\n",
    "    'GPF_maturity_bracket'] = '2y to 5y'\n",
    "CaliforniaData.loc[(CaliforniaData['maturity_in_years']>5)&(CaliforniaData['maturity_in_years']<=10),\n",
    "    'GPF_maturity_bracket'] = '5y to 10y'\n",
    "CaliforniaData.loc[(CaliforniaData['maturity_in_years']>10)&(CaliforniaData['maturity_in_years']<=20),\n",
    "    'GPF_maturity_bracket'] = '10y to 20y'\n",
    "CaliforniaData.loc[(CaliforniaData['maturity_in_years']>20)&(CaliforniaData['maturity_in_years']<=30),\n",
    "    'GPF_maturity_bracket'] = '20y to 30y'\n",
    "CaliforniaData.loc[(CaliforniaData['maturity_in_years']>30)&(CaliforniaData['maturity_in_years']<=40),\n",
    "    'GPF_maturity_bracket'] = '30y to 40y'\n",
    "CaliforniaData.loc[CaliforniaData['maturity_in_years']>40,\n",
    "    'GPF_maturity_bracket'] = 'Greater than 40y'\n",
    "\n",
    "CaliforniaData['GPF_if_refunding'] = CaliforniaData['Refunding Amount']>0\n",
    "\n",
    "#-----------------------#\n",
    "# Keep useful variables #\n",
    "#-----------------------#\n",
    "\n",
    "CaliforniaData = CaliforniaData[[\n",
    "    'Issuer','County','State','year',\n",
    "    'AdvisorFeeRatio', 'CRFeeRatio','InsureFeeRatio',\n",
    "    'GPF_if_prior_insurer', 'GPF_if_prior_advisor', 'GPF_if_prior_rater',\n",
    "    'GPF_security_type', 'GPF_taxable_code', 'GPF_Bid',\n",
    "    'GPF_amount_inf_adjusted', 'GPF_amount_bracket', 'GPF_maturity_bracket',\n",
    "    'GPF_if_refunding']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e18b3-e8bb-4ed1-b2d9-d9028b5d1c34",
   "metadata": {},
   "source": [
    "# 2. Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c830da2-e5e4-4293-be38-51487e383b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TexasData = pd.read_excel('../RawData/Texas/Local Fees with Maturity FY03-23 (Renping) (1.25.24).xlsb')\n",
    "\n",
    "#############################\n",
    "# Pin down county of issuer #\n",
    "#############################\n",
    "\n",
    "# Note that some issuers might not belong to any county\n",
    "\n",
    "# Codes below are adapted from codes for pre-processing Mergent data\n",
    "\n",
    "# Notes:\n",
    "# (1) Some counties, including those that issue municipal bonds, are not part of any CBSA. See Harlan County as in\n",
    "# https://en.wikipedia.org/wiki/Kentucky_statistical_areas.\n",
    "# (2) To avoid issues from counties with \"&\" in between be identified as issues from multiple counties, and also to facilitate \n",
    "# parsing of cities, I pre-process both county/city data and issuer data by (1) Adding \"_\"s for any county/city names with more\n",
    "# than one words, and (2) Replacing phrases of names in issuer data with versions that have different words connected with \"_\" \n",
    "# in between.\n",
    "\n",
    "###################\n",
    "# List of issuers #\n",
    "###################\n",
    "\n",
    "# Match each issue to a county (if possible) by name\n",
    "issuers = pd.DataFrame(TexasData.value_counts(['GovernmentName']))\n",
    "issuers = issuers.reset_index()\n",
    "issuers = issuers.rename(columns={0:'n_issues'})\n",
    "# Create a unique ID for each issuer\n",
    "issuers['issuer_id'] = range(0,len(issuers))\n",
    "TexasData = TexasData.merge(issuers,on=['GovernmentName'])\n",
    "issuers['GovernmentName'] = issuers['GovernmentName'].str.upper()\n",
    "issuers['State'] = 'TX'\n",
    "\n",
    "%run -i SCRIPT_us_states.py\n",
    "\n",
    "###################\n",
    "# Import counties #\n",
    "###################\n",
    "\n",
    "# Complete list of counties, including those not part of CSA \n",
    "all_counties = pd.read_csv(\"../RawData/MSA/fips-by-state.csv\",sep=',',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "all_counties = all_counties.rename(columns={'name':'County','state':'State'})\n",
    "all_counties['County'] = all_counties['County'].str.upper()\n",
    "all_counties['County'] = all_counties['County'].str.replace(' COUNTY','')\n",
    "all_counties['County'] = all_counties['County'].str.replace(' AND ',' & ')\n",
    "all_counties['County'] = all_counties['County'].str.replace(' ','_')\n",
    "all_counties['County'] = all_counties['County'].str.replace('.','',regex=False)\n",
    "\n",
    "# Keep only Texas\n",
    "all_counties = all_counties[all_counties['State']=='TX']\n",
    "\n",
    "#######################################\n",
    "# Import cities and city equivalences #\n",
    "#######################################\n",
    "\n",
    "# (https://github.com/grammakov/USA-cities-and-states/blob/master/us_cities_states_counties.csv)\n",
    "all_cities = pd.read_csv(\"../RawData/MSA/us_cities_states_counties.csv\",sep='|',encoding=\"ISO-8859-1\",low_memory=False)\n",
    "all_cities_alias = all_cities.drop(columns=['City']).rename(columns={'City alias':'City'})\n",
    "all_cities = pd.concat([all_cities.drop(columns=['City alias']),all_cities_alias])\n",
    "all_cities = all_cities.drop_duplicates()\n",
    "all_cities['City'] = all_cities['City'].str.upper()\n",
    "# Facilitate matching in scenarios like \"Lewis and Clark County\"\n",
    "all_cities['County'] = all_cities['County'].str.replace(' AND ',' & ')\n",
    "all_cities['City'] = all_cities['City'].str.replace(' AND ',' & ')\n",
    "# Add \"_\" to county names with spaces\n",
    "all_cities['County'] = all_cities['County'].str.replace(' ','_')\n",
    "all_cities['City'] = all_cities['City'].str.replace(' ','_')\n",
    "all_cities['County'] = all_cities['County'].str.replace('.','',regex=False)\n",
    "all_cities['City'] = all_cities['City'].str.replace('.','',regex=False)\n",
    "all_cities = all_cities.rename(columns={'State short':'State'})\n",
    "\n",
    "# Keep only Texas\n",
    "all_cities = all_cities[all_cities['State']=='TX']\n",
    "\n",
    "##########################\n",
    "# Import school district #\n",
    "##########################\n",
    "\n",
    "all_schooldistrcts = pd.read_csv(\"../RawData/MSA/school-districts_lea_directory.csv\",low_memory=False)\n",
    "all_schooldistrcts = all_schooldistrcts[['lea_name','state_mailing','county_name']]\n",
    "all_schooldistrcts = all_schooldistrcts.drop_duplicates()\n",
    "all_schooldistrcts = all_schooldistrcts.rename(\n",
    "    columns={'lea_name':'SchoolDistrict','state_mailing':'State','county_name':'County'})\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.upper()\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' COUNTY','')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' AND ',' & ')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace(' ','_')\n",
    "all_schooldistrcts['County'] = all_schooldistrcts['County'].str.replace('.','',regex=False)\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.upper()\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.replace(' AND ',' & ')\n",
    "all_schooldistrcts['SchoolDistrict'] = all_schooldistrcts['SchoolDistrict'].str.replace('.','',regex=False)\n",
    "\n",
    "all_schooldistrcts = all_schooldistrcts[~pd.isnull(all_schooldistrcts['SchoolDistrict'])]\n",
    "all_schooldistrcts[all_schooldistrcts['County']!='-1']\n",
    "all_schooldistrcts[all_schooldistrcts['County']!='-2']\n",
    "\n",
    "# A version of school district name that has subfixes removed, which will be used to merge with issuer data\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict']\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace('-',' ')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' INC','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CORP','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DISTRICT','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' DISTRICT','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' LOCAL SCHOOL DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL DIS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCH DIST','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCH DIS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEMENTARY','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEM','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ISD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ISDA','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' DISTRIC','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' LOCAL SD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CSD','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' AREA SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' MUNICIPAL SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' PUBLIC SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOLS','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' TOWNSHIP','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' CHARTER SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' HIGH SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' PREPARATORY SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' ELEMENTARY SCHOOL','')\n",
    "all_schooldistrcts['SchoolDistrict_NoSubfix'] = all_schooldistrcts['SchoolDistrict_NoSubfix'].str.replace(' SCHOOL','')\n",
    "\n",
    "# Keep only Texas\n",
    "all_schooldistrcts = all_schooldistrcts[all_schooldistrcts['State']=='TX']\n",
    "\n",
    "##########################################\n",
    "# Round 1: Match by county or city names #\n",
    "##########################################\n",
    "\n",
    "def proc_list(issuers):\n",
    "\n",
    "    issuers = issuers.copy()\n",
    "    \n",
    "    # Pre-process issuer data and replace phrases of names in issuer data with versions that have different words connected with\n",
    "    # \"_\" in between. This facilitates parsing and matching. Note that in this step I add \"_\" to distinguish blocks in issuer names\n",
    "    # only if the specific issuer is in the same state as the entity for which the name has multiple words. This addresses cases like\n",
    "    # \"Thief River Falls\" and \"River Falls\" which are in different states\n",
    "    for idx,row in all_counties[all_counties['County'].str.contains('_')].iterrows():\n",
    "        issuers.loc[issuers['State']==row['State'],'GovernmentName'] = \\\n",
    "            issuers[issuers['State']==row['State']]['GovernmentName'].str.replace(\n",
    "            row['County'].replace('_',' '),row['County'],regex=False)\n",
    "    for idx,row in all_cities[all_cities['City'].str.contains('_')].iterrows():\n",
    "        issuers.loc[issuers['State']==row['State'],'GovernmentName'] = \\\n",
    "            issuers[issuers['State']==row['State']]['GovernmentName'].str.replace(\n",
    "            row['City'].replace('_',' '),row['City'],regex=False)\n",
    "\n",
    "    # Initialize fields\n",
    "    issuers['County'] = None\n",
    "    issuers['City'] = None\n",
    "\n",
    "    # Check if it is a county\n",
    "    for idx,row in issuers.iterrows():\n",
    "    \n",
    "        ###########################################################################################\n",
    "        # Handle those clean cases where county names and the key word \"CNTY\" are in issuer names #\n",
    "        ###########################################################################################\n",
    "\n",
    "        # If issue is by one single county\n",
    "        name_county = row['GovernmentName']\n",
    "        all_counties_frag = all_counties[(all_counties['County']==name_county)&(all_counties['State']==row['State'])]\\\n",
    "            .reset_index()\n",
    "        if len(all_counties_frag)==1:\n",
    "            issuers.at[idx,'County'] = name_county\n",
    "\n",
    "        ##############################################################################\n",
    "        # Match by county or city or city equivalence or school district or hospital #\n",
    "        ##############################################################################\n",
    "\n",
    "        # Conduct this as long as a match by county has not been identifed. Also note that I handle prefixes (NEWS) in this step\n",
    "\n",
    "        # Match by county again. This addresses cases where key words \"CNTY\" are not in issuer names\n",
    "        if issuers.at[idx,'County']==None:\n",
    "            issuer_long_name = row['GovernmentName']\n",
    "            issuer_long_name_noloc = issuer_long_name[10:] if issuer_long_name[:9]=='NORTHEAST' else issuer_long_name\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHEAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='NORTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='EAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='WEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='NORTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='SOUTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[8:] if issuer_long_name_noloc[:7]=='CENTRAL' else issuer_long_name_noloc\n",
    "            name_county = issuer_long_name.split(' ')[0]\n",
    "            name_county_noloc = issuer_long_name_noloc.split(' ')[0]\n",
    "            all_counties_frag = all_counties[\n",
    "                ((all_counties['County']==name_county)|(all_counties['County']==name_county_noloc))\n",
    "                &(all_counties['State']==row['State'])].reset_index()\n",
    "            if len(all_counties_frag)==1:\n",
    "                issuers.at[idx,'County'] = all_counties_frag['County'][0]\n",
    "\n",
    "        # Match by city\n",
    "        if issuers.at[idx,'County']==None:\n",
    "            issuer_long_name = row['GovernmentName']\n",
    "            issuer_long_name_noloc = issuer_long_name[10:] if issuer_long_name[:9]=='NORTHEAST' else issuer_long_name\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHEAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='NORTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[10:] if issuer_long_name_noloc[:9]=='SOUTHWEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='EAST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[5:] if issuer_long_name_noloc[:4]=='WEST' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='NORTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[6:] if issuer_long_name_noloc[:5]=='SOUTH' else issuer_long_name_noloc\n",
    "            issuer_long_name_noloc = issuer_long_name_noloc[8:] if issuer_long_name_noloc[:7]=='CENTRAL' else issuer_long_name_noloc\n",
    "            name_city = issuer_long_name.split(' ')[0]\n",
    "            name_city_noloc = issuer_long_name_noloc.split(' ')[0]\n",
    "            all_cities_frag = all_cities[\n",
    "                ((all_cities['City']==name_city)|(all_cities['City']==name_city_noloc))\n",
    "                &(all_cities['State']==row['State'])].reset_index()\n",
    "            # Note that there could be duplicate entries in \"all_cities\"\n",
    "            if len(all_cities_frag)>=1:\n",
    "                issuers.at[idx,'City'] = name_city\n",
    "                issuers.at[idx,'County'] = all_cities_frag['County'][0]\n",
    "\n",
    "    return issuers\n",
    "\n",
    "meta_columns = list(proc_list(issuers[:10]).columns)\n",
    "issuers_dd = dd.from_pandas(issuers, npartitions=10)\n",
    "with dask.config.set(scheduler='processes',num_workers=10):\n",
    "    issuers = issuers_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Round 2: Match using school district names #\n",
    "##############################################\n",
    "\n",
    "# Sometimes school district name can be not related to county/city name. E.g., Calallen Independent School District is a school \n",
    "# district located in northwest Corpus Christi, Texas in northern Nueces County.\n",
    "\n",
    "for idx,row in issuers.iterrows():\n",
    "\n",
    "    if str(row['County'])=='None':\n",
    "\n",
    "        GovernmentName = row['GovernmentName']\n",
    "        GovernmentName = GovernmentName.replace('-',' ')\n",
    "        GovernmentName = GovernmentName.replace(' INC','')\n",
    "        GovernmentName = GovernmentName.replace(' CORP','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHOOL DISTRICT','')\n",
    "        GovernmentName = GovernmentName.replace(' DISTRICT','')\n",
    "        GovernmentName = GovernmentName.replace(' LOCAL SCHOOL DIST','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHOOL DIST','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHOOL DIS','')\n",
    "        GovernmentName = GovernmentName.replace(' SCH DIST','')\n",
    "        GovernmentName = GovernmentName.replace(' SCH DIS','')\n",
    "        GovernmentName = GovernmentName.replace(' ELEMENTARY','')\n",
    "        GovernmentName = GovernmentName.replace(' ELEM','')\n",
    "        GovernmentName = GovernmentName.replace(' ISD','')\n",
    "        GovernmentName = GovernmentName.replace(' ISDA','')\n",
    "        GovernmentName = GovernmentName.replace(' DISTRIC','')\n",
    "        GovernmentName = GovernmentName.replace(' LOCAL SD','')\n",
    "        GovernmentName = GovernmentName.replace(' SD','')\n",
    "        GovernmentName = GovernmentName.replace(' CSD','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHS','')\n",
    "        GovernmentName = GovernmentName.replace(' AREA SCHOOLS','')\n",
    "        GovernmentName = GovernmentName.replace(' MUNICIPAL SCHOOLS','')\n",
    "        GovernmentName = GovernmentName.replace(' PUBLIC SCHOOLS','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHOOLS','')\n",
    "        GovernmentName = GovernmentName.replace(' TOWNSHIP','')\n",
    "        GovernmentName = GovernmentName.replace(' CHARTER SCHOOL','')\n",
    "        GovernmentName = GovernmentName.replace(' HIGH SCHOOL','')\n",
    "        GovernmentName = GovernmentName.replace(' PREPARATORY SCHOOL','')\n",
    "        GovernmentName = GovernmentName.replace(' ELEMENTARY SCHOOL','')\n",
    "        GovernmentName = GovernmentName.replace(' SCHOOL','')\n",
    "        \n",
    "        all_schooldistrcts_frag = all_schooldistrcts[(all_schooldistrcts['SchoolDistrict_NoSubfix']==GovernmentName)\n",
    "            &(all_schooldistrcts['State']==row['State'])]\\\n",
    "            .reset_index()\n",
    "\n",
    "        if len(all_schooldistrcts_frag)>=1:\n",
    "            issuers.at[idx,'County'] = all_schooldistrcts_frag['County'][0]\n",
    "\n",
    "# Merge in county info\n",
    "TexasData = TexasData.merge(issuers[['issuer_id','County']],on='issuer_id')\n",
    "\n",
    "# Most governments can be matched to a county. Note that \"_\" is still in the \"County\" field currently\n",
    "TexasData['County'] = TexasData['County'].str.replace('_',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c874c496-2d67-41b9-a079-cba353ca34a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 43s, sys: 15.8 s, total: 7min 59s\n",
      "Wall time: 8min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#----------------------------#\n",
    "# Keep only fees of interest #\n",
    "#----------------------------#\n",
    "\n",
    "TexasData = TexasData[\n",
    "    (TexasData['FeeName']=='FinancialAdvisor')|\n",
    "    (TexasData['FeeName']=='Rating')|\n",
    "    (TexasData['FeeName']=='SpreadExpenses')|\n",
    "    (TexasData['FeeName']=='BondInsurance')|\n",
    "    (TexasData['FeeName']=='NetUnderwritersSpread')\n",
    "    ].copy()\n",
    "TexasData.loc[TexasData['FeeName']=='SpreadExpenses','FeeName'] = 'NetUnderwritersSpread'\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "# Whether the issuer has worked with the CRA/financial advisor/insurer in prior years (to capture frequent issuer discount) #\n",
    "#---------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "TexasData_gb = TexasData.groupby('IssuerName')\n",
    "\n",
    "TexasData['GPF_if_prior_insurer'] = None\n",
    "TexasData['GPF_if_prior_advisor'] = None\n",
    "TexasData['GPF_if_prior_rater'] = None\n",
    "\n",
    "for idx,row in TexasData.iterrows():\n",
    "    \n",
    "    TexasData_oneissuer = TexasData_gb.get_group(row['IssuerName'])\n",
    "    TexasData_oneissuer = TexasData_oneissuer[TexasData_oneissuer['FiscalYearIssuance']<row['FiscalYearIssuance']]\n",
    "\n",
    "    # Whether having prior relationship with insurer\n",
    "    if row['FeeName']=='BondInsurance':\n",
    "        TexasData_oneissuer_insurers = TexasData_oneissuer[TexasData_oneissuer['FeeName']=='BondInsurance']\n",
    "        prior_insurers = list(TexasData_oneissuer_insurers['FirmName'].unique())\n",
    "        prior_insurers = [item for item in prior_insurers if item!=None]\n",
    "        prior_insurers = [item for item in prior_insurers if str(item)!='nan']\n",
    "        prior_insurers = [item for item in prior_insurers if str(item)!='None']\n",
    "        if_prior_insurer = row['FirmName'] in prior_insurers\n",
    "        # Edit for all issues of the issuer in the same year\n",
    "        TexasData.loc[(TexasData['IssuerName']==row['IssuerName'])\n",
    "            &(TexasData['FiscalYearIssuance']==row['FiscalYearIssuance']),\n",
    "            'GPF_if_prior_insurer'] = if_prior_insurer\n",
    "\n",
    "    # Whether having prior relationship with credit rater\n",
    "    if row['FeeName']=='Rating':\n",
    "        TexasData_oneissuer_raters = TexasData_oneissuer[TexasData_oneissuer['FeeName']=='Rating']\n",
    "        prior_raters = list(TexasData_oneissuer_raters['FirmName'].unique())\n",
    "        prior_raters = [item for item in prior_raters if item!=None]\n",
    "        prior_raters = [item for item in prior_raters if str(item)!='nan']\n",
    "        prior_raters = [item for item in prior_raters if str(item)!='None']\n",
    "        if_prior_rater = row['FirmName'] in prior_raters\n",
    "        # Edit for all issues of the issuer in the same year\n",
    "        TexasData.loc[(TexasData['IssuerName']==row['IssuerName'])\n",
    "            &(TexasData['FiscalYearIssuance']==row['FiscalYearIssuance']),\n",
    "            'GPF_if_prior_rater'] = if_prior_rater\n",
    "\n",
    "    # Whether having prior relationship with advisor\n",
    "    if row['FeeName']=='FinancialAdvisor':\n",
    "        TexasData_oneissuer_advisors = TexasData_oneissuer[TexasData_oneissuer['FeeName']=='FinancialAdvisor']\n",
    "        prior_advisors = list(TexasData_oneissuer_advisors['FirmName'].unique())\n",
    "        prior_advisors = [item for item in prior_advisors if item!=None]\n",
    "        prior_advisors = [item for item in prior_advisors if str(item)!='nan']\n",
    "        prior_advisors = [item for item in prior_advisors if str(item)!='None']\n",
    "        if_prior_advisor = row['FirmName'] in prior_advisors\n",
    "        # Edit for all issues of the issuer in the same year\n",
    "        TexasData.loc[(TexasData['IssuerName']==row['IssuerName'])\n",
    "            &(TexasData['FiscalYearIssuance']==row['FiscalYearIssuance']),\n",
    "            'GPF_if_prior_advisor'] = if_prior_advisor\n",
    "\n",
    "#-----------------------------#\n",
    "# Convert data to issue level #\n",
    "#-----------------------------#\n",
    "\n",
    "# Aggregate within an issue\n",
    "TexasData_sum = TexasData.groupby( \\\n",
    "    ['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate','FeeName']). \\\n",
    "    agg({'ActualFee':sum})\n",
    "TexasData_sum = TexasData_sum.reset_index()\n",
    "TexasData = TexasData.drop(columns=['ActualFee'])\n",
    "TexasData = TexasData.drop_duplicates(subset= \\\n",
    "    ['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate','FeeName'])\n",
    "TexasData = TexasData.merge(TexasData_sum, \\\n",
    "    on=['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate','FeeName'])\n",
    "\n",
    "# Pivot table to wide\n",
    "TexasData_wide = TexasData.pivot( \\\n",
    "    index=['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate'],\n",
    "    columns='FeeName',values='ActualFee')\n",
    "TexasData_wide = TexasData_wide.reset_index()\n",
    "TexasData = TexasData.drop(columns=['FeeName','FirmName','FeeType','IssuanceFeeComments','ActualFee'])\n",
    "TexasData = TexasData.drop_duplicates( \\\n",
    "    subset=['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate'])\n",
    "TexasData = TexasData_wide.merge(TexasData, \\\n",
    "    on=['GovernmentName','GovernmentType','IssuerName','IssuanceName','FiscalYearIssuance','ClosingDate'])\n",
    "\n",
    "# Rename to have consistent variable names with California\n",
    "TexasData = TexasData.rename(columns={\n",
    "    'IssuerName':'Issuer',\n",
    "    'BondInsurance':'Credit Enhancement Fee',\n",
    "    'FinancialAdvisor':'Financial Advisor Fee',\n",
    "    'Rating':'Rating Agency Fee',\n",
    "    'ActualPar':'Principal Amount',\n",
    "    'FiscalYearIssuance':'year',\n",
    "    'YearsToMaturity':'maturity_in_years'})\n",
    "TexasData['State'] = 'TX'\n",
    "\n",
    "#------------------------#\n",
    "# Generate fee variables #\n",
    "#------------------------#\n",
    "\n",
    "TexasData['AdvisorFeeRatio'] = TexasData['Financial Advisor Fee']/TexasData['Principal Amount']\n",
    "TexasData['CRFeeRatio'] = TexasData['Rating Agency Fee']/TexasData['Principal Amount']\n",
    "TexasData['InsureFeeRatio'] = TexasData['Credit Enhancement Fee']/TexasData['Principal Amount']\n",
    "\n",
    "# Winsorize data\n",
    "upper_limit = np.percentile(TexasData['AdvisorFeeRatio'][np.logical_not(np.isnan(TexasData['AdvisorFeeRatio']))],99)\n",
    "lower_limit = np.percentile(TexasData['AdvisorFeeRatio'][np.logical_not(np.isnan(TexasData['AdvisorFeeRatio']))],1)\n",
    "TexasData.loc[(TexasData['AdvisorFeeRatio']>upper_limit)&(np.logical_not(np.isnan(TexasData['AdvisorFeeRatio']))),\n",
    "    'AdvisorFeeRatio'] = \\\n",
    "    upper_limit\n",
    "TexasData.loc[(TexasData['AdvisorFeeRatio']<lower_limit)&(np.logical_not(np.isnan(TexasData['AdvisorFeeRatio']))),\n",
    "    'AdvisorFeeRatio'] = \\\n",
    "    lower_limit\n",
    "upper_limit = np.percentile(TexasData['CRFeeRatio'][np.logical_not(np.isnan(TexasData['CRFeeRatio']))],99)\n",
    "lower_limit = np.percentile(TexasData['CRFeeRatio'][np.logical_not(np.isnan(TexasData['CRFeeRatio']))],1)\n",
    "TexasData.loc[(TexasData['CRFeeRatio']>upper_limit)&(np.logical_not(np.isnan(TexasData['CRFeeRatio']))),\n",
    "    'CRFeeRatio'] = \\\n",
    "    upper_limit\n",
    "TexasData.loc[(TexasData['CRFeeRatio']<lower_limit)&(np.logical_not(np.isnan(TexasData['CRFeeRatio']))),\n",
    "    'CRFeeRatio'] = \\\n",
    "    lower_limit\n",
    "upper_limit = np.percentile(TexasData['InsureFeeRatio'][np.logical_not(np.isnan(TexasData['InsureFeeRatio']))],99)\n",
    "lower_limit = np.percentile(TexasData['InsureFeeRatio'][np.logical_not(np.isnan(TexasData['InsureFeeRatio']))],1)\n",
    "TexasData.loc[(TexasData['InsureFeeRatio']>upper_limit)&(np.logical_not(np.isnan(TexasData['InsureFeeRatio']))),\n",
    "    'InsureFeeRatio'] = \\\n",
    "    upper_limit\n",
    "TexasData.loc[(TexasData['InsureFeeRatio']<lower_limit)&(np.logical_not(np.isnan(TexasData['InsureFeeRatio']))),\n",
    "    'InsureFeeRatio'] = \\\n",
    "    lower_limit\n",
    "\n",
    "#----------------------------------------------#\n",
    "# Recreate variables to be consistent with GPF #\n",
    "#----------------------------------------------#\n",
    "\n",
    "# Fields for if prior relationships are missing if not using a certain fee\n",
    "TexasData.loc[pd.isnull(TexasData['GPF_if_prior_insurer']),'GPF_if_prior_insurer'] = True\n",
    "TexasData.loc[pd.isnull(TexasData['GPF_if_prior_advisor']),'GPF_if_prior_advisor'] = True\n",
    "TexasData.loc[pd.isnull(TexasData['GPF_if_prior_rater']),'GPF_if_prior_rater'] = True\n",
    "\n",
    "TexasData['GPF_security_type'] = None\n",
    "TexasData.loc[TexasData['PledgeType']=='GO','GPF_security_type'] = 'GO'\n",
    "TexasData.loc[TexasData['PledgeType']=='REV','GPF_security_type'] = 'RV'\n",
    "\n",
    "TexasData['GPF_taxable_code'] = 'E'\n",
    "\n",
    "TexasData['GPF_Bid'] = None\n",
    "TexasData.loc[TexasData['SaleType']=='Negotiated','GPF_Bid'] = 'N'\n",
    "TexasData.loc[TexasData['SaleType']=='Competitive','GPF_Bid'] = 'C'\n",
    "\n",
    "# Adjust inflation\n",
    "FPCPITOTLZGUSA = pd.read_csv(\"../RawData/StLouisFed/FPCPITOTLZGUSA.csv\")\n",
    "FPCPITOTLZGUSA['year'] = FPCPITOTLZGUSA['DATE'].str[:4].astype(int)\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA.sort_values('year',ascending=False).reset_index(drop=True)\n",
    "scaler = 1\n",
    "FPCPITOTLZGUSA['scaler'] = None\n",
    "for idx,row in FPCPITOTLZGUSA.iterrows():\n",
    "    if idx==0:\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = 1\n",
    "    else:\n",
    "        scaler = scaler*(FPCPITOTLZGUSA.at[idx-1,'FPCPITOTLZGUSA']/100+1)\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = scaler\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA[['scaler','year']]\n",
    "\n",
    "TexasData = TexasData.merge(FPCPITOTLZGUSA,on=['year'])\n",
    "TexasData['GPF_amount_inf_adjusted'] = TexasData['Principal Amount']*TexasData['scaler']\n",
    "\n",
    "TexasData['GPF_amount_bracket'] = None\n",
    "TexasData.loc[TexasData['GPF_amount_inf_adjusted']<=1*1000000,\n",
    "    'GPF_amount_bracket'] = 'Less than 1M'\n",
    "TexasData.loc[(TexasData['GPF_amount_inf_adjusted']>1*1000000)&(TexasData['GPF_amount_inf_adjusted']<=5*1000000),\n",
    "    'GPF_amount_bracket'] = '1M to 5M'\n",
    "TexasData.loc[(TexasData['GPF_amount_inf_adjusted']>5*1000000)&(TexasData['GPF_amount_inf_adjusted']<=10*1000000),\n",
    "    'GPF_amount_bracket'] = '5M to 10M'\n",
    "TexasData.loc[(TexasData['GPF_amount_inf_adjusted']>10*1000000)&(TexasData['GPF_amount_inf_adjusted']<=50*1000000),\n",
    "    'GPF_amount_bracket'] = '10M to 50M'\n",
    "TexasData.loc[(TexasData['GPF_amount_inf_adjusted']>50*1000000)&(TexasData['GPF_amount_inf_adjusted']<=100*1000000),\n",
    "    'GPF_amount_bracket'] = '50M to 100M'\n",
    "TexasData.loc[TexasData['GPF_amount_inf_adjusted']>100*1000000,\n",
    "    'GPF_amount_bracket'] = 'Greater than 100M'\n",
    "\n",
    "TexasData['GPF_maturity_bracket'] = None\n",
    "TexasData.loc[TexasData['maturity_in_years']<=2,'GPF_maturity_bracket'] = 'Less then 2y'\n",
    "TexasData.loc[(TexasData['maturity_in_years']>2)&(TexasData['maturity_in_years']<=5),\n",
    "    'GPF_maturity_bracket'] = '2y to 5y'\n",
    "TexasData.loc[(TexasData['maturity_in_years']>5)&(TexasData['maturity_in_years']<=10),\n",
    "    'GPF_maturity_bracket'] = '5y to 10y'\n",
    "TexasData.loc[(TexasData['maturity_in_years']>10)&(TexasData['maturity_in_years']<=20),\n",
    "    'GPF_maturity_bracket'] = '10y to 20y'\n",
    "TexasData.loc[(TexasData['maturity_in_years']>20)&(TexasData['maturity_in_years']<=30),\n",
    "    'GPF_maturity_bracket'] = '20y to 30y'\n",
    "TexasData.loc[(TexasData['maturity_in_years']>30)&(TexasData['maturity_in_years']<=40),\n",
    "    'GPF_maturity_bracket'] = '30y to 40y'\n",
    "TexasData.loc[TexasData['maturity_in_years']>40,\n",
    "    'GPF_maturity_bracket'] = 'Greater than 40y'\n",
    "\n",
    "TexasData['GPF_if_refunding'] = TexasData['RefundingPar']>0\n",
    "\n",
    "#-----------------------#\n",
    "# Keep useful variables #\n",
    "#-----------------------#\n",
    "\n",
    "TexasData = TexasData[[\n",
    "    'Issuer','County','State','year',\n",
    "    'AdvisorFeeRatio', 'CRFeeRatio','InsureFeeRatio',\n",
    "    'GPF_if_prior_insurer', 'GPF_if_prior_advisor', 'GPF_if_prior_rater',\n",
    "    'GPF_security_type', 'GPF_taxable_code', 'GPF_Bid',\n",
    "    'GPF_amount_inf_adjusted', 'GPF_amount_bracket', 'GPF_maturity_bracket',\n",
    "    'GPF_if_refunding']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0028c-a257-4edc-9adc-25503542ca06",
   "metadata": {},
   "source": [
    "# 3. Combine California and Texas, and predict costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6495d15a-41e9-48a1-a816-aa51c79e958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------#\n",
    "# Combine California and Texas #\n",
    "#------------------------------#\n",
    "\n",
    "CostSample = pd.concat([TexasData,CaliforniaData])\n",
    "\n",
    "######################################\n",
    "# Merge in county level demographics #\n",
    "######################################\n",
    "\n",
    "County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "\n",
    "# Get national average county-level income and population\n",
    "County_Composite_yearlyavg = County_Composite.groupby('year').agg({'pop':'mean','inc':'mean'})\n",
    "County_Composite_yearlyavg = County_Composite_yearlyavg.rename(columns={'pop':'pop_yearlyavg','inc':'inc_yearlyavg'})\n",
    "County_Composite = County_Composite.merge(County_Composite_yearlyavg,on=['year'])\n",
    "County_Composite['pop_to_avg'] = County_Composite['pop']/County_Composite['pop_yearlyavg']\n",
    "County_Composite['inc_to_avg'] = County_Composite['inc']/County_Composite['inc_yearlyavg']\n",
    "\n",
    "CostSample = CostSample.merge(County_Composite[['State','County','year','pop_to_avg','inc_to_avg']],on=['State','County','year'])\n",
    "\n",
    "# Convert True or False into 0 and 1\n",
    "CostSample['GPF_if_prior_insurer'] = CostSample['GPF_if_prior_insurer'].astype(int)\n",
    "CostSample['GPF_if_prior_advisor'] = CostSample['GPF_if_prior_advisor'].astype(int)\n",
    "CostSample['GPF_if_prior_rater'] = CostSample['GPF_if_prior_rater'].astype(int)\n",
    "CostSample['GPF_if_refunding'] = CostSample['GPF_if_refunding'].astype(int)\n",
    "\n",
    "CostSample.to_csv('../CleanData/California/0H_SumStats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7568c9-dec3-45f2-a8c7-dac90fe72ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Import GPF #\n",
    "##############\n",
    "\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "\n",
    "GPF['is_security_type_GO'] = GPF['security_type']=='GO'\n",
    "GPF['is_security_type_RV'] = GPF['security_type']=='RV'\n",
    "GPF['is_Bid_C'] = GPF['Bid']=='C'\n",
    "GPF['is_Bid_N'] = GPF['Bid']=='N'\n",
    "GPF['is_taxable_code_A'] = GPF['taxable_code']=='A'\n",
    "GPF['is_taxable_code_E'] = GPF['taxable_code']=='E'\n",
    "GPF['is_taxable_code_T'] = GPF['taxable_code']=='T'\n",
    "\n",
    "GPF = GPF.merge(FPCPITOTLZGUSA.rename(columns={'year':'sale_year'}),on=['sale_year'],how='outer',indicator=True)\n",
    "GPF = GPF[GPF['_merge']!='right_only']\n",
    "GPF = GPF.drop(columns=['_merge'])\n",
    "GPF['amount_inf_adjusted'] = GPF['amount']*GPF['scaler']*1000000\n",
    "\n",
    "GPF['is_amount_Less_than_1M'] = GPF['amount_inf_adjusted']<1*1000000\n",
    "GPF['is_amount_1M_to_5M'] = (GPF['amount_inf_adjusted']>1*1000000)&(GPF['amount_inf_adjusted']<=5*1000000)\n",
    "GPF['is_amount_5M_to_10M'] = (GPF['amount_inf_adjusted']>5*1000000)&(GPF['amount_inf_adjusted']<=10*1000000)\n",
    "GPF['is_amount_10M_to_50M'] = (GPF['amount_inf_adjusted']>10*1000000)&(GPF['amount_inf_adjusted']<=50*1000000)\n",
    "GPF['is_amount_50M_to_100M'] = (GPF['amount_inf_adjusted']>50*1000000)&(GPF['amount_inf_adjusted']<=100*1000000)\n",
    "GPF['is_amount_Greater_than_100M'] = GPF['amount_inf_adjusted']>100*1000000\n",
    "\n",
    "GPF['maturity_in_years'] = np.round(GPF['avg_maturity']/365)\n",
    "\n",
    "GPF['is_maturity_Less_than_2y'] = GPF['maturity_in_years']<2\n",
    "GPF['is_maturity_2y_to_5y'] = (GPF['maturity_in_years']>2)&(GPF['maturity_in_years']<=5)\n",
    "GPF['is_maturity_5y_to_10y'] = (GPF['maturity_in_years']>5)&(GPF['maturity_in_years']<=10)\n",
    "GPF['is_maturity_10y_to_20y'] = (GPF['maturity_in_years']>10)&(GPF['maturity_in_years']<=20)\n",
    "GPF['is_maturity_20y_to_30y'] = (GPF['maturity_in_years']>20)&(GPF['maturity_in_years']<=30)\n",
    "GPF['is_maturity_30y_to_40y'] = (GPF['maturity_in_years']>30)&(GPF['maturity_in_years']<=40)\n",
    "GPF['is_maturity_Greater_than_40y'] = GPF['maturity_in_years']>40\n",
    "\n",
    "# Convert True or False into 0 and 1\n",
    "GPF['if_prior_insurer'] = GPF['if_prior_insurer'].astype(int)\n",
    "GPF['if_prior_advisor'] = GPF['if_prior_advisor'].astype(int)\n",
    "GPF['if_prior_rater'] = GPF['if_prior_rater'].astype(int)\n",
    "\n",
    "GPF['if_refunding'] = GPF['Refunding']!='N'\n",
    "GPF['if_refunding'] = GPF['if_refunding'].astype(int)\n",
    "\n",
    "# Dummy variable for which year it is\n",
    "for year in list(GPF['sale_year'].unique()):\n",
    "    GPF['if_year_'+str(year)] = GPF['sale_year']==year\n",
    "column_years = ['if_year_'+str(year) for year in list(GPF['sale_year'].unique())]\n",
    "\n",
    "# Merge in county income/population data\n",
    "GPF = GPF.merge(County_Composite[['State','County','year','pop_to_avg','inc_to_avg']]\\\n",
    "    .rename(columns={'year':'sale_year'}),on=['State','County','sale_year'],how='outer',indicator=True)\n",
    "GPF = GPF[GPF['_merge']!='right_only']\n",
    "GPF = GPF.drop(columns=['_merge'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dd44b-03af-45bf-9782-fbb9a518eb0a",
   "metadata": {},
   "source": [
    "## 3.1 Model with no year FEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d19f30-9444-4090-ac97-e52be70718e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Advisors fee/Credit rating fee/Insurance fee #\n",
    "################################################\n",
    "\n",
    "fee_names = [\n",
    "    ['Advisor','if_prior_advisor'],\n",
    "    ['CR','if_prior_rater'],\n",
    "    ['Insure','if_prior_insurer'],\n",
    "    ]\n",
    "\n",
    "for item in fee_names:\n",
    "\n",
    "    fee_name = item[0]\n",
    "    if_prior_relationship = item[1]\n",
    "    \n",
    "    # Run regression in the California sample\n",
    "    # To make sure that order of first category is fixed\n",
    "    CostSample = CostSample.sort_values(['GPF_maturity_bracket','GPF_amount_bracket','GPF_Bid','GPF_security_type','GPF_taxable_code'])\n",
    "    maturity_bracket = pd.get_dummies(CostSample['GPF_maturity_bracket'], drop_first=True)\n",
    "    amount_bracket = pd.get_dummies(CostSample['GPF_amount_bracket'], drop_first=True)\n",
    "    Bid = pd.get_dummies(CostSample['GPF_Bid'], drop_first=True)\n",
    "    security_type = pd.get_dummies(CostSample['GPF_security_type'], drop_first=True)\n",
    "    taxable_code = pd.get_dummies(CostSample['GPF_taxable_code'], drop_first=True)\n",
    "    \n",
    "    CostSample_RegData = pd.concat([CostSample,maturity_bracket,amount_bracket,Bid,security_type,taxable_code],axis=1)\n",
    "    CostSample_RegData = CostSample_RegData.dropna(subset=[fee_name+'FeeRatio'])\n",
    "    CostSample_RegData[fee_name+'FeeRatio'] = CostSample_RegData[fee_name+'FeeRatio']*10000\n",
    "    \n",
    "    upper_limit = np.percentile(CostSample_RegData[fee_name+'FeeRatio'][np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))],99)\n",
    "    lower_limit = np.percentile(CostSample_RegData[fee_name+'FeeRatio'][np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))],1)\n",
    "    CostSample_RegData.loc[\n",
    "        (CostSample_RegData[fee_name+'FeeRatio']>upper_limit)&(np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))),\n",
    "        fee_name+'FeeRatio'] = upper_limit\n",
    "    CostSample_RegData.loc[\n",
    "        (CostSample_RegData[fee_name+'FeeRatio']<lower_limit)&(np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))),\n",
    "        fee_name+'FeeRatio'] = lower_limit\n",
    "    \n",
    "    CostSample_RegData = CostSample_RegData[CostSample_RegData[fee_name+'FeeRatio']>0.00001]\n",
    "    \n",
    "    X = CostSample_RegData[['pop_to_avg','inc_to_avg','GPF_'+if_prior_relationship,'GPF_if_refunding']\n",
    "        +list(maturity_bracket.columns)\n",
    "        +list(amount_bracket.columns)\n",
    "        +list(Bid.columns)\n",
    "        +list(security_type.columns)\n",
    "        +list(taxable_code.columns)]\n",
    "    y = CostSample_RegData[fee_name+'FeeRatio']\n",
    "    \n",
    "    CostSample_RegData.to_csv(\"../CleanData/California/0H_CostSample_RegData_\"+fee_name+\"FeeRatio.csv\")\n",
    "    \n",
    "    model = sm.OLS(y, sm.add_constant(X))\n",
    "    result = model.fit()\n",
    "    \n",
    "    # Predict would-be costs\n",
    "    GPF[fee_name+'FeeRatio_hat'] = result.params['const']+\\\n",
    "        result.params['pop_to_avg']*GPF['pop_to_avg']+\\\n",
    "        result.params['inc_to_avg']*GPF['inc_to_avg']+\\\n",
    "        result.params['GPF_'+if_prior_relationship]*GPF[if_prior_relationship]+\\\n",
    "        result.params['GPF_if_refunding']*GPF['if_refunding']+\\\n",
    "        result.params['20y to 30y']*GPF['is_maturity_20y_to_30y']+\\\n",
    "        result.params['2y to 5y']*GPF['is_maturity_2y_to_5y']+\\\n",
    "        result.params['30y to 40y']*GPF['is_maturity_30y_to_40y']+\\\n",
    "        result.params['5y to 10y']*GPF['is_maturity_5y_to_10y']+\\\n",
    "        result.params['Greater than 40y']*GPF['is_maturity_Greater_than_40y']+\\\n",
    "        result.params['Less then 2y']*GPF['is_maturity_Less_than_2y']+\\\n",
    "        result.params['1M to 5M']*GPF['is_amount_1M_to_5M']+\\\n",
    "        result.params['50M to 100M']*GPF['is_amount_50M_to_100M']+\\\n",
    "        result.params['5M to 10M']*GPF['is_amount_5M_to_10M']+\\\n",
    "        result.params['Greater than 100M']*GPF['is_amount_Greater_than_100M']+\\\n",
    "        result.params['Less than 1M']*GPF['is_amount_Less_than_1M']+\\\n",
    "        result.params['N']*GPF['is_Bid_N']+\\\n",
    "        result.params['RV']*GPF['is_security_type_RV']+\\\n",
    "        result.params['E']*GPF['is_taxable_code_E']+\\\n",
    "        result.params['T']*GPF['is_taxable_code_T']\n",
    "    \n",
    "    GPF.loc[GPF[fee_name+'FeeRatio_hat']<0,fee_name+'FeeRatio_hat'] = 0\n",
    "    GPF[fee_name+'FeeRatio_hat'] = GPF[fee_name+'FeeRatio_hat'].astype(float)\n",
    "    \n",
    "    upper_limit = np.percentile(GPF[fee_name+'FeeRatio_hat'][np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat']))],99)\n",
    "    lower_limit = np.percentile(GPF[fee_name+'FeeRatio_hat'][np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat']))],1)\n",
    "    GPF.loc[(GPF[fee_name+'FeeRatio_hat']>upper_limit)&(np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat']))),fee_name+'FeeRatio_hat'] = \\\n",
    "        upper_limit\n",
    "    GPF.loc[(GPF[fee_name+'FeeRatio_hat']<lower_limit)&(np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat']))),fee_name+'FeeRatio_hat'] = \\\n",
    "        lower_limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e346a-a210-4f32-9947-5368f16e8464",
   "metadata": {},
   "source": [
    "## 3.2 Model with year FEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ff8bc-6f5d-4ee2-8fad-77d3cf38eefd",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Prediction is done using data where each fee is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a32ac6-5a6f-48c0-b9dc-2dc3095186cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dummy for years\n",
    "for year in list(CostSample['year'].unique()):\n",
    "    CostSample['if_year_'+str(year)] = CostSample['year']==year\n",
    "    CostSample['if_year_'+str(year)] = CostSample['if_year_'+str(year)].astype(int)\n",
    "column_years_CostSample = ['if_year_'+str(year) for year in list(CostSample['year'].unique())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce742af-3a43-4c1d-a9bf-a77069bf6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Advisors fee/Credit rating fee/Insurance fee #\n",
    "################################################\n",
    "\n",
    "fee_names = [\n",
    "    ['Advisor','if_prior_advisor'],\n",
    "    ['CR','if_prior_rater'],\n",
    "    ['Insure','if_prior_insurer'],\n",
    "    ]\n",
    "\n",
    "for item in fee_names:\n",
    "\n",
    "    fee_name = item[0]\n",
    "    if_prior_relationship = item[1]\n",
    "\n",
    "    # Run regression in the California sample\n",
    "    # To make sure that order of first category is fixed\n",
    "    CostSample = CostSample.sort_values(['GPF_maturity_bracket','GPF_amount_bracket','GPF_Bid','GPF_security_type','GPF_taxable_code'])\n",
    "    maturity_bracket = pd.get_dummies(CostSample['GPF_maturity_bracket'], drop_first=True)\n",
    "    amount_bracket = pd.get_dummies(CostSample['GPF_amount_bracket'], drop_first=True)\n",
    "    Bid = pd.get_dummies(CostSample['GPF_Bid'], drop_first=True)\n",
    "    security_type = pd.get_dummies(CostSample['GPF_security_type'], drop_first=True)\n",
    "    taxable_code = pd.get_dummies(CostSample['GPF_taxable_code'], drop_first=True)\n",
    "    \n",
    "    CostSample_RegData = pd.concat([CostSample,maturity_bracket,amount_bracket,Bid,security_type,taxable_code],axis=1)\n",
    "    CostSample_RegData = CostSample_RegData.dropna(subset=[fee_name+'FeeRatio'])\n",
    "    CostSample_RegData[fee_name+'FeeRatio'] = CostSample_RegData[fee_name+'FeeRatio']*10000\n",
    "    \n",
    "    upper_limit = np.percentile(CostSample_RegData[fee_name+'FeeRatio'][np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))],99)\n",
    "    lower_limit = np.percentile(CostSample_RegData[fee_name+'FeeRatio'][np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))],1)\n",
    "    CostSample_RegData.loc[\n",
    "        (CostSample_RegData[fee_name+'FeeRatio']>upper_limit)&(np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))),\n",
    "        fee_name+'FeeRatio'] = upper_limit\n",
    "    CostSample_RegData.loc[\n",
    "        (CostSample_RegData[fee_name+'FeeRatio']<lower_limit)&(np.logical_not(np.isnan(CostSample_RegData[fee_name+'FeeRatio']))),\n",
    "        fee_name+'FeeRatio'] = lower_limit\n",
    "    \n",
    "    CostSample_RegData = CostSample_RegData[CostSample_RegData[fee_name+'FeeRatio']>0.00001]\n",
    "    \n",
    "    # Year 2000 is chosen as the left-out group\n",
    "    column_years_CostSample_in_regression = [item for item in column_years_CostSample if item!='if_year_2000']\n",
    "    # Further, throw out columns for which there is no observation for this type of fee\n",
    "    years_in_sample = ['if_year_'+str(year) for year in list(CostSample_RegData['year'].unique())]\n",
    "    column_years_CostSample_in_regression = set(column_years_CostSample_in_regression).intersection(set(years_in_sample))\n",
    "    column_years_CostSample_in_regression = list(column_years_CostSample_in_regression)\n",
    "    # \"column_years_CostSample_in_regression\" is a list of year dummies used in the regression\n",
    "    \n",
    "    X = CostSample_RegData[['pop_to_avg','inc_to_avg','GPF_'+if_prior_relationship,'GPF_if_refunding']\n",
    "        +list(maturity_bracket.columns)\n",
    "        +list(amount_bracket.columns)\n",
    "        +list(Bid.columns)\n",
    "        +list(security_type.columns)\n",
    "        +list(taxable_code.columns)\n",
    "        +column_years_CostSample_in_regression]\n",
    "    y = CostSample_RegData[fee_name+'FeeRatio']\n",
    "    \n",
    "    CostSample_RegData.to_csv(\"../CleanData/California/0H_CostSample_RegData_AdvisorFeeRatio.csv\")\n",
    "    \n",
    "    model = sm.OLS(y, sm.add_constant(X))\n",
    "    result = model.fit()\n",
    "    \n",
    "    # Predict would-be costs\n",
    "    GPF[fee_name+'FeeRatio_hat_model_timeFE'] = result.params['const']+\\\n",
    "        result.params['pop_to_avg']*GPF['pop_to_avg']+\\\n",
    "        result.params['inc_to_avg']*GPF['inc_to_avg']+\\\n",
    "        result.params['GPF_'+if_prior_relationship]*GPF[if_prior_relationship]+\\\n",
    "        result.params['GPF_if_refunding']*GPF['if_refunding']+\\\n",
    "        result.params['20y to 30y']*GPF['is_maturity_20y_to_30y']+\\\n",
    "        result.params['2y to 5y']*GPF['is_maturity_2y_to_5y']+\\\n",
    "        result.params['30y to 40y']*GPF['is_maturity_30y_to_40y']+\\\n",
    "        result.params['5y to 10y']*GPF['is_maturity_5y_to_10y']+\\\n",
    "        result.params['Greater than 40y']*GPF['is_maturity_Greater_than_40y']+\\\n",
    "        result.params['Less then 2y']*GPF['is_maturity_Less_than_2y']+\\\n",
    "        result.params['1M to 5M']*GPF['is_amount_1M_to_5M']+\\\n",
    "        result.params['50M to 100M']*GPF['is_amount_50M_to_100M']+\\\n",
    "        result.params['5M to 10M']*GPF['is_amount_5M_to_10M']+\\\n",
    "        result.params['Greater than 100M']*GPF['is_amount_Greater_than_100M']+\\\n",
    "        result.params['Less than 1M']*GPF['is_amount_Less_than_1M']+\\\n",
    "        result.params['N']*GPF['is_Bid_N']+\\\n",
    "        result.params['RV']*GPF['is_security_type_RV']+\\\n",
    "        result.params['E']*GPF['is_taxable_code_E']+\\\n",
    "        result.params['T']*GPF['is_taxable_code_T']\n",
    "    \n",
    "    # Add effects of years\n",
    "    for column in column_years_CostSample_in_regression:\n",
    "        GPF[fee_name+'FeeRatio_hat_model_timeFE'] = GPF[fee_name+'FeeRatio_hat_model_timeFE']+result.params[column]*GPF[column]\n",
    "    \n",
    "    GPF.loc[GPF[fee_name+'FeeRatio_hat_model_timeFE']<0,fee_name+'FeeRatio_hat_model_timeFE'] = 0\n",
    "    GPF[fee_name+'FeeRatio_hat_model_timeFE'] = GPF[fee_name+'FeeRatio_hat_model_timeFE'].astype(float)\n",
    "    \n",
    "    # Throw out predicted values for years out of training sample range\n",
    "    GPF.loc[~GPF['sale_year'].isin(list(CostSample_RegData['year'].unique())),fee_name+'FeeRatio_hat_model_timeFE'] = None\n",
    "    \n",
    "    upper_limit = np.percentile(GPF[fee_name+'FeeRatio_hat_model_timeFE']\\\n",
    "        [np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat_model_timeFE']))],99)\n",
    "    lower_limit = np.percentile(GPF[fee_name+'FeeRatio_hat_model_timeFE']\\\n",
    "        [np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat_model_timeFE']))],1)\n",
    "    GPF.loc[(GPF[fee_name+'FeeRatio_hat_model_timeFE']>upper_limit)\n",
    "        &(np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat_model_timeFE']))),fee_name+'FeeRatio_hat_model_timeFE'] = \\\n",
    "        upper_limit\n",
    "    GPF.loc[(GPF[fee_name+'FeeRatio_hat_model_timeFE']<lower_limit)\n",
    "        &(np.logical_not(np.isnan(GPF[fee_name+'FeeRatio_hat_model_timeFE']))),fee_name+'FeeRatio_hat_model_timeFE'] = \\\n",
    "        lower_limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c933d-21ad-4416-a73b-66930431fc2b",
   "metadata": {},
   "source": [
    "# 4. Generate measure of total financing cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82078e-461f-4878-8d2a-0faf92a6582a",
   "metadata": {},
   "source": [
    "Obtain a modified version of TIC, which sets discounted value of all future payments to (purchase price - three costs above). Here purchase price equals reoffering price minus the gross spread.\n",
    "\n",
    "The calculation is done using dollar values (in millions), to handle different bonds within an issue and their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d93a32-30d0-428a-9882-5f935de894de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def proc_list(GPF):\n",
    "\n",
    "    GPF = GPF.copy()\n",
    "    GPF['mod_tic'] = None\n",
    "    GPF['mod_tic_spread_treasury'] = None\n",
    "    GPF['mod_tic_spread_MMA'] = None\n",
    "    GPF['mod_tic_timeFE'] = None\n",
    "    GPF['mod_tic_spread_treasury_timeFE'] = None\n",
    "    GPF['mod_tic_spread_MMA_timeFE'] = None\n",
    "\n",
    "    # Handle case by case of each variable being missing, and within each case allow for multiple maturities\n",
    "    for idx,row in GPF.iterrows():\n",
    "\n",
    "        # Do nothing if the issue contains bonds with irregular type of coupon payments\n",
    "        if row['IF_irregular_coupon_type']==True:\n",
    "            continue\n",
    "    \n",
    "        # Do nothing if dated date is missing\n",
    "        if str(row['dated_date'])=='nan' or str(row['dated_date'])=='NaT':\n",
    "            continue\n",
    "    \n",
    "        # Extract vallues that describe data availablility for a particular bond issue\n",
    "\n",
    "        IF_has_maturity_date = row['IF_has_maturity_date']\n",
    "        IF_has_coupon_rate = row['IF_has_coupon_rate']\n",
    "        IF_has_price_or_yield = row['IF_has_price_or_yield']\n",
    "        IF_has_amount_by_maturity = row['IF_has_amount_by_maturity']\n",
    "        IF_has_gross_spread = row['IF_has_gross_spread']\n",
    "        \n",
    "        N_coupon_rate = row['N_coupon_rate']\n",
    "        N_price_or_yield = row['N_price_or_yield']\n",
    "        N_maturity_date = row['N_maturity_date']\n",
    "        N_amount = row['N_amount']\n",
    "    \n",
    "        IF_num_bonds_all_consistent = row['IF_num_bonds_all_consistent']\n",
    "        IF_num_bonds_yield_mat_amt_consistent = row['IF_num_bonds_yield_mat_amt_consistent']\n",
    "        IF_num_bonds_mat_amt_consistent = row['IF_num_bonds_mat_amt_consistent']\n",
    "\n",
    "        IF_has_AdvisorFee = \\\n",
    "            row['AdvisorFeeRatio_hat']!=None and \\\n",
    "            str(row['AdvisorFeeRatio_hat'])!='nan' and \\\n",
    "            str(row['AdvisorFeeRatio_hat'])!='None'\n",
    "        IF_has_CRFee = \\\n",
    "            row['CRFeeRatio_hat']!=None and \\\n",
    "            str(row['CRFeeRatio_hat'])!='nan' and \\\n",
    "            str(row['CRFeeRatio_hat'])!='None'\n",
    "        IF_has_InsureFee = \\\n",
    "            row['InsureFeeRatio_hat']!=None and \\\n",
    "            str(row['InsureFeeRatio_hat'])!='nan' and \\\n",
    "            str(row['InsureFeeRatio_hat'])!='None'\n",
    "        IF_has_AdvisorFee_timeFE = \\\n",
    "            row['AdvisorFeeRatio_hat_model_timeFE']!=None and \\\n",
    "            str(row['AdvisorFeeRatio_hat_model_timeFE'])!='nan' and \\\n",
    "            str(row['AdvisorFeeRatio_hat_model_timeFE'])!='None'\n",
    "        IF_has_CRFee_timeFE = \\\n",
    "            row['CRFeeRatio_hat_model_timeFE']!=None and \\\n",
    "            str(row['CRFeeRatio_hat_model_timeFE'])!='nan' and \\\n",
    "            str(row['CRFeeRatio_hat_model_timeFE'])!='None'\n",
    "        IF_has_InsureFee_timeFE = \\\n",
    "            row['InsureFeeRatio_hat_model_timeFE']!=None and \\\n",
    "            str(row['InsureFeeRatio_hat_model_timeFE'])!='nan' and \\\n",
    "            str(row['InsureFeeRatio_hat_model_timeFE'])!='None'\n",
    "\n",
    "        # Whether the bond issue is using credit ratings, insurance, or financial advisor\n",
    "        has_rating = row['has_Moodys'] or row['has_Fitch']\n",
    "        insured_ratio = row['insured_amount']/row['amount']\n",
    "        if_advisor_coded = row['if_advisor']==\"Yes\"\n",
    "\n",
    "        # Note that function \"npf.npv\" start with a period 0 cash flow\n",
    "\n",
    "        # The calculation below is based on dollar amounts of the whole bond issue, rather than every $100 par value\n",
    "        \n",
    "        if IF_has_maturity_date and IF_has_coupon_rate and IF_has_price_or_yield and IF_has_amount_by_maturity \\\n",
    "            and IF_has_gross_spread \\\n",
    "            and IF_num_bonds_all_consistent:\n",
    "\n",
    "            # Case A: If single maturity\n",
    "            if N_maturity_date==1:\n",
    "\n",
    "                maturity = (datetime.strptime(row['maturity_date'],\"%Y-%m-%d %H:%M:%S\")-\n",
    "                    datetime.strptime(row['dated_date'],\"%Y-%m-%d %H:%M:%S\")).days\n",
    "                n_coupon = round(maturity/182)\n",
    "    \n",
    "                # Obtain the discount of reoffering price relative to par vlaue\n",
    "                # Assume that if a number is more than 80 and less than 120, it is issuing price. If less than 20, it is issuing \n",
    "                # yield. Otherwise, undetermined\n",
    "                if float(row['price_or_yield'])<20:\n",
    "                    reoffering_yield = float(row['price_or_yield'])/100\n",
    "                    reoffering_price = npf.npv(sqrt(1+reoffering_yield)-1,\n",
    "                        [0]+[float(row['amount'])*float(row['coupon_rate'])/100/2]*(n_coupon-1)+[float(row['amount'])+float(row['amount'])*float(row['coupon_rate'])/100/2])\n",
    "                elif float(row['price_or_yield'])>80 and float(row['price_or_yield'])<120:\n",
    "                    reoffering_price = float(row['price_or_yield'])/100*float(row['amount'])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Calculate purchase price\n",
    "                if IF_has_AdvisorFee and IF_has_CRFee and IF_has_InsureFee:\n",
    "                    # Gross spread is represented in the unit of every $1000\n",
    "                    purchase_price = reoffering_price-row['gross_spread']/1000*float(row['amount'])\n",
    "                    # Other fees are represented in the unit of basis points\n",
    "                    if if_advisor_coded:\n",
    "                        purchase_price = purchase_price-row['AdvisorFeeRatio_hat']/10000*float(row['amount'])\n",
    "                    if has_rating:\n",
    "                        purchase_price = purchase_price-row['CRFeeRatio_hat']/10000*float(row['amount'])\n",
    "                    if insured_ratio:\n",
    "                        purchase_price = purchase_price-row['InsureFeeRatio_hat']/10000*float(row['amount'])\n",
    "                \n",
    "                    # Calculate TIC implied by NIC\n",
    "                    mod_tic = (1+npf.irr([-purchase_price]+\\\n",
    "                        [float(row['coupon_rate'])/2/100*float(row['amount'])]*(n_coupon-1)+[float(row['amount'])+float(row['coupon_rate'])/2/100*float(row['amount'])]))\\\n",
    "                        **2-1        \n",
    "                    GPF.loc[idx,'mod_tic'] = mod_tic\n",
    "                    if str(row['sync_treasury_bond_avg_yield'])!='nan':\n",
    "                        GPF.loc[idx,'mod_tic_spread_treasury'] = mod_tic-row['sync_treasury_bond_avg_yield']\n",
    "                    if str(row['MMA_avg_yield'])!='nan':\n",
    "                        GPF.loc[idx,'mod_tic_spread_MMA'] = mod_tic-row['MMA_avg_yield']\n",
    "\n",
    "                if IF_has_AdvisorFee_timeFE and IF_has_CRFee_timeFE and IF_has_InsureFee_timeFE:\n",
    "                    # Gross spread is represented in the unit of every $1000\n",
    "                    purchase_price = reoffering_price-row['gross_spread']/1000*float(row['amount'])\n",
    "                    # Other fees are represented in the unit of basis points\n",
    "                    if if_advisor_coded:\n",
    "                        purchase_price = purchase_price-row['AdvisorFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "                    if has_rating:\n",
    "                        purchase_price = purchase_price-row['CRFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "                    if insured_ratio:\n",
    "                        purchase_price = purchase_price-row['InsureFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "\n",
    "                    # Calculate TIC implied by NIC\n",
    "                    mod_tic_timeFE = (1+npf.irr([-purchase_price]+\\\n",
    "                        [float(row['coupon_rate'])/2/100*float(row['amount'])]*(n_coupon-1)+[float(row['amount'])+float(row['coupon_rate'])/2/100*float(row['amount'])]))\\\n",
    "                        **2-1        \n",
    "                    GPF.loc[idx,'mod_tic_timeFE'] = mod_tic_timeFE\n",
    "                    if str(row['sync_treasury_bond_avg_yield'])!='nan':\n",
    "                        GPF.loc[idx,'mod_tic_spread_treasury_timeFE'] = mod_tic_timeFE-row['sync_treasury_bond_avg_yield']\n",
    "                    if str(row['MMA_avg_yield'])!='nan':\n",
    "                        GPF.loc[idx,'mod_tic_spread_MMA_timeFE'] = mod_tic_timeFE-row['MMA_avg_yield']\n",
    "    \n",
    "            # Case B: If multiple maturity\n",
    "            else:\n",
    "                # If number of tranches not consistent across fields, skip\n",
    "                if N_price_or_yield!=N_coupon_rate:\n",
    "                    continue\n",
    "                elif N_price_or_yield!=N_maturity_date:\n",
    "                    continue\n",
    "                else:\n",
    "                    reoffering_prices = []\n",
    "                    cash_flows = [] # Cash flow of the whole bond, not every $100 par value\n",
    "                    for tranch in range(0,row['coupon_rate'].count('\\n')+1):\n",
    "                        maturity = (datetime.strptime(row['maturity_date'].split('\\n')[tranch],\"%m/%d/%y\")\\\n",
    "                            -datetime.strptime(row['dated_date'],\"%Y-%m-%d %H:%M:%S\")).days\n",
    "                        coupon_rate = float(row['coupon_rate'].split('\\n')[tranch])\n",
    "                        price_or_yield = float(row['price_or_yield'].split('\\n')[tranch])\n",
    "                        n_coupon = round(maturity/182)\n",
    "                        amount = float(row['amount_by_maturity'].split('\\n')[tranch].replace(',',''))\n",
    "                        cash_flows = cash_flows+[[0]+[coupon_rate/2/100*amount]*(n_coupon-1)+[amount+coupon_rate/2/100*amount]]\n",
    "                        if price_or_yield<20:\n",
    "                            reoffering_yield = price_or_yield/100\n",
    "                            # Below is a value per every $100 par value, which needs to be converted to the value of the whole bond\n",
    "                            reoffering_price = npf.npv(sqrt(1+reoffering_yield)-1,[0]+[coupon_rate/2]*(n_coupon-1)+[100+coupon_rate/2])\n",
    "                            reoffering_price = reoffering_price/100*amount\n",
    "                            reoffering_prices = reoffering_prices+[reoffering_price]\n",
    "                        elif price_or_yield>80 and price_or_yield<120:\n",
    "                            reoffering_price = price_or_yield\n",
    "                            reoffering_price = reoffering_price/100*amount\n",
    "                            reoffering_prices = reoffering_prices+[reoffering_price]\n",
    "                        else:\n",
    "                            reoffering_prices = reoffering_prices+[None]\n",
    "    \n",
    "                # Aggregate across trenches and calculate the NIC implied underwriter discount and TIC\n",
    "                if None in reoffering_prices:\n",
    "                    continue\n",
    "                else:\n",
    "                    # Calculate purchase price\n",
    "                    if IF_has_AdvisorFee and IF_has_CRFee and IF_has_InsureFee:\n",
    "                        # Gross spread is represented in the unit of every $1000\n",
    "                        purchase_price = np.sum(reoffering_prices)-row['gross_spread']/1000*float(row['amount'])\n",
    "                        # Other fees are represented in the unit of basis points\n",
    "                        if if_advisor_coded:\n",
    "                            purchase_price = purchase_price-row['AdvisorFeeRatio_hat']/10000*float(row['amount'])\n",
    "                        if has_rating:\n",
    "                            purchase_price = purchase_price-row['CRFeeRatio_hat']/10000*float(row['amount'])\n",
    "                        if insured_ratio:\n",
    "                            purchase_price = purchase_price-row['InsureFeeRatio_hat']/10000*float(row['amount'])\n",
    "                        \n",
    "                        # Calculate TIC implied by NIC\n",
    "                        max_length = max(len(lst) for lst in cash_flows)\n",
    "                        padded_lists = [lst+[0]*(max_length-len(lst)) for lst in cash_flows]\n",
    "                        cash_flow = [sum(elements) for elements in zip(*padded_lists)]\n",
    "                        cash_flow[0] = cash_flow[0]-purchase_price\n",
    "                        mod_tic = (1+npf.irr(cash_flow))**2-1\n",
    "                        GPF.loc[idx,'mod_tic'] = mod_tic\n",
    "                        if str(row['sync_treasury_bond_avg_yield'])!='nan':\n",
    "                            GPF.loc[idx,'mod_tic_spread_treasury'] = mod_tic-row['sync_treasury_bond_avg_yield']\n",
    "                        if str(row['MMA_avg_yield'])!='nan':\n",
    "                            GPF.loc[idx,'mod_tic_spread_MMA'] = mod_tic-row['MMA_avg_yield']\n",
    "\n",
    "                    if IF_has_AdvisorFee_timeFE and IF_has_CRFee_timeFE and IF_has_InsureFee_timeFE:\n",
    "                        # Gross spread is represented in the unit of every $1000\n",
    "                        purchase_price = np.sum(reoffering_prices)-row['gross_spread']/1000*float(row['amount'])\n",
    "                        # Other fees are represented in the unit of basis points\n",
    "                        if if_advisor_coded:\n",
    "                            purchase_price = purchase_price-row['AdvisorFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "                        if has_rating:\n",
    "                            purchase_price = purchase_price-row['CRFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "                        if insured_ratio:\n",
    "                            purchase_price = purchase_price-row['InsureFeeRatio_hat_model_timeFE']/10000*float(row['amount'])\n",
    "                    \n",
    "                        # Calculate TIC implied by NIC\n",
    "                        max_length = max(len(lst) for lst in cash_flows)\n",
    "                        padded_lists = [lst+[0]*(max_length-len(lst)) for lst in cash_flows]\n",
    "                        cash_flow = [sum(elements) for elements in zip(*padded_lists)]\n",
    "                        cash_flow[0] = cash_flow[0]-purchase_price\n",
    "                        mod_tic_timeFE = (1+npf.irr(cash_flow))**2-1\n",
    "                        GPF.loc[idx,'mod_tic_timeFE'] = mod_tic_timeFE\n",
    "                        if str(row['sync_treasury_bond_avg_yield'])!='nan':\n",
    "                            GPF.loc[idx,'mod_tic_spread_treasury_timeFE'] = mod_tic_timeFE-row['sync_treasury_bond_avg_yield']\n",
    "                        if str(row['MMA_avg_yield'])!='nan':\n",
    "                            GPF.loc[idx,'mod_tic_spread_MMA_timeFE'] = mod_tic_timeFE-row['MMA_avg_yield']\n",
    "\n",
    "    return GPF\n",
    "\n",
    "GPF = GPF.copy()\n",
    "meta_columns = list(proc_list(GPF.sample(10)).columns)\n",
    "GPF_dd = dd.from_pandas(GPF, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    GPF = GPF_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d978f593-ab84-4421-9537-2bcca7b787fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize data. Handle missing values cases carefully\n",
    "\n",
    "vars_to_winsor = [\n",
    "    'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "    'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE'\n",
    "    ]\n",
    "\n",
    "for var in vars_to_winsor:\n",
    "    GPF[var] = pd.to_numeric(GPF[var],errors='coerce')\n",
    "    upper_limit = np.percentile(GPF[var][np.logical_not(np.isnan(GPF[var]))],99)\n",
    "    lower_limit = np.percentile(GPF[var][np.logical_not(np.isnan(GPF[var]))],1)\n",
    "    GPF.loc[(GPF[var]>upper_limit)&(np.logical_not(np.isnan(GPF[var]))),var] = upper_limit\n",
    "    GPF.loc[(GPF[var]<lower_limit)&(np.logical_not(np.isnan(GPF[var]))),var] = lower_limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c5586c-e7e8-4a5a-a94a-27ec6733a758",
   "metadata": {},
   "source": [
    "# 5. Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410da5d1-b669-4bce-a4d9-5267d454f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------#\n",
    "# Export a version of GPF with imputed other costs #\n",
    "#--------------------------------------------------#\n",
    "\n",
    "for year in list(GPF['sale_year'].unique()):\n",
    "    GPF['if_year_'+str(year)] = GPF['sale_year']==year\n",
    "column_years = ['if_year_'+str(year) for year in list(GPF['sale_year'].unique())]\n",
    "\n",
    "GPF = GPF.drop(columns=[\n",
    "    'is_security_type_GO', 'is_security_type_RV', 'is_Bid_C', 'is_Bid_N',\n",
    "    'is_taxable_code_A', 'is_taxable_code_E', 'is_taxable_code_T', 'scaler',\n",
    "    'amount_inf_adjusted', 'is_amount_Less_than_1M', 'is_amount_1M_to_5M',\n",
    "    'is_amount_5M_to_10M', 'is_amount_10M_to_50M', 'is_amount_50M_to_100M',\n",
    "    'is_amount_Greater_than_100M', 'maturity_in_years',\n",
    "    'is_maturity_Less_than_2y', 'is_maturity_2y_to_5y',\n",
    "    'is_maturity_5y_to_10y', 'is_maturity_10y_to_20y',\n",
    "    'is_maturity_20y_to_30y', 'is_maturity_30y_to_40y',\n",
    "    'is_maturity_Greater_than_40y', 'pop_to_avg', 'inc_to_avg',\n",
    "    ]+column_years)\n",
    "GPF.to_csv(\"../CleanData/SDC/0A_GPF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81679180-da3b-49ba-8de3-77d3182bbc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b883ac3-be5c-4478-827b-d947163e9146",
   "metadata": {},
   "source": [
    "# 6. Export a Version of GPF for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7024732-049c-47ea-a252-4c68e10cd5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3620c5f9-a3f6-44a3-a74b-9819186db88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust inflation\n",
    "FPCPITOTLZGUSA = pd.read_csv(\"../RawData/StLouisFed/FPCPITOTLZGUSA.csv\")\n",
    "FPCPITOTLZGUSA['year'] = FPCPITOTLZGUSA['DATE'].str[:4].astype(int)\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA.sort_values('year',ascending=False).reset_index(drop=True)\n",
    "scaler = 1\n",
    "FPCPITOTLZGUSA['scaler'] = None\n",
    "for idx,row in FPCPITOTLZGUSA.iterrows():\n",
    "    if idx==0:\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = 1\n",
    "    else:\n",
    "        scaler = scaler*(FPCPITOTLZGUSA.at[idx-1,'FPCPITOTLZGUSA']/100+1)\n",
    "        FPCPITOTLZGUSA.at[idx,'scaler'] = scaler\n",
    "FPCPITOTLZGUSA = FPCPITOTLZGUSA[['scaler','year']]\n",
    "FPCPITOTLZGUSA = pd.concat([FPCPITOTLZGUSA,pd.DataFrame([{'scaler':1/(1+3.2/100),'year':2023}])])\n",
    "\n",
    "HHI_byCSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac179a-60c8-41f1-902c-8951abd5b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdc9c9f-835b-4128-b0f1-e143e6cfbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPF_OLS = GPF[['CSA Code','CBSA Code','sale_year',\n",
    "    'gross_spread','avg_yield',\n",
    "    'treasury_avg_spread','MMA_avg_spread',\n",
    "    'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "    'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "    'underpricing_15to30','underpricing_15to60',\n",
    "    'Issuer','issuer_type',\n",
    "    'if_advisor','if_dual_advisor','Bid','taxable_code','security_type','amount','avg_maturity',\n",
    "    'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "    'use_of_proceeds_general','if_callable','CB_Eligible',\n",
    "    'TBB_n_bidders',\n",
    "    'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "    'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "    'State','County']]\n",
    "GPF_OLS = GPF_OLS.rename(columns={'sale_year':'calendar_year'})\n",
    "GPF_OLS = GPF_OLS.merge(FPCPITOTLZGUSA,left_on='calendar_year',right_on='year')\n",
    "GPF_OLS = GPF_OLS.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "GPF_OLS.to_csv('../CleanData/SDC/1A_GPF_OLS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7904adb-4d3d-4041-89e4-93a01e8a07db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efde0223-071b-45d8-98bc-ac6aefa6f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number: Total amount of issuance in 2022\n",
    "amount = np.sum(GPF[GPF['sale_year']==2022]['amount'])/1000\n",
    "amount = \"{:.0f}\".format(amount)\n",
    "with open('../Draft/nums/Amount_MunicipalBond.tex','w') as file:\n",
    "    file.write(str(amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455dbbf-2926-4994-ba6e-dfa887b2f876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4db7bf-b8ab-4bf4-ac1e-8e3b25a70176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
