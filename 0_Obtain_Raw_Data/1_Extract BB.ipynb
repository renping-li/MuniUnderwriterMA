{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a968f0-4258-4208-bc3a-93b4e2c8d0f1",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- There are several ways costs are reported in TBB:\n",
    "    - \"Purchased S5,436,000.00 by Commerce Cap Mkts, at 3.5\\%, plus S52,354.12, effective rate 2.5369%.\"  \n",
    "    Coupon rate is 3.5\\%, and there is a premium (purchase price minus par value) of S52,354.12, which renders NIC to be 2.5369%.  \n",
    "    Used for short-term notes.\n",
    "    - \"Winning bid: Stifel Nicolaus, at 100.1031, TIC 4.253%.\"  \n",
    "    Bidding is done by each bidding a TIC.\n",
    "    - \"Winning bid: Roosevelt & Cross, at n/a, NIC 4.6755%.\"  \n",
    "    Bidding is done by each bidding a NIC.\n",
    "- Reoffering yield is often missing in TBB data (\"NRO\"), which is because these have been fully subscribed and not offered to the public (https://www.bondbuyer.com/news/msrb-limit-use-of-nro). They will, however, later be reported after first day of trading. Likely, I will not use them but rather use price/yield in SDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9715c66-3cfb-4859-824c-b9557ccaa998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "all_states = [\n",
    "    \"ALABAMA\",\"ALASKA\",\"ARIZONA\",\"ARKANSAS\",\"CALIFORNIA\",\"COLORADO\",\"CONNECTICUT\",\"DELAWARE\",\n",
    "    \"FLORIDA\",\"GEORGIA\",\"HAWAII\",\"IDAHO\",\"ILLINOIS\",\"INDIANA\",\"IOWA\",\"KANSAS\",\"KENTUCKY\",\n",
    "    \"LOUISIANA\",\"MAINE\",\"MARYLAND\",\"MASSACHUSETTS\",\"MICHIGAN\",\"MINNESOTA\",\"MISSISSIPPI\",\n",
    "    \"MISSOURI\",\"MONTANA\",\"NEBRASKA\",\"NEVADA\",\"NEW HAMPSHIRE\",\"NEW JERSEY\",\"NEW MEXICO\",\n",
    "    \"NEW YORK\",\"NORTH CAROLINA\",\"NORTH DAKOTA\",\"OHIO\",\"OKLAHOMA\",\"OREGON\",\"PENNSYLVANIA\",\n",
    "    \"RHODE ISLAND\",\"SOUTH CAROLINA\",\"SOUTH DAKOTA\",\"TENNESSEE\",\"TEXAS\",\"UTAH\",\"VERMONT\",\n",
    "    \"VIRGINIA\",\"WASHINGTON\",\"WEST VIRGINIA\",\"WISCONSIN\",\"WYOMING\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd9df3-399d-4201-bb2b-fcd6402394c6",
   "metadata": {},
   "source": [
    "# 1. Download and parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0444186-684a-4ea1-8e7c-d6091f61f25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Obtain web address from the index files\n",
    "index_files = os.listdir('../../RawData/BondBuyer/IndexFiles')\n",
    "index_files = [item for item in index_files if item[-4:]=='html']\n",
    "\n",
    "# Process (download, parse, and save) year by year\n",
    "years = list(set([item[:4] for item in index_files]))\n",
    "\n",
    "#-----------------#\n",
    "# Initiate driver #\n",
    "#-----------------#\n",
    "\n",
    "try:\n",
    "    driver.close() \n",
    "except:\n",
    "    pass\n",
    "driver_path = \"../../RawData/BondBuyer/chromedriver.exe\"\n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.set_page_load_timeout(60)\n",
    "try:\n",
    "    driver.get('https://www.bondbuyer.com/')\n",
    "except:\n",
    "    pass\n",
    "time.sleep(60) # Manually log in here\n",
    "\n",
    "# Process year by year\n",
    "for year in years:\n",
    "\n",
    "    #---------------------#\n",
    "    # Extract web address #\n",
    "    #---------------------#\n",
    "\n",
    "    index_files_oneyear = [item for item in index_files if item[:4]==year]\n",
    "    \n",
    "    web_addresses = []\n",
    "    for index_file in index_files_oneyear:\n",
    "\n",
    "        with open('../../RawData/BondBuyer/IndexFiles/'+index_file, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        a_tags = soup.find_all('a')\n",
    "        pattern = re.compile(r'href=\"([^\"]+)\" title=')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            a_tag_str = str(a_tag)\n",
    "            match = pattern.search(a_tag_str)\n",
    "            if match:\n",
    "                href_content = match.group(1)\n",
    "                web_addresses = web_addresses+[href_content]\n",
    "\n",
    "    #---------------#\n",
    "    # Download data #\n",
    "    #---------------#\n",
    "\n",
    "    # Initiate driver\n",
    "\n",
    "    BBData = []\n",
    "\n",
    "    for web_address in web_addresses:\n",
    "        driver.get(web_address)\n",
    "        time.sleep(3)\n",
    "        BBData = BBData+[{'source':web_address,'text':driver.page_source}]\n",
    "\n",
    "    with open('../../RawData/BondBuyer/WebPages/BBData'+year+'.pkl', 'wb') as file:\n",
    "        pickle.dump(BBData, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1285ca-7fcc-4a6a-b121-54ecf0314605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "def proc_one_issue(lines):\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------#\n",
    "    # Find the line with lowest bid, case by case. Handle case where multiple winnders (with potentially #\n",
    "    # different prices)                                                                                  #\n",
    "    #----------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "    #--------#\n",
    "    # Case 1 #\n",
    "    #--------#\n",
    "    \n",
    "    # Example: \"Purchased $1,782,470.00 by Janney Montgomery, at 4.5%, plus n/a, effective rate n/a.\"\n",
    "    \n",
    "    CaseEffRate_amounts = None\n",
    "    CaseEffRate_purchasers = None\n",
    "    CaseEffRate_coupon_rates = None\n",
    "    CaseEffRate_purchase_price_minus_pars = None\n",
    "    CaseEffRate_effective_rates = None\n",
    "    CaseEffRate_lines_other_bidders = None\n",
    "    \n",
    "    lines_with_purchase_price = [item for item in lines if 'Purchased ' in item]\n",
    "    \n",
    "    if len(lines_with_purchase_price)>0:\n",
    "        CaseEffRate_amounts = []\n",
    "        CaseEffRate_purchasers = []\n",
    "        CaseEffRate_coupon_rates = []\n",
    "        CaseEffRate_purchase_price_minus_pars = []\n",
    "        CaseEffRate_effective_rates = []\n",
    "        for line in lines_with_purchase_price:\n",
    "    \n",
    "            match = re.search(r'Purchased (.*?) by',line)\n",
    "            amount = None\n",
    "            if match:\n",
    "                amount = match.group(1)\n",
    "                amount = amount.replace('.00','').strip()\n",
    "                amount = amount.replace('$','').strip()\n",
    "                amount = amount.replace(',','').strip()\n",
    "            CaseEffRate_amounts = CaseEffRate_amounts+[amount]\n",
    "    \n",
    "            match = re.search(r'by (.*?), at',line)\n",
    "            purchaser = None\n",
    "            if match:\n",
    "                purchaser = match.group(1)\n",
    "            CaseEffRate_purchasers = CaseEffRate_purchasers+[purchaser]\n",
    "    \n",
    "            match = re.search(r'by (.*?) at (.*?),',line)\n",
    "            coupon_rate = None\n",
    "            if match:\n",
    "                coupon_rate = match.group(2)\n",
    "            CaseEffRate_coupon_rates = CaseEffRate_coupon_rates+[coupon_rate]\n",
    "    \n",
    "            match = re.search(r'plus (.*?), effective rate',line)\n",
    "            purchase_price_minus_par = None\n",
    "            if match:\n",
    "                purchase_price_minus_par = match.group(1)\n",
    "            CaseEffRate_purchase_price_minus_pars = CaseEffRate_purchase_price_minus_pars+[purchase_price_minus_par]\n",
    "    \n",
    "            match = re.search(r'effective rate (\\d+\\.\\d+%)',line)\n",
    "            effective_rate = None\n",
    "            if match:\n",
    "                effective_rate = match.group(1)\n",
    "            CaseEffRate_effective_rates = CaseEffRate_effective_rates+[effective_rate]\n",
    "    \n",
    "        # Find information on other bidders\n",
    "        for index in range(0,len(lines)):\n",
    "            if 'Other bidders' in lines[index]:\n",
    "                CaseEffRate_lines_other_bidders = lines[index+1:]\n",
    "                CaseEffRate_lines_other_bidders = [line for line in CaseEffRate_lines_other_bidders if line!='']\n",
    "                CaseEffRate_lines_other_bidders = [line for line in CaseEffRate_lines_other_bidders if len(line)<=100]\n",
    "                CaseEffRate_lines_other_bidders = [line for line in CaseEffRate_lines_other_bidders if 'Effective Rate' in line]\n",
    "\n",
    "    #--------#\n",
    "    # Case 2 #\n",
    "    #--------#\n",
    "    \n",
    "    # Example: \"Winning bid: BMO Capital Markets, at 99.0000, TIC 6.4338%.\"\n",
    "    \n",
    "    CaseTIC_purchaser = None\n",
    "    CaseTIC_purchase_price = None\n",
    "    CaseTIC_tic = None\n",
    "    CaseTIC_lines_other_bidders = None\n",
    "    \n",
    "    lines_with_purchase_price = [item for item in lines if ('Winning bid:' in item) and ('TIC' in item)]\n",
    "    \n",
    "    if len(lines_with_purchase_price)==1:\n",
    "    \n",
    "        line = lines_with_purchase_price[0]\n",
    "    \n",
    "        match = re.search(r'Winning bid: (.*?), at',line)\n",
    "        if match:\n",
    "            CaseTIC_purchaser = match.group(1)\n",
    "    \n",
    "        match = re.search(r'at (.*?), TIC',line)\n",
    "        if match:\n",
    "            CaseTIC_purchase_price = match.group(1)\n",
    "    \n",
    "        match = re.search(r'TIC (\\d+\\.\\d+%)',line)\n",
    "        if match:\n",
    "            CaseTIC_tic = match.group(1)\n",
    "    \n",
    "        # Find information on other bidders\n",
    "        CaseTIC_lines_other_bidders = None\n",
    "        for index in range(0,len(lines)):\n",
    "            if 'Other bidders' in lines[index]:\n",
    "                CaseTIC_lines_other_bidders = lines[index+1:]\n",
    "                CaseTIC_lines_other_bidders = [line for line in CaseTIC_lines_other_bidders if line!='']\n",
    "                CaseTIC_lines_other_bidders = [line for line in CaseTIC_lines_other_bidders if len(line)<=100]\n",
    "                CaseTIC_lines_other_bidders = [line for line in CaseTIC_lines_other_bidders if 'TIC' in line]\n",
    "\n",
    "    #--------#\n",
    "    # Case 3 #\n",
    "    #--------#\n",
    "    \n",
    "    # Example: \"Winning bid: BMO Capital Markets, at n/a, NIC 5.8176%.\"\n",
    "    \n",
    "    CaseNIC_purchaser = None\n",
    "    CaseNIC_purchase_price = None\n",
    "    CaseNIC_nic = None\n",
    "    CaseNIC_lines_other_bidders = None\n",
    "    \n",
    "    lines_with_purchase_price = [item for item in lines if ('Winning bid:' in item) and ('NIC' in item)]\n",
    "    \n",
    "    if len(lines_with_purchase_price)==1:\n",
    "    \n",
    "        line = lines_with_purchase_price[0]\n",
    "    \n",
    "        match = re.search(r'Winning bid: (.*?), at',line)\n",
    "        if match:\n",
    "            CaseNIC_purchaser = match.group(1)\n",
    "    \n",
    "        match = re.search(r'at (.*?), NIC',line)\n",
    "        if match:\n",
    "            CaseNIC_purchase_price = match.group(1)\n",
    "    \n",
    "        match = re.search(r'NIC (\\d+\\.\\d+%)',line)\n",
    "        if match:\n",
    "            CaseNIC_nic = match.group(1)\n",
    "    \n",
    "        # Find information on other bidders\n",
    "        lines_other_bidders = None\n",
    "        for index in range(0,len(lines)):\n",
    "            if 'Other bidders' in lines[index]:\n",
    "                CaseNIC_lines_other_bidders = lines[index+1:]\n",
    "                CaseNIC_lines_other_bidders = [line for line in CaseNIC_lines_other_bidders if line!='']\n",
    "                # Exclude lines that are not other bidders\n",
    "                CaseNIC_lines_other_bidders = [line for line in CaseNIC_lines_other_bidders if len(line)<=100]\n",
    "                CaseNIC_lines_other_bidders = [line for line in CaseNIC_lines_other_bidders if 'NIC' in line]\n",
    "    \n",
    "    OneIssueData = {\n",
    "    'CaseEffRate_amounts':CaseEffRate_amounts,\n",
    "    'CaseEffRate_purchasers':CaseEffRate_purchasers,\n",
    "    'CaseEffRate_coupon_rates':CaseEffRate_coupon_rates,\n",
    "    'CaseEffRate_purchase_price_minus_pars':CaseEffRate_purchase_price_minus_pars,\n",
    "    'CaseEffRate_effective_rates':CaseEffRate_effective_rates,\n",
    "    'CaseEffRate_lines_other_bidders':CaseEffRate_lines_other_bidders,\n",
    "    'CaseTIC_purchaser':CaseTIC_purchaser,\n",
    "    'CaseTIC_purchase_price':CaseTIC_purchase_price,\n",
    "    'CaseTIC_TIC':CaseTIC_tic,\n",
    "    'CaseTIC_lines_other_bidders':CaseTIC_lines_other_bidders,\n",
    "    'CaseNIC_purchaser':CaseNIC_purchaser,\n",
    "    'CaseNIC_purchase_price':CaseNIC_purchase_price,\n",
    "    'CaseNIC_NIC':CaseNIC_nic,\n",
    "    'CaseNIC_lines_other_bidders':CaseNIC_lines_other_bidders,\n",
    "    }\n",
    "\n",
    "    return OneIssueData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c5dd7-14e2-44ee-aea3-71bc95796b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc1fa5-4666-4ee7-836f-22b16afb132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "#########################\n",
    "# Process and save data #\n",
    "#########################\n",
    "\n",
    "# How the page is organized, i.e., how different issues are separated. There are three cases:\n",
    "# (a) No special formatting (https://data.bondbuyer.com/salesresults/GetDetails/2000)\n",
    "# (b) State and issuers are in \"h3\" (https://data.bondbuyer.com/salesresults/GetDetails/1000)\n",
    "# (c) State is in bold, while issuer is in italics (https://data.bondbuyer.com/salesresults/GetDetails/10000)\n",
    "\n",
    "# Parsing is based on the line with amount and date, hence not affected by how the page is organized.\n",
    "# There are two cases:\n",
    "# (1) \"Dec 27, 2012 . . . . . . $3,040,324\"\n",
    "# (2) \"1-Mar-22  $171,345,000\"\n",
    "\n",
    "for year in range(2008,2025):\n",
    "\n",
    "    with open('../../RawData/BondBuyer/WebPages/BBData'+str(year)+'.pkl', 'rb') as file:\n",
    "        BBData = pickle.load(file)\n",
    "    \n",
    "    BBIssueData = []\n",
    "    \n",
    "    for webpage in BBData:\n",
    "\n",
    "        # Skip one date with formatting error, for which data will be manually coded\n",
    "        if webpage['source']=='https://data.bondbuyer.com/salesresults/GetDetails/9589':\n",
    "            continue\n",
    "\n",
    "        webpage['text'] = webpage['text'].replace('<br><br>','<br>')\n",
    "\n",
    "        soup = BeautifulSoup(webpage['text'], 'html.parser')\n",
    "    \n",
    "        headline_paragraph = soup.find('p', class_='Headlinecls').text\n",
    "    \n",
    "        notice_month = headline_paragraph.split('.')[0]\n",
    "        notice_month = int(notice_month)\n",
    "        notice_day = headline_paragraph.split('.')[1]\n",
    "        notice_day = int(notice_day)\n",
    "        notice_year = headline_paragraph.split('.')[2].split(':')[0]\n",
    "        notice_year = int(notice_year)+2000\n",
    "    \n",
    "        # Extract data only for competitive sales\n",
    "        if \"competitive sales\" not in headline_paragraph:\n",
    "            continue\n",
    "    \n",
    "        #----------------------------------------------------------------------------------------#\n",
    "        # Parse documents into segments of each bond issue, with corresponding state information #\n",
    "        #----------------------------------------------------------------------------------------#\n",
    "    \n",
    "        BondIssues = []\n",
    "        matched_substrings = []\n",
    "    \n",
    "        #--------#\n",
    "        # Case 1 #\n",
    "        #--------#\n",
    "    \n",
    "        # Find lines with amount and date (unique for each issue)\n",
    "        pattern = r\"<b>(.{1,99})\\. \\. \\. \\. \\. \\. (.*?)</b>\"\n",
    "        matches = list(re.finditer(pattern,webpage['text']))\n",
    "        if len(matches)>0:\n",
    "            matched_substrings = [(match.group(), match.start(), match.end()) for match in matches]\n",
    "    \n",
    "        #--------#\n",
    "        # Case 2 #\n",
    "        #--------#\n",
    "    \n",
    "        # Find lines with amount and date (unique for each issue)\n",
    "        pattern = r'\\b\\d{1,2}-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}\\b'\n",
    "        matches = list(re.finditer(pattern,webpage['text']))\n",
    "        if len(matches)>0:\n",
    "            matched_substrings = [(match.group(), match.start(), match.end()) for match in matches]\n",
    "    \n",
    "        if len(matched_substrings)>0:\n",
    "    \n",
    "            # Process issues one by one\n",
    "            for i in range(0,len(matched_substrings)):\n",
    "                issue = matched_substrings[i]\n",
    "                # Extract issuer, for either format\n",
    "                if '<h3>' in webpage['text']:\n",
    "                    # The immediate line before that is the issuer, which is extracted with reverse search\n",
    "                    pattern = r\">3h/<(.*?)>3h<\"\n",
    "                    matches = list(re.finditer(pattern,webpage['text'][:issue[1]][::-1]))\n",
    "                    issuer = str(matches[0].group())[::-1]\n",
    "                else:\n",
    "                    # The immediate line before that is the issuer, which is extracted with reverse search\n",
    "                    pattern = r\">rb<(.*?)>rb<\"\n",
    "                    matches = list(re.finditer(pattern,webpage['text'][:issue[1]][::-1]))\n",
    "                    issuer = str(matches[0].group())[::-1]\n",
    "                # The immediate state name before that is the state\n",
    "                occurrences = []\n",
    "                for state in all_states:\n",
    "                    for match in re.finditer(re.escape(state), webpage['text'][:issue[1]]):\n",
    "                        occurrences.append((match.group(), match.start(), match.end()))\n",
    "                sorted_occurrences = sorted(occurrences, key=lambda x: x[1])\n",
    "                state = sorted_occurrences[-1][0]\n",
    "                # Tuples of state, issuer, and issue information\n",
    "                # Note that issue information can contain the next issuer or state, which is totally okay given how it is parsed\n",
    "                if i<len(matched_substrings)-1:\n",
    "                    issue_text = webpage['text'][matched_substrings[i][1]:matched_substrings[i+1][1]]\n",
    "                else:\n",
    "                    issue_text = webpage['text'][matched_substrings[i][1]:]\n",
    "                BondIssues = BondIssues+[[state,issuer,issue_text]]\n",
    "    \n",
    "    \n",
    "    \n",
    "        #------------------------------------------#\n",
    "        # Go over segments and extract information #\n",
    "        #------------------------------------------#\n",
    "    \n",
    "        for OneBondIssue in BondIssues:\n",
    "    \n",
    "            state = OneBondIssue[0]\n",
    "            issuer = BeautifulSoup(OneBondIssue[1],'html.parser').get_text()\n",
    "            paragraph = OneBondIssue[2]\n",
    "            paragraph = paragraph.replace('<br/>','<br>')\n",
    "            lines = paragraph.split('<br>')\n",
    "    \n",
    "            #-----------------------------------------------#\n",
    "            # Find the line with sale date and total amount #\n",
    "            #-----------------------------------------------#\n",
    "    \n",
    "            # First, determine how the date is formated\n",
    "            # Format 1: \"1-Mar-22  $171,345,000\"\n",
    "            date_pattern = r'\\b\\d{1,2}-(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{2}\\b'\n",
    "            line_with_sale_date = [line for line in lines if re.search(date_pattern, line)]\n",
    "            if_format1 = False\n",
    "            if len(line_with_sale_date)==1:\n",
    "                if_format1 = True\n",
    "            # Format 2: \"Nov 19, 2013 . . . . . . $220,000\"\n",
    "            line_with_sale_date = [item for item in lines if '. . . . . .' in item]\n",
    "            if_format2 = False\n",
    "            if len(line_with_sale_date)==1:\n",
    "                if_format2 = True\n",
    "    \n",
    "            if if_format1:\n",
    "                line_with_sale_date = [line for line in lines if re.search(date_pattern, line)]\n",
    "                if '&nbsp;&nbsp;' in line_with_sale_date[0]:\n",
    "                    line_with_sale_date = line_with_sale_date[0].split('&nbsp;&nbsp;')\n",
    "                    sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                    amount = line_with_sale_date[1].replace('</b>','').strip()\n",
    "                    amount = amount.replace('.00','').strip()\n",
    "                    amount = amount.replace('$','').strip()\n",
    "                    amount = amount.replace(',','').strip()\n",
    "                elif '&nbsp;' in line_with_sale_date[0]:\n",
    "                    line_with_sale_date = line_with_sale_date[0].split('&nbsp;')\n",
    "                    sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                    amount = line_with_sale_date[1].replace('</b>','').strip()\n",
    "                    amount = amount.replace('.00','').strip()\n",
    "                    amount = amount.replace('$','').strip()\n",
    "                    amount = amount.replace(',','').strip()\n",
    "                elif '\\t' in line_with_sale_date[0]:\n",
    "                    line_with_sale_date = line_with_sale_date[0].split('\\t')\n",
    "                    sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                    amount = line_with_sale_date[1].replace('</b>','').strip()\n",
    "                    amount = amount.replace('.00','').strip()\n",
    "                    amount = amount.replace('$','').strip()\n",
    "                    amount = amount.replace(',','').strip()\n",
    "                elif ' ' in line_with_sale_date[0]:\n",
    "                    line_with_sale_date = line_with_sale_date[0].split(' ')\n",
    "                    sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                    amount = line_with_sale_date[1].replace('</b>','').strip()\n",
    "                    amount = amount.replace('.00','').strip()\n",
    "                    amount = amount.replace('$','').strip()\n",
    "                    amount = amount.replace(',','').strip()\n",
    "                else:\n",
    "                    sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                    amount = None\n",
    "            if if_format2:\n",
    "                line_with_sale_date = [item for item in lines if '. . . . . .' in item]\n",
    "                line_with_sale_date = line_with_sale_date[0].split('. . . . . .')\n",
    "                sale_date = line_with_sale_date[0].replace('<b>','').strip()\n",
    "                amount = line_with_sale_date[1].replace('</b>','').strip()\n",
    "                amount = amount.replace('.00','').strip()\n",
    "                amount = amount.replace('$','').strip()\n",
    "                amount = amount.replace(',','').strip()\n",
    "    \n",
    "            #-------------------------------#\n",
    "            # Find the line with dated date #\n",
    "            #-------------------------------#\n",
    "    \n",
    "            line_with_dated_date = [item for item in lines if 'Dated ' in item]\n",
    "            dated_date = line_with_dated_date[0].replace('Dated ','').strip()\n",
    "            dated_date = dated_date.replace('.','').strip()\n",
    "    \n",
    "            OneIssueData = proc_one_issue(lines)\n",
    "            OneIssueData['source'] = webpage['source']\n",
    "            OneIssueData['issuer'] = issuer\n",
    "            OneIssueData['state'] = state\n",
    "            OneIssueData['amount'] = amount\n",
    "            OneIssueData['sale_date'] = sale_date\n",
    "            OneIssueData['notice_month'] = notice_month\n",
    "            OneIssueData['notice_day'] = notice_day\n",
    "            OneIssueData['notice_year'] = notice_year\n",
    "            OneIssueData['dated_date'] = dated_date\n",
    "            BBIssueData = BBIssueData+[OneIssueData]\n",
    "\n",
    "        with open('../../RawData/BondBuyer/WebPages/BBIssueData'+str(year)+'.pkl', 'wb') as file:\n",
    "            pickle.dump(BBIssueData, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007110aa-046f-4715-b8a4-e43f27fe1b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bcdfb2-16fa-4ebb-a939-4b0b5dba9029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
