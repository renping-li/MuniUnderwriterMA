{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2023a33d-359f-4520-9bab-54e82b735f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "try:\n",
    "    del(FUN_proc_name)\n",
    "except:\n",
    "    pass\n",
    "import FUN_proc_name\n",
    "importlib.reload(FUN_proc_name)\n",
    "from FUN_proc_name import FUN_proc_name\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c3c95-8406-4fe8-8f75-365c796cf07e",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4994ea8-c5df-4d65-a432-b45a9d30e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOD = pd.read_csv('../RawData/FDIC/SOD.csv')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].str.replace(',','')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].astype(int)\n",
    "SNL_in_SOD = pd.read_csv('../RawData/FDIC/SNL_in_SOD.csv')\n",
    "SNL_in_SOD['year'] = SNL_in_SOD['Completion Date'].str[:4].astype(int)\n",
    "SNL_in_SOD = SNL_in_SOD[SNL_in_SOD['Target']!=SNL_in_SOD['Buyer']]\n",
    "SNL_in_SOD = SNL_in_SOD[['Target','Buyer','year']]\n",
    "CSA_affected = pd.read_parquet('../CleanData/MAEvent/CSA_affected.parquet')\n",
    "CBSA_affected = pd.read_parquet('../CleanData/MAEvent/CBSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb87de2-0484-4ee7-a915-1281ea0a2eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc105c5-11f4-410f-8519-868294f874b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e63b1b-ac40-497c-8d37-4c6ad9a22ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d81a3fe-7933-4121-b2d4-af6cc3dec5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 2.56 s, total: 12.6 s\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "##################################\n",
    "# Summary stats on number of M&A #\n",
    "##################################\n",
    "\n",
    "SNL_in_SOD_withchars = SNL_in_SOD.copy()\n",
    "\n",
    "SNL_in_SOD_withchars['both_active'] = False\n",
    "SNL_in_SOD_withchars['both_active_overlap_CSA'] = False\n",
    "\n",
    "# M&As where both underwrite municipal bonds right before merger\n",
    "def proc_list(SNL_in_SOD_withchars):\n",
    "    for idx,row in SNL_in_SOD_withchars.iterrows():\n",
    "        SOD_oneyear = SOD[SOD['year']==row['year']-1]\n",
    "        names = list(chain.from_iterable(list(np.array(SOD_oneyear[['name']]))))\n",
    "        names = list(set(names))\n",
    "        if (row['Target'] in names) and \\\n",
    "            (row['Buyer'] in names):\n",
    "            SNL_in_SOD_withchars.at[idx,'both_active'] = True\n",
    "    return SNL_in_SOD_withchars\n",
    "\n",
    "output_columns = proc_list(SNL_in_SOD_withchars[:3]).columns # Process one year to get columns\n",
    "SNL_in_SOD_withchars_dd = dd.from_pandas(SNL_in_SOD_withchars, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    SNL_in_SOD_withchars = SNL_in_SOD_withchars_dd.map_partitions(proc_list, \\\n",
    "        meta=pd.DataFrame(columns=output_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa07c4a-47d1-4da1-83d2-aff41ce09466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M&As where both underwirte municipal bonds before merger and have market overlap in terms of CSA\n",
    "def proc_list(SNL_in_SOD_withchars):\n",
    "    for idx,row in SNL_in_SOD_withchars.iterrows():\n",
    "        SOD_oneyear = SOD[SOD['year']==row['year']-1]\n",
    "        CSAs = list(SOD_oneyear['CSA Code'].unique())\n",
    "        CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "        for CSA in CSAs:\n",
    "            SOD_oneyearCSA = SOD_oneyear[SOD_oneyear['CSA Code']==CSA]\n",
    "            names = list(chain.from_iterable(list(np.array(SOD_oneyearCSA[['name']]))))\n",
    "            names = list(set(names))\n",
    "            # If for any CSA there is overlap, then there is overlap\n",
    "            if (row['Target'] in names) and \\\n",
    "                (row['Buyer'] in names):\n",
    "                SNL_in_SOD_withchars.at[idx,'both_active_overlap_CSA'] = True\n",
    "    return SNL_in_SOD_withchars\n",
    "\n",
    "output_columns = proc_list(SNL_in_SOD_withchars[:3]).columns # Process one year to get columns\n",
    "SNL_in_SOD_withchars_dd = dd.from_pandas(SNL_in_SOD_withchars, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    SNL_in_SOD_withchars = SNL_in_SOD_withchars_dd.map_partitions(proc_list, \\\n",
    "        meta=pd.DataFrame(columns=output_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39586b72-46db-40e7-ab6e-dab0cd9c040e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f297dd7d-301b-42c7-abd2-d687c2301863",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CBMA_both_active = np.sum(SNL_in_SOD_withchars['both_active']==True)\n",
    "n_CBMA_both_active_overlap_CSA = np.sum(SNL_in_SOD_withchars['both_active_overlap_CSA']==True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a82b12-7800-4b40-9f2e-6102f908c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number: Number of M&As where both sides are active #\n",
    "n_CBMA_both_active = '{:,}'.format(n_CBMA_both_active)\n",
    "with open('../Slides/nums/n_CBMA_both_active.tex','w') as file:\n",
    "    file.write(str(n_CBMA_both_active))\n",
    "\n",
    "# Number: Number of M&As where both sides are active and have geographic overlap #\n",
    "n_CBMA_both_active_overlap_CSA = '{:,}'.format(n_CBMA_both_active_overlap_CSA)\n",
    "with open('../Slides/nums/n_CBMA_both_active_overlap_CSA.tex','w') as file:\n",
    "    file.write(str(n_CBMA_both_active_overlap_CSA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a1f46-6b8b-4083-b959-47680a90113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i SCRIPT_us_states.py\n",
    "%run -i SCRIPT_import_GPF_CBSA.py\n",
    "\n",
    "# Merge GPF with ultimate parent of underwriters\n",
    "\n",
    "# Put ultimate parent names into \"GPF\"\n",
    "GPF = GPF.drop(columns=['_merge'],errors='ignore')\n",
    "GPF_names = pd.read_parquet('../CleanData/SDC/GPF_names.parquet')\n",
    "GPF_names = GPF_names[['name_GPF','parent_name','sale_year']]\n",
    "GPF_names = GPF_names.drop_duplicates(['name_GPF','sale_year'])\n",
    "\n",
    "# Add a column of cleaned underwriter names\n",
    "column_ind = 0\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "for column in raw_name_GPF_colnames:\n",
    "    # Obtain the parent name\n",
    "    # Note that this merge should be perfect. Check if it indeed is\n",
    "    GPF = GPF.merge(GPF_names.rename(columns={'name_GPF':column[4:]}),on=[column[4:],'sale_year'],how='outer',indicator=True)\n",
    "    GPF = GPF[(GPF['_merge']=='both')|(GPF['_merge']=='left_only')]\n",
    "    GPF = GPF.drop(columns=['_merge'])\n",
    "    GPF = GPF.rename(columns={'parent_name':'parent_name_'+str(column_ind)})\n",
    "    column_ind = column_ind+1\n",
    "\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71a84b-7158-4c12-b1c2-c4751d1fa207",
   "metadata": {},
   "source": [
    "# 2. Identify CB Merger Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6524962-9818-48db-b48a-a15a24150107",
   "metadata": {},
   "source": [
    "**Find CB mergers that shift HHI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27fd174a-a8e0-4b25-b179-c756e4fcceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "# Version 1: CSA, Delta HHI > 20 #\n",
    "#--------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.002:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI20 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#--------------------------------#\n",
    "# Version 2: CSA, Delta HHI > 50 #\n",
    "#--------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.005:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#---------------------------------#\n",
    "# Version 3: CSA, Delta HHI > 100 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.01:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100 = pd.DataFrame(CB_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "327602d5-5438-47e8-a560-6f5f71796928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number: Number of within-market CB M&As #\n",
    "n_CB_CSA_episodes_DeltaHHI100 = '{:,}'.format(len(CB_CSA_episodes_DeltaHHI100))\n",
    "with open('../Slides/nums/n_CB_CSA_episodes_DeltaHHI100.tex','w') as file:\n",
    "    file.write(str(n_CB_CSA_episodes_DeltaHHI100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280db33-55b0-4e5e-ad70-0dac13b07765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438ee27-db9b-4a18-83ca-afc1f1d5fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------#\n",
    "# Version 1: CBSA, Delta HHI > 20 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.002:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI20 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#---------------------------------#\n",
    "# Version 2: CBSA, Delta HHI > 50 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.005:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#----------------------------------#\n",
    "# Version 3: CBSA, Delta HHI > 100 #\n",
    "#----------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.01:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100 = pd.DataFrame(CB_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232468dd-9e06-4a93-87e9-ff85aeca6afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad76acd3-52b6-40eb-af87-72dcb8127b34",
   "metadata": {},
   "source": [
    "**Apply the restriction criteria: No significant IB mergers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409d882a-b3f2-4525-8e35-6bf5b891c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CSA_episodes_DeltaHHI20['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI20.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI20.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI20 = CB_CSA_episodes_DeltaHHI20[~CB_CSA_episodes_DeltaHHI20['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI50.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI50.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50 = CB_CSA_episodes_DeltaHHI50[~CB_CSA_episodes_DeltaHHI50['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI100.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI100.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100 = CB_CSA_episodes_DeltaHHI100[~CB_CSA_episodes_DeltaHHI100['if_contaminated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6ee9d-819b-4b82-a8c5-4a8bc565508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CBSA_episodes_DeltaHHI20['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI20.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI20.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI20 = CB_CBSA_episodes_DeltaHHI20[~CB_CBSA_episodes_DeltaHHI20['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI50.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI50.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50 = CB_CBSA_episodes_DeltaHHI50[~CB_CBSA_episodes_DeltaHHI50['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI100.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI100.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100 = CB_CBSA_episodes_DeltaHHI100[~CB_CBSA_episodes_DeltaHHI100['if_contaminated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001cd58-20cd-41b9-9b92-8c9811bfef43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3854d62-8d19-422a-9755-7019b180f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple versions of episodes definiton (by market share or HHI, cutoff on implied HHI increases, etc.). I go over each\n",
    "# version here\n",
    "\n",
    "episodes_files = [\n",
    "    [CB_CSA_episodes_DeltaHHI20,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI20.csv',\n",
    "    ],\n",
    "    [CB_CSA_episodes_DeltaHHI50,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI50.csv',\n",
    "    ],\n",
    "    [CB_CSA_episodes_DeltaHHI100,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI100.csv',\n",
    "    ],\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    episodes = episodes_file[0].copy()\n",
    "    N_matches = episodes_file[1]\n",
    "    file_path = episodes_file[2]\n",
    "\n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CSA_POP = pd.read_csv(\"../RawData/MSA/POP/CSA_POP.csv\")\n",
    "    CSA_INC = pd.read_csv(\"../RawData/MSA/CAINC1/CSA_INC.csv\")\n",
    "    CSA_Data = CSA_POP.merge(CSA_INC,on=['CSA Code','year'])\n",
    "    CSA_Data = CSA_Data[['CSA Code','year','inc','pop']]\n",
    "    Same_State_CSA_pairs = pd.read_csv(\"../RawData/MSA/CAINC1/Same_State_CSA_pairs.csv\")\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CSA\n",
    "        CSA_Data_oneyear = CSA_Data[CSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CSA\n",
    "        CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "        if len(CSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "        # Remove other CSAs in the same state from potential matches\n",
    "        Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==subrow['CSA Code']]\n",
    "            CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,5))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_Dollar_avg']\n",
    "        elif 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "        if 'target_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_Dollar_avg']\n",
    "        elif 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "        if 'other_targets_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_Dollar_avg']\n",
    "        elif 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==row['CSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CSA Code','sale_year','State',\n",
    "            'gross_spread','avg_yield','avg_spread','avg_maturity','spread_by_maturity','amount',\n",
    "            'yield_by_maturity','all_maturity','all_amount',\n",
    "            'County','Bid','taxable_code','issuer_type','Issuer','security_type','if_advisor','if_dual_advisor',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorRatio_hat','CRRatio_hat','InsureRatio_hat',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        # Some cross-sectional heterogeneity\n",
    "\n",
    "        # Whether the underwriter is the target bank in M&A. Note that I only use mergers where both sides have market share > 0\n",
    "        # Also note that I define \"bank_is_acquiror\" and \"bank_is_target\" for control CSAs too - For those areas, even though M&As\n",
    "        # are not significant enough to make it treated, there might still be some within-market merger going on. To avoid such firms\n",
    "        # having different trends, later I will include \"bank_is_involved\" times \"post\" interaction terms\n",
    "\n",
    "        # Only using banks involved in M&A post \"episode_start_year\"\n",
    "        if False:\n",
    "            mergers = row['mergers']\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "        # code block)\n",
    "        mergers = CSA_affected[\n",
    "            (CSA_affected['CSA Code']==row['CSA Code'])&\n",
    "            (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CSA Code','sale_year','State',\n",
    "                'gross_spread','avg_yield','avg_spread','avg_maturity','spread_by_maturity','amount',\n",
    "                'yield_by_maturity','all_maturity','all_amount',\n",
    "                'County','Bid','taxable_code','issuer_type','Issuer','security_type','if_advisor','if_dual_advisor',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorRatio_hat','CRRatio_hat','InsureRatio_hat',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "            mergers = CSA_affected[\n",
    "                (CSA_affected['CSA Code']==item)&\n",
    "                (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    black_pop = pd.read_csv(\"../RawData/MSA/POP/black_pop.csv\")\n",
    "    black_pop = black_pop[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(black_pop,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5ffbd-283f-4db7-9da1-89f4d07bf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple versions of episodes definiton (by market share or HHI, cutoff on implied HHI increases, etc.). I go over each\n",
    "# version here\n",
    "\n",
    "episodes_files = [\n",
    "    [CB_CBSA_episodes_DeltaHHI20,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI20.csv',\n",
    "    ],\n",
    "    [CB_CBSA_episodes_DeltaHHI50,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI50.csv',\n",
    "    ],\n",
    "    [CB_CBSA_episodes_DeltaHHI100,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI100.csv',\n",
    "    ],\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    episodes = episodes_file[0].copy()\n",
    "    N_matches = episodes_file[1]\n",
    "    file_path = episodes_file[2]\n",
    "\n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CBSA_POP = pd.read_csv(\"../RawData/MSA/POP/CBSA_POP.csv\")\n",
    "    CBSA_INC = pd.read_csv(\"../RawData/MSA/CAINC1/CBSA_INC.csv\")\n",
    "    CBSA_Data = CBSA_POP.merge(CBSA_INC,on=['CBSA Code','year'])\n",
    "    CBSA_Data = CBSA_Data[['CBSA Code','year','inc','pop']]\n",
    "    Same_State_CBSA_pairs = pd.read_csv(\"../RawData/MSA/CAINC1/Same_State_CBSA_pairs.csv\")\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CBSA\n",
    "        CBSA_Data_oneyear = CBSA_Data[CBSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CBSA\n",
    "        CBSA_Data_oneyear_frag = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']==row['CBSA Code']].copy()\n",
    "        if len(CBSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CBSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CBSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CBSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CBSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CBSA_Data_oneyear['inc'] = winsor2(CBSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CBSA_Data_oneyear['pop'] = winsor2(CBSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CBSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CBSA_Data_oneyear['dist'] = CBSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']!=row['CBSA Code']]\n",
    "        # Remove other CBSAs in the same state from potential matches\n",
    "        Same_State_CBSAs = list(Same_State_CBSA_pairs[Same_State_CBSA_pairs['CBSA_1']==row['CBSA Code']]['CBSA_2'])\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[~CBSA_Data_oneyear['CBSA Code'].isin(Same_State_CBSAs)]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CBSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==subrow['CBSA Code']]\n",
    "            CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CBSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,5))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_Dollar_avg']\n",
    "        elif 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "        if 'target_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_Dollar_avg']\n",
    "        elif 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "        if 'other_targets_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_Dollar_avg']\n",
    "        elif 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==row['CBSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CBSA Code','sale_year','State',\n",
    "            'gross_spread','avg_yield','avg_spread','avg_maturity','spread_by_maturity','amount',\n",
    "            'yield_by_maturity','all_maturity','all_amount',\n",
    "            'County','Bid','taxable_code','issuer_type','Issuer','security_type','if_advisor','if_dual_advisor',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorRatio_hat','CRRatio_hat','InsureRatio_hat',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        # Some cross-sectional heterogeneity\n",
    "\n",
    "        # Whether the underwriter is the target bank in M&A. Note that I only use mergers where both sides have market share > 0\n",
    "        # Also note that I define \"bank_is_acquiror\" and \"bank_is_target\" for control CBSAs too - For those areas, even though M&As\n",
    "        # are not significant enough to make it treated, there might still be some within-market merger going on. To avoid such firms\n",
    "        # having different trends, later I will include \"bank_is_involved\" times \"post\" interaction terms\n",
    "\n",
    "        # Only using banks involved in M&A post \"episode_start_year\"\n",
    "        if False:\n",
    "            mergers = row['mergers']\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "        # code block)\n",
    "        mergers = CBSA_affected[\n",
    "            (CBSA_affected['CBSA Code']==row['CBSA Code'])&\n",
    "            (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CBSA Code','sale_year','State',\n",
    "                'gross_spread','avg_yield','avg_spread','avg_maturity','spread_by_maturity','amount',\n",
    "                'yield_by_maturity','all_maturity','all_amount',\n",
    "                'County','Bid','taxable_code','issuer_type','Issuer','security_type','if_advisor','if_dual_advisor',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorRatio_hat','CRRatio_hat','InsureRatio_hat',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "            mergers = CBSA_affected[\n",
    "                (CBSA_affected['CBSA Code']==item)&\n",
    "                (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    black_pop = pd.read_csv(\"../RawData/MSA/POP/black_pop.csv\")\n",
    "    black_pop = black_pop[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(black_pop,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
