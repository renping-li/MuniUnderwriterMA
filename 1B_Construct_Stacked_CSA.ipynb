{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc1ba46-629a-4c44-8b87-e3c84c37ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2fbd3-d304-40e5-b2e5-145d019e9f6c",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab79e34-03a1-4ba0-b5c1-022f7c4875bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPF\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "# This \"has_ratings\" indicator will be used to determine if a bond issue falls in the expertise of the merging underwriters.\n",
    "# For the program to carry through, convert it to string, which will be consistent with other variables of categories of issues.\n",
    "GPF['has_ratings'] = GPF['has_ratings'].astype(str)\n",
    "\n",
    "# Parent relationship\n",
    "GPF_names = pd.read_parquet('../CleanData/SDC/0H_GPF_Parent.parquet')\n",
    "\n",
    "# HHI and market share of each underwriter\n",
    "HHI_byCSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCSA.csv')\n",
    "market_share_all_markets_byCSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCSA.csv')\n",
    "HHI_byCBSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCBSA.csv')\n",
    "market_share_all_markets_byCBSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCBSA.csv')\n",
    "\n",
    "# Portfolio weights of CSAs within underwriter\n",
    "csa_share_withinbank = pd.read_csv('../CleanData/SDC/1A_csa_share_withinbank.csv')\n",
    "\n",
    "# All M&As\n",
    "MA = pd.read_parquet('../CleanData/SDC/0B_M&A.parquet')\n",
    "MA = MA.reset_index(drop=True)\n",
    "\n",
    "# Withdrawn M&As\n",
    "MA_withdrawn = pd.read_csv(\"../CleanData/SDC/0I_MA_withdrawn.csv\")\n",
    "\n",
    "# Quantity of issuance\n",
    "StateXCountyXBid = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXBid.parquet\")\n",
    "StateXCountyXUsageBB = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageBB.parquet\")\n",
    "StateXCountyXUsageGeneral = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageGeneral.parquet\")\n",
    "StateXCountyXUsageMain = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageMain.parquet\")\n",
    "StateXCountyXIssuerType = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXIssuerType.parquet\")\n",
    "\n",
    "StateXCounty = StateXCountyXBid.groupby(['State','County','sale_year']).agg({'amount':sum})\n",
    "StateXCounty = StateXCounty.reset_index()\n",
    "\n",
    "# Demographics\n",
    "CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "CBSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "CBSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "\n",
    "#-------------#\n",
    "# Import CBSA #\n",
    "#-------------#\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "# \"CSA\" is for metropolitan and \"CBSA\" includes also those micropolitan\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSAData = CBSAData.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSAData = CBSAData.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSAData = CBSAData[CBSAData['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSAData['County'] = CBSAData['County'].str.upper()\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' COUNTY','')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' AND ',' & ')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace('.','',regex=False)\n",
    "CBSAData['CSA Code'] = CBSAData['CSA Code'].astype(float)\n",
    "CBSAData['CBSA Code'] = CBSAData['CBSA Code'].astype(float)\n",
    "\n",
    "# CSA characteristics to be used in matching, which are average yield and spread in the county\n",
    "\n",
    "CSACharsForMatch = GPF[['CSA Code','sale_year','avg_yield','gross_spread']]\n",
    "CSACharsForMatch = CSACharsForMatch[~(pd.isnull(CSACharsForMatch['CSA Code']))]\n",
    "CSACharsForMatch = CSACharsForMatch[~(pd.isnull(CSACharsForMatch['gross_spread']))]\n",
    "CSACharsForMatchSpread = CSACharsForMatch.groupby(['CSA Code','sale_year']).agg({'gross_spread':'mean'})\n",
    "CSACharsForMatchSpread = CSACharsForMatchSpread.reset_index()\n",
    "\n",
    "CSACharsForMatch = GPF[['CSA Code','sale_year','avg_yield','gross_spread']]\n",
    "CSACharsForMatch = CSACharsForMatch[~(pd.isnull(CSACharsForMatch['CSA Code']))]\n",
    "CSACharsForMatch = CSACharsForMatch[~(pd.isnull(CSACharsForMatch['avg_yield']))]\n",
    "CSACharsForMatchYield = CSACharsForMatch.groupby(['CSA Code','sale_year']).agg({'avg_yield':'mean'})\n",
    "CSACharsForMatchYield = CSACharsForMatchYield.reset_index()\n",
    "\n",
    "CSACharsForMatch = CSACharsForMatchSpread.merge(CSACharsForMatchYield,on=['CSA Code','sale_year'],how='outer')\n",
    "\n",
    "# Importance of an underwriter within US or within a CSA, used to determine underwriter expertise\n",
    "RankBankWithinCategoryUS = pd.read_csv(\"../CleanData/SDC/0D_Rank_Bank_Within_Category_US.csv\")\n",
    "RankBankWithinCategoryCSA = pd.read_csv(\"../CleanData/SDC/0D_Rank_Bank_Within_Category_CSA.csv\")\n",
    "RankBankWithinCategoryUS_gb = RankBankWithinCategoryUS.groupby('underwriter')\n",
    "RankBankWithinCategoryCSA_gb = RankBankWithinCategoryCSA.groupby('underwriter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce56c23-9213-4363-bba3-a3fab3858dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ef9f6-8a41-4917-bb7c-c65510ca51c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3facbabf-dfeb-4dcf-882c-ad712cbb311c",
   "metadata": {},
   "source": [
    "## 1.1. Find Episodes Less Confounded by Commercial Bank M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119489ed-825d-467d-b46c-eb12d96ae4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 563 ms, total: 3.22 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This is used to construct a more \"clean\" sample that is not affected by concurrent CB M&A, which could also affect local\n",
    "# economy and then affect yield\n",
    "\n",
    "#----------------------------------#\n",
    "# Identify CSAs affected by CB M&A #\n",
    "#----------------------------------#\n",
    "\n",
    "try:\n",
    "    del(FUN_1B_Get_Delta_CB_HHI)\n",
    "except:\n",
    "    pass\n",
    "import FUN_1B_Get_Delta_CB_HHI\n",
    "importlib.reload(FUN_1B_Get_Delta_CB_HHI)\n",
    "from FUN_1B_Get_Delta_CB_HHI import FUN_1B_Get_Delta_CB_HHI\n",
    "\n",
    "# CBs in SOD\n",
    "SOD = pd.read_csv('../CleanData/FDIC/0I_SOD.csv')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].str.replace(',','')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].astype(int)\n",
    "\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "\n",
    "divided_list = [[CSAs[i::10]] for i in range(10)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        Delta_CB_HHI = p.starmap(FUN_1B_Get_Delta_CB_HHI, divided_list)\n",
    "Delta_CB_HHI = pd.concat(Delta_CB_HHI)\n",
    "\n",
    "# Generate lagged and forward versions of the variable\n",
    "Delta_CB_HHI = Delta_CB_HHI.sort_values(by=['CSA Code','year'])\n",
    "Delta_CB_HHI['CB_hhi_dif_m1'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(1)\n",
    "Delta_CB_HHI['CB_hhi_dif_m2'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(2)\n",
    "Delta_CB_HHI['CB_hhi_dif_m3'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(3)\n",
    "Delta_CB_HHI['CB_hhi_dif_m4'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(4)\n",
    "Delta_CB_HHI['CB_hhi_dif_p1'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(-1)\n",
    "Delta_CB_HHI['CB_hhi_dif_p2'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(-2)\n",
    "Delta_CB_HHI['CB_hhi_dif_p3'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(-3)\n",
    "Delta_CB_HHI['CB_hhi_dif_p4'] = Delta_CB_HHI.groupby('CSA Code')['CB_hhi_dif'].shift(-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d93b09-440a-4d92-9831-416b27dd34a1",
   "metadata": {},
   "source": [
    "# 2. Sample description & Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc927eaf-ed05-437d-80bc-c21b418fc525",
   "metadata": {},
   "source": [
    "## 2.1 All M&As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386dcc28-21b8-4d58-b000-dc00facf58ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total number of M&As\n",
    "n_MA = len(MA)\n",
    "\n",
    "MA['both_active'] = False\n",
    "MA['both_active_overlap_CSA'] = False\n",
    "\n",
    "# M&As where both underwrite municipal bonds right before merger\n",
    "for idx,row in MA.iterrows():\n",
    "    GPF_oneyear = GPF[GPF['sale_year']==row['sale_year']-1]\n",
    "    names = list(chain.from_iterable(list(np.array(GPF_oneyear[name_GPF_colnames]))))\n",
    "    names = list(set(names))\n",
    "    parent_names = list(chain.from_iterable(list(np.array(GPF_oneyear[parent_name_GPF_colnames]))))\n",
    "    parent_names = list(set(parent_names))\n",
    "    if (row['target'] in names or row['target'] in parent_names) and \\\n",
    "        (row['acquiror'] in names or row['acquiror'] in parent_names):\n",
    "        MA.at[idx,'both_active'] = True\n",
    "\n",
    "# M&As where both underwirte municipal bonds before merger and have market overlap in terms of CSA\n",
    "for idx,row in MA.iterrows():\n",
    "    GPF_oneyear = GPF[GPF['sale_year']==row['sale_year']-1]\n",
    "    CSAs = list(GPF_oneyear['CSA Code'].unique())\n",
    "    CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "    for CSA in CSAs:\n",
    "        GPF_oneyearCSA = GPF_oneyear[GPF_oneyear['CSA Code']==CSA]\n",
    "        names = list(chain.from_iterable(list(np.array(GPF_oneyearCSA[name_GPF_colnames]))))\n",
    "        names = list(set(names))\n",
    "        parent_names = list(chain.from_iterable(list(np.array(GPF_oneyearCSA[parent_name_GPF_colnames]))))\n",
    "        parent_names = list(set(parent_names))\n",
    "        # If for any CSA there is overlap, then there is overlap\n",
    "        if (row['target'] in names or row['target'] in parent_names) and \\\n",
    "            (row['acquiror'] in names or row['acquiror'] in parent_names):\n",
    "            MA.at[idx,'both_active_overlap_CSA'] = True\n",
    "\n",
    "n_MA_both_active = np.sum(MA['both_active']==True)\n",
    "n_MA_both_active_overlap_CSA = np.sum(MA['both_active_overlap_CSA']==True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07218124-a775-4e86-9c17-a986ed74ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = \"\"\n",
    "formatted_row = \"\\\\# of M\\\\&As\"+\"&\"+f\"{n_MA:.0f}\"+\"\\\\\\\\\\n\"\n",
    "latex_table += formatted_row\n",
    "formatted_row = \"\\\\# of M\\\\&As, both sides active\"+\"&\"+f\"{n_MA_both_active:.0f}\"+\"\\\\\\\\\\n\"\n",
    "latex_table += formatted_row\n",
    "formatted_row = \"\\\\# of M\\\\&As, both sides active and with CSA overlap\"+\"&\"+f\"{n_MA_both_active_overlap_CSA:.0f}\"+\"\\\\\\\\\\n\"\n",
    "latex_table += formatted_row\n",
    "with open(\"../Draft/tabs/sum_stats_MA.tex\", \"w\") as file:\n",
    "    file.write(latex_table)\n",
    "\n",
    "# Number: Number of M&As where both sides are active #\n",
    "with open('../Draft/nums/n_MA_both_active.tex','w') as file:\n",
    "    file.write(str(n_MA_both_active))\n",
    "\n",
    "# Number: Number of M&As where both sides are active and have geographic overlap #\n",
    "with open('../Draft/nums/n_MA_both_active_overlap_CSA.tex','w') as file:\n",
    "    file.write(str(n_MA_both_active_overlap_CSA))\n",
    "\n",
    "# Number: Number of M&As where both sides are active post 1990 #\n",
    "n_MA_both_active_1990on = len(MA[(MA['both_active']==True)&(MA['sale_year']>=1990)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6758-3c02-476d-95eb-5211dd5c726c",
   "metadata": {},
   "source": [
    "## 2.2 Withdrawn M&As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7f5ac5-8888-4204-a79a-3074a27828e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_withdrawn = MA_withdrawn.rename(columns={'announce_year':'sale_year'})[['target','acquiror','sale_year']]\n",
    "\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "\n",
    "MA_withdrawn['both_active'] = False\n",
    "MA_withdrawn['both_active_overlap_CSA'] = False\n",
    "\n",
    "# M&As where both underwrite municipal bonds right before merger\n",
    "for idx,row in MA_withdrawn.iterrows():\n",
    "    GPF_oneyear = GPF[GPF['sale_year']==row['sale_year']-1]\n",
    "    names = list(chain.from_iterable(list(np.array(GPF_oneyear[name_GPF_colnames]))))\n",
    "    names = list(set(names))\n",
    "    parent_names = list(chain.from_iterable(list(np.array(GPF_oneyear[parent_name_GPF_colnames]))))\n",
    "    parent_names = list(set(parent_names))\n",
    "    if (row['target'] in names or row['target'] in parent_names) and \\\n",
    "        (row['acquiror'] in names or row['acquiror'] in parent_names):\n",
    "        MA_withdrawn.at[idx,'both_active'] = True\n",
    "\n",
    "# M&As where both underwirte municipal bonds before merger and have market overlap in terms of CSA\n",
    "for idx,row in MA_withdrawn.iterrows():\n",
    "    GPF_oneyear = GPF[GPF['sale_year']==row['sale_year']-1]\n",
    "    CSAs = list(GPF_oneyear['CSA Code'].unique())\n",
    "    CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "    for CSA in CSAs:\n",
    "        GPF_oneyearCSA = GPF_oneyear[GPF_oneyear['CSA Code']==CSA]\n",
    "        names = list(chain.from_iterable(list(np.array(GPF_oneyearCSA[name_GPF_colnames]))))\n",
    "        names = list(set(names))\n",
    "        parent_names = list(chain.from_iterable(list(np.array(GPF_oneyearCSA[parent_name_GPF_colnames]))))\n",
    "        parent_names = list(set(parent_names))\n",
    "        # If for any CSA there is overlap, then there is overlap\n",
    "        if (row['target'] in names or row['target'] in parent_names) and \\\n",
    "            (row['acquiror'] in names or row['acquiror'] in parent_names):\n",
    "            MA_withdrawn.at[idx,'both_active_overlap_CSA'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "557c4ee2-8c9a-46a6-8e63-1d128798fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_MA_withdrawn_both_active = np.sum(MA_withdrawn['both_active']==True)\n",
    "n_MA_withdrawn_both_active_overlap_CSA = np.sum(MA_withdrawn['both_active_overlap_CSA']==True)\n",
    "\n",
    "# Number: Number of M&As where both sides are active #\n",
    "with open('../Draft/nums/n_MA_withdrawn_both_active.tex','w') as file:\n",
    "    file.write(str(n_MA_withdrawn_both_active))\n",
    "\n",
    "# Number: Number of M&As where both sides are active and have geographic overlap #\n",
    "with open('../Draft/nums/n_MA_withdrawn_both_active_overlap_CSA.tex','w') as file:\n",
    "    file.write(str(n_MA_withdrawn_both_active_overlap_CSA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266031ff-1fde-425d-8d41-c4417565cbc4",
   "metadata": {},
   "source": [
    "## 2.3 Distribution of number of underwriters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff4e4949-a88a-44d6-8f6f-69590cbd2856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "GPF_gb = GPF[(~pd.isnull(GPF['CSA Code']))&(~pd.isnull(GPF['sale_year']))].groupby(['CSA Code','sale_year'])\n",
    "n_banks_byCSAyear = []\n",
    "keys = list(GPF_gb.groups.keys())\n",
    "\n",
    "for group in keys:\n",
    "\n",
    "    GPF_oneCSAyear = GPF_gb.get_group(group)\n",
    "    GPF_oneCSAyearm1 = pd.DataFrame()\n",
    "    # try:\n",
    "    #     GPF_oneCSAyearm1 = GPF_gb.get_group((group[0],group[1]-1))\n",
    "    # except:\n",
    "    #     pass\n",
    "    GPF_oneCSAyearp1 = pd.DataFrame()\n",
    "    # try:\n",
    "    #     GPF_oneCSAyearp1 = GPF_gb.get_group((group[0],group[1]+1))\n",
    "    # except:\n",
    "    #     pass\n",
    "    GPF_CSAthreeyears = pd.concat([GPF_oneCSAyear,GPF_oneCSAyearm1,GPF_oneCSAyearp1])\n",
    "\n",
    "    banks = list(chain.from_iterable(list(np.array(GPF_CSAthreeyears[parent_name_GPF_colnames]))))\n",
    "    banks = list(set(banks))\n",
    "    banks = [item for item in banks if str(item)!='nan']\n",
    "    n_banks = len(banks)\n",
    "    amount_total = np.sum(GPF_CSAthreeyears['amount'])\n",
    "\n",
    "    # Record data\n",
    "    n_banks_byCSAyear = n_banks_byCSAyear+[{\n",
    "        'CSA Code':group[0],\n",
    "        'year':group[1],\n",
    "        'n_banks':n_banks,\n",
    "        'amount_total':amount_total,\n",
    "        }]\n",
    "\n",
    "n_banks_byCSAyear = pd.DataFrame(n_banks_byCSAyear)\n",
    "\n",
    "n_banks_byCSAyear_oneyear_withgroup = []\n",
    "years = list(n_banks_byCSAyear['year'].unique())\n",
    "for year in years:\n",
    "    n_banks_byCSAyear_oneyear = n_banks_byCSAyear[n_banks_byCSAyear['year']==year].copy()\n",
    "    n_banks_byCSAyear_oneyear['rank'] = n_banks_byCSAyear_oneyear['amount_total'].rank()\n",
    "    n_banks_byCSAyear_oneyear['group'] = None\n",
    "    n_banks_byCSAyear_oneyear.loc[\n",
    "        n_banks_byCSAyear_oneyear['rank']<=len(n_banks_byCSAyear_oneyear)/3,\n",
    "        'group'] = 1\n",
    "    n_banks_byCSAyear_oneyear.loc[\n",
    "        (n_banks_byCSAyear_oneyear['rank']>len(n_banks_byCSAyear_oneyear)/3)&\\\n",
    "        (n_banks_byCSAyear_oneyear['rank']<=len(n_banks_byCSAyear_oneyear)/3*2),\n",
    "        'group'] = 2\n",
    "    n_banks_byCSAyear_oneyear.loc[\n",
    "        (n_banks_byCSAyear_oneyear['rank']>len(n_banks_byCSAyear_oneyear)/3*2)&\\\n",
    "        (n_banks_byCSAyear_oneyear['rank']<=len(n_banks_byCSAyear_oneyear)),\n",
    "        'group'] = 3\n",
    "    n_banks_byCSAyear_oneyear_withgroup = n_banks_byCSAyear_oneyear_withgroup+[n_banks_byCSAyear_oneyear]\n",
    "n_banks_byCSAyear = pd.concat(n_banks_byCSAyear_oneyear_withgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b401b8ca-a38f-408f-82a2-5c8a53c38f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Divide into three groups based on size\n",
    "sns.kdeplot(n_banks_byCSAyear[n_banks_byCSAyear['group']==1]['n_banks'],\n",
    "    fill=False,color='blue', linestyle='-',label='Small CSAs')\n",
    "sns.kdeplot(n_banks_byCSAyear[n_banks_byCSAyear['group']==2]['n_banks'],\n",
    "    fill=False,color='green', linestyle='--',label='Medium CSAs')\n",
    "sns.kdeplot(n_banks_byCSAyear[n_banks_byCSAyear['group']==3]['n_banks'],\n",
    "    fill=False,color='red', linestyle=':',label='Large CSAs')\n",
    "\n",
    "plt.xlim(0, 60)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('Number of active underwriters',fontsize=12)\n",
    "plt.ylabel('Density',fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.savefig('../Draft/figs/NumBanks.eps', format='eps', bbox_inches='tight')\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "with open('../Draft/nums/n_banks_median.tex','w') as file:\n",
    "    file.write(str(round(np.median(n_banks_byCSAyear['n_banks']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6604c3-0dc3-47f2-9e72-0743341e675e",
   "metadata": {},
   "source": [
    "# 3. Construct Events of M&As, Using CSAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2c7ff-da19-428b-afef-08163579473e",
   "metadata": {},
   "source": [
    "## 3.1. Find CSA X Year affected by merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece052f-e625-4b98-8951-8aaac7315f8f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Go over each merger. Check the CSAs affected by the merger (i.e., either side has business in the CSA in the year prior to the merger). Check if the merger affects just one underwriter or affects multiple underwriters in this CSA.\n",
    "- Note that for the column \"market share of other targets\", the optimal object to put there is the market share of the other target alone. Here I am instead putting in market share of the other target's parent. This should make a minimal difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd7d69d-9072-4700-9e9c-e9d2e6d0e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# %%time\n",
    "\n",
    "def proc_list(MA_frag):\n",
    "\n",
    "    raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "    name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    \n",
    "    CSA_affected = []\n",
    "    MA_frag = MA_frag.reset_index(drop=True)\n",
    "    \n",
    "    for idx,row in MA_frag.iterrows():\n",
    "        \n",
    "        # Find CSAs that this merger affects\n",
    "        # Determine if an underwriter is active in an CSA based on activity of PRIOR years\n",
    "        GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "\n",
    "        # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "        # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "        # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "        other_targets = \\\n",
    "            list(MA[(MA['acquiror']==row['acquiror'])&\n",
    "            (MA['sale_year']==row['sale_year'])&\n",
    "            (MA['target']!=row['target'])]['target'])\n",
    "        \n",
    "        for CSA in list(GPF_prioryears['CSA Code'].unique()):\n",
    "\n",
    "            GPF_prioryears_oneCSA = GPF_prioryears[GPF_prioryears['CSA Code']==CSA]\n",
    "\n",
    "            # Underwriters in this state\n",
    "            underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSA[name_GPF_colnames]))))\n",
    "            underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "            underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "            # Parents of underwriters in this state\n",
    "            parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSA[parent_name_colnames]))))\n",
    "            parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "            parents_priorMA = list(set(parents_priorMA))\n",
    "            # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "            subsidiaries_priorMA = list(GPF_names[\n",
    "                (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "\n",
    "            # Determine if merger affects the CSA, and if both sides have business\n",
    "            IF_acquiror_active = None\n",
    "            IF_target_active = None\n",
    "            IF_other_target_active = None\n",
    "            if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                IF_acquiror_active = True\n",
    "            if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                IF_target_active = True\n",
    "            for other_target in other_targets:\n",
    "                if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                    IF_other_target_active = True\n",
    "\n",
    "            # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "            # share \"market_share_all_markets_byCSA\" is calculated at the parent level. There are many cases where market share of a\n",
    "            # firm in an area is unavailable, which is because of no presence.\n",
    "\n",
    "\n",
    "\n",
    "            #-------------------------#\n",
    "            # Market share by N deals #\n",
    "            #-------------------------#\n",
    "\n",
    "            # (1) Market share of acquiror\n",
    "            # Determine parent of target, as \"market_share_all_markets_byCSA\" is at parent level\n",
    "            try:\n",
    "                # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                # Situation where acquiror is a parent\n",
    "                acquiror_parent = row['acquiror']\n",
    "            try:\n",
    "                acquiror_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m1 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m2 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m3 = 0\n",
    "\n",
    "            # (2) Market share of target\n",
    "            try:\n",
    "                # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                target_parent = row['target']\n",
    "            try:\n",
    "                target_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m1 = 0\n",
    "            try:\n",
    "                target_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m2 = 0\n",
    "            try:\n",
    "                target_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m3 = 0\n",
    "\n",
    "            # (3) Market share of other targets in the same transaction\n",
    "            # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "            other_targets_parents = \\\n",
    "                list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                list(other_targets)\n",
    "            other_targets_parents = list(set(other_targets_parents))\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m1 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m2 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m3 = 0\n",
    "\n",
    "\n",
    "\n",
    "            # Record data\n",
    "            if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                CSA_affected = CSA_affected+[{\n",
    "                    'CSA Code':CSA,\n",
    "                    'sale_year':row['sale_year'],\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'target':row['target'],\n",
    "                    'other_targets':other_targets,\n",
    "                    'acquiror_parent':acquiror_parent,\n",
    "                    'target_parent':target_parent,\n",
    "                    'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                    'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                    'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                    'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                    'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                    'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                    'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                    'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                    'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                }]\n",
    "            acquiror_market_share_N_m1 = None\n",
    "            acquiror_market_share_N_m2 = None\n",
    "            acquiror_market_share_N_m3 = None\n",
    "            target_market_share_N_m1 = None\n",
    "            target_market_share_N_m2 = None\n",
    "            target_market_share_N_m3 = None\n",
    "            other_targets_market_share = None\n",
    "            other_targets_market_share_N_m1 = None\n",
    "            other_targets_market_share_N_m2 = None\n",
    "            other_targets_market_share_N_m3 = None\n",
    "    \n",
    "    CSA_affected = pd.DataFrame(CSA_affected)\n",
    "    return CSA_affected\n",
    "\n",
    "MA_dd = dd.from_pandas(MA, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    CSA_affected = MA_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=\n",
    "    ['CSA Code','sale_year','acquiror','target',\n",
    "    'other_targets','acquiror_parent','target_parent',\n",
    "    'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "    'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "    'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "    ])).compute()\n",
    "\n",
    "# Average market share over past three years\n",
    "CSA_affected['acquiror_market_share_N_avg'] = \\\n",
    "    (CSA_affected['acquiror_market_share_N_m1']+\\\n",
    "    CSA_affected['acquiror_market_share_N_m2']+\\\n",
    "    CSA_affected['acquiror_market_share_N_m3'])/3\n",
    "CSA_affected['target_market_share_N_avg'] = \\\n",
    "    (CSA_affected['target_market_share_N_m1']+\\\n",
    "    CSA_affected['target_market_share_N_m2']+\\\n",
    "    CSA_affected['target_market_share_N_m3'])/3\n",
    "CSA_affected['other_targets_market_share_N_avg'] = \\\n",
    "    (CSA_affected['other_targets_market_share_N_m1']+\\\n",
    "    CSA_affected['other_targets_market_share_N_m2']+\\\n",
    "    CSA_affected['other_targets_market_share_N_m3'])/3\n",
    "\n",
    "# As this step takes significant time, export output\n",
    "CSA_affected.to_parquet('../CleanData/MAEvent/1B_CSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "387fe001-d82b-4687-b286-029228c408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSA_affected = pd.read_parquet('../CleanData/MAEvent/1B_CSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851c1fc-baa1-4422-92b0-83fa6b615b5f",
   "metadata": {},
   "source": [
    "## 3.2 Identify merger episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bd08e-f6c9-46b8-8560-1291b3d7dfe2",
   "metadata": {},
   "source": [
    "### 3.2.1 Method 1: By market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1377937f-b208-4c67-b642-109f38717e1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n"
     ]
    }
   ],
   "source": [
    "#----------------------------#\n",
    "# Market share by N of deals #\n",
    "#----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CSA_affected\"\n",
    "\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "CSA_episodes_marketshare_N = []\n",
    "\n",
    "for CSA in list(CSA_affected['CSA Code'].unique()):\n",
    "\n",
    "    CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==CSA]\n",
    "    CSA_affected_part = CSA_affected_part[\n",
    "        (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    years = CSA_affected_part['sale_year'].unique()\n",
    "    years = sorted(years)\n",
    "    for sale_year in years:\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CSA_affected_episode = CSA_affected_part[(CSA_affected_part['sale_year']>=sale_year)&(CSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        # When a firm acquires multiple firms, market share of other firms are in \"other_targets_market_share_N_avg\", so just keeping one record\n",
    "        # is sufficient\n",
    "        CSA_affected_episode = CSA_affected_episode.drop_duplicates(['acquiror','sale_year'])\n",
    "        # Alternative aggregation methods might be more reasonable. Also, this does not account for that target tends to be smaller so threshold\n",
    "        # for them should be smaller too. Even better, can compute the implied-HHI change (based on historical data) of this merger, and put threshold\n",
    "        # on that, which is definitely more powerful.\n",
    "        acquiror_market_share_N_avg = np.sum(CSA_affected_episode['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_avg = np.sum(CSA_affected_episode['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_avg = np.sum(CSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Out of all mergers in this episode, calculate\n",
    "        # (1) the max of sum of market shares of merging entities\n",
    "        max_sum_share = \\\n",
    "            np.max(CSA_affected_episode['acquiror_market_share_N_avg']+CSA_affected_episode['target_market_share_N_avg']\n",
    "                +CSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "        # (2) the max of min of market shares of merging entities\n",
    "        max_min_share = \\\n",
    "            np.max(np.minimum(CSA_affected_episode['acquiror_market_share_N_avg'],\n",
    "                CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']))\n",
    "        # (3) the mean of sum of market shares of merging entities\n",
    "        mean_sum_share = \\\n",
    "            np.mean(CSA_affected_episode['acquiror_market_share_N_avg']+CSA_affected_episode['target_market_share_N_avg']\n",
    "                +CSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if acquiror_market_share_N_avg>0.05 and target_market_share_N_avg+other_targets_market_share_N_avg>0.05:\n",
    "            # An episode is identified\n",
    "            CSA_episodes_marketshare_N = CSA_episodes_marketshare_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CSA Code':CSA,\n",
    "                'mergers':CSA_affected_episode,\n",
    "                'acquiror_market_share_N_avg':acquiror_market_share_N_avg,\n",
    "                'target_market_share_N_avg':target_market_share_N_avg,\n",
    "                'other_targets_market_share_N_avg':other_targets_market_share_N_avg,\n",
    "                'max_sum_share':max_sum_share,\n",
    "                'max_min_share':max_min_share,\n",
    "                'mean_sum_share':mean_sum_share,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CSA_episodes_marketshare_N = pd.DataFrame(CSA_episodes_marketshare_N)\n",
    "\n",
    "print(len(CSA_episodes_marketshare_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e45e738-6945-4480-8ad4-5d7f12957623",
   "metadata": {},
   "source": [
    "### 3.2.2 Method 2: By implied rise in HHI due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc18a79-ee2e-4514-9e51-4f382baac579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------#\n",
    "# Change in HHI by N of deals #\n",
    "#-----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CSA_affected\"\n",
    "\n",
    "CSA_episodes_impliedHHI_N = []\n",
    "\n",
    "for CSA in list(CSA_affected['CSA Code'].unique()):\n",
    "\n",
    "    CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==CSA]\n",
    "    CSA_affected_part = CSA_affected_part[\n",
    "        (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in CSA_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CSA_affected_episode = CSA_affected_part[(CSA_affected_part['sale_year']>=sale_year)&(CSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneCSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CSA Code']==CSA)]\n",
    "        \n",
    "        # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCSA_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneCSA_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # HHI prior to merger\n",
    "        hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "\n",
    "        # Implied HHI post merger\n",
    "        CSA_affected_episode = CSA_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in CSA_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        hhi_dif = hhi_predicted-hhi_piror\n",
    "\n",
    "        # Market shares used in summary statistics\n",
    "        CSA_affected_episode['min_share'] = np.minimum(CSA_affected_episode['acquiror_market_share_N_avg'],\n",
    "            CSA_affected_episode['target_market_share_N_avg']+\\\n",
    "            CSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "        CSA_affected_episode = CSA_affected_episode.sort_values('min_share')\n",
    "        CSA_affected_episode_topshare = CSA_affected_episode[-1:]\n",
    "        acquiror_market_share_N_max = np.max(CSA_affected_episode_topshare['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_max = np.max(CSA_affected_episode_topshare['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_max = np.max(CSA_affected_episode_topshare['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if hhi_dif>0.01:\n",
    "            # An episode is identified\n",
    "            CSA_episodes_impliedHHI_N = CSA_episodes_impliedHHI_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CSA Code':CSA,\n",
    "                'mergers':CSA_affected_episode,\n",
    "                'hhi_dif':hhi_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                'acquiror_market_share_N_max':acquiror_market_share_N_max,\n",
    "                'target_market_share_N_max':target_market_share_N_max,\n",
    "                'other_targets_market_share_N_max':other_targets_market_share_N_max,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CSA_episodes_impliedHHI_N = pd.DataFrame(CSA_episodes_impliedHHI_N)\n",
    "\n",
    "print(len(CSA_episodes_impliedHHI_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42be8c8-7adb-4ff3-9761-ccd8a216bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for summary stats\n",
    "CSA_episodes_impliedHHI_N[['episode_start_year','CSA Code','hhi_dif',\n",
    "    'acquiror_market_share_N_max','target_market_share_N_max','other_targets_market_share_N_max']].\\\n",
    "    to_csv('../CleanData/MAEvent/1B_CSA_episodes_impliedHHI_SumStats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1df796-f8f8-48f8-a085-9c0e611f07ea",
   "metadata": {},
   "source": [
    "### 3.2.3 Method 3: By implied rise in top 5 share due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da1618f9-550d-4647-b68c-2e6f9c6c9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------#\n",
    "# Change in top 5 share by N of deals #\n",
    "#-------------------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CSA_affected\"\n",
    "\n",
    "CSA_episodes_top5share_N = []\n",
    "\n",
    "for CSA in list(CSA_affected['CSA Code'].unique()):\n",
    "\n",
    "    CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==CSA]\n",
    "    CSA_affected_part = CSA_affected_part[\n",
    "        (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in CSA_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CSA_affected_episode = CSA_affected_part[(CSA_affected_part['sale_year']>=sale_year)&(CSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneCSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CSA Code']==CSA)]\n",
    "        \n",
    "        # Calculate (1) Top 5 share (by parent firm) in the three years prior (2) Predicted top 5 share after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCSA_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneCSA_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # Top 5 share prior to merger\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_prior = 1\n",
    "        else:\n",
    "            top5share_prior = np.sum(n_deals['marketshare'][:5])\n",
    "\n",
    "        # Implied top 5 share post merger\n",
    "        CSA_affected_episode = CSA_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in CSA_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_post = 1\n",
    "        else:\n",
    "            top5share_post = np.sum(n_deals['marketshare'][:5])\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        top5share_dif = top5share_post-top5share_prior\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if top5share_dif>0.05:\n",
    "            # An episode is identified\n",
    "            CSA_episodes_top5share_N = CSA_episodes_top5share_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CSA Code':CSA,\n",
    "                'mergers':CSA_affected_episode,\n",
    "                'top5share_dif':top5share_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CSA_episodes_top5share_N = pd.DataFrame(CSA_episodes_top5share_N)\n",
    "\n",
    "print(len(CSA_episodes_top5share_N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88849517-350f-4773-bec1-84d30beb5b94",
   "metadata": {},
   "source": [
    "### 3.2.4 Add characteristics of events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a664fa-4da7-496a-bc5a-f3fc8cbff2f8",
   "metadata": {},
   "source": [
    "Another characteristic, whether there is concurrent CB M&A, is implemented in the stage of sample construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efa6ea-1950-409d-a4d5-6a1abc5292ee",
   "metadata": {},
   "source": [
    "#### 3.2.4.1 Share of CSA within underwriter's business portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "672aa0dd-0f40-40ea-bc7a-c412d4502655",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_files = ['CSA_episodes_marketshare_N','CSA_episodes_impliedHHI_N','CSA_episodes_top5share_N']\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    if episodes_file=='CSA_episodes_marketshare_N':\n",
    "        CSA_episodes = CSA_episodes_marketshare_N.copy()\n",
    "    if episodes_file=='CSA_episodes_impliedHHI_N':\n",
    "        CSA_episodes = CSA_episodes_impliedHHI_N.copy()\n",
    "    if episodes_file=='CSA_episodes_top5share_N':\n",
    "        CSA_episodes = CSA_episodes_top5share_N.copy()\n",
    "\n",
    "    # Add the maximum share of the treated CSA within the acquiror or the target firm's portfolio\n",
    "    CSA_episodes['max_acquiror_weight'] = None\n",
    "    CSA_episodes['max_target_weight'] = None\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        # Weight of acquiror\n",
    "        mergers = row['mergers'].reset_index(drop=True)\n",
    "        mergers['acquiror_weight'] = None\n",
    "        for sub_idx,sub_row in mergers.iterrows():\n",
    "            acquiror_weight = csa_share_withinbank[\n",
    "                (csa_share_withinbank['underwriter']==sub_row['acquiror_parent'])&\n",
    "                (csa_share_withinbank['CSA Code']==row['CSA Code'])&\n",
    "                (csa_share_withinbank['year']==sub_row['sale_year'])]\n",
    "            acquiror_weight = acquiror_weight.reset_index(drop=True)\n",
    "            if len(acquiror_weight):\n",
    "                mergers.loc[sub_idx,'acquiror_weight'] = acquiror_weight['csa_share'][0]\n",
    "        CSA_episodes.at[idx,'max_acquiror_weight'] = np.max(mergers['acquiror_weight'])\n",
    "        # Weight of target\n",
    "        mergers = row['mergers'].reset_index(drop=True)\n",
    "        mergers['target_weight'] = None\n",
    "        for sub_idx,sub_row in mergers.iterrows():\n",
    "            acquiror_weight = csa_share_withinbank[\n",
    "                (csa_share_withinbank['underwriter']==sub_row['target_parent'])&\n",
    "                (csa_share_withinbank['CSA Code']==row['CSA Code'])&\n",
    "                (csa_share_withinbank['year']==sub_row['sale_year'])]\n",
    "            acquiror_weight = acquiror_weight.reset_index(drop=True)\n",
    "            if len(acquiror_weight)>0:\n",
    "                mergers.loc[sub_idx,'target_weight'] = acquiror_weight['csa_share'][0]\n",
    "        CSA_episodes.at[idx,'max_target_weight'] = np.max(mergers['target_weight'])\n",
    "\n",
    "    if episodes_file=='CSA_episodes_marketshare_N':\n",
    "        CSA_episodes_marketshare_N = CSA_episodes.copy()\n",
    "    if episodes_file=='CSA_episodes_impliedHHI_N':\n",
    "        CSA_episodes_impliedHHI_N = CSA_episodes.copy()\n",
    "    if episodes_file=='CSA_episodes_top5share_N':\n",
    "        CSA_episodes_top5share_N = CSA_episodes.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6808e8c-cad9-47d5-a52c-1a1fbe600e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bothweight_lt5 = np.sum((CSA_episodes_impliedHHI_N['max_acquiror_weight']<0.05)&\n",
    "    (CSA_episodes_impliedHHI_N['max_target_weight']<0.05))\n",
    "\n",
    "# Number: Number of M&As where both sides are active #\n",
    "with open('../Draft/nums/n_bothweight_lt5.tex','w') as file:\n",
    "    file.write(str(n_bothweight_lt5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04fe27-5e72-4cd2-a179-4ae265bd55b0",
   "metadata": {},
   "source": [
    "#### 3.2.4.2 Mark out M&As for which stated reason could be related to local economic conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0647db6-ece2-49bd-868f-220aa4db7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_files = ['CSA_episodes_marketshare_N','CSA_episodes_impliedHHI_N','CSA_episodes_top5share_N']\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    if episodes_file=='CSA_episodes_marketshare_N':\n",
    "        CSA_episodes = CSA_episodes_marketshare_N.copy()\n",
    "    if episodes_file=='CSA_episodes_impliedHHI_N':\n",
    "        CSA_episodes = CSA_episodes_impliedHHI_N.copy()\n",
    "    if episodes_file=='CSA_episodes_top5share_N':\n",
    "        CSA_episodes = CSA_episodes_top5share_N.copy()\n",
    "    \n",
    "    #---------------------#\n",
    "    # Whether endogeneous #\n",
    "    #---------------------#\n",
    "    \n",
    "    CSA_episodes['reasonMA_endo_possible'] = \"False\"\n",
    "    \n",
    "    MA_local_reasons = pd.read_csv('SCRIPT_M&A_local_reasons.csv');\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        merged = row['mergers'].merge(MA_local_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_endo_possible'] = \"True\"\n",
    "    \n",
    "    MA_nonfound_reasons = pd.read_csv('SCRIPT_M&A_nonfound_reasons.csv');\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        merged = row['mergers'].merge(MA_nonfound_reasons,on=['target','acquiror'])\n",
    "        if len(merged)==len(row['mergers']):\n",
    "            CSA_episodes.at[idx,'reasonMA_endo_possible'] = \"Unsure\"\n",
    "    \n",
    "    # Number: Number of M&As where both sides are active #\n",
    "    n_reasonMA_endo_not_possible = np.sum(CSA_episodes['reasonMA_endo_possible']==\"False\")\n",
    "    with open('../Draft/nums/n_reasonMA_endo_not_possible.tex','w') as file:\n",
    "        file.write(str(n_reasonMA_endo_not_possible))\n",
    "\n",
    "    MA_all_reasons = pd.read_csv('SCRIPT_M&A_all_reasons.csv');\n",
    "\n",
    "    CSA_episodes['reasonMA_fin_stress_any'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='financial stress']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        merged = row['mergers'].merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_fin_stress_any'] = \"True\"\n",
    "\n",
    "    #------------------------#\n",
    "    # Specific reason of M&A #\n",
    "    #------------------------#\n",
    "\n",
    "    # For these specific reason of M&A, I mark out the reason based on the reason of the most important M&A.\n",
    "    # This way, the heterogeneity by them makes more sense. Otherwise, they are not mutually exclusive, and\n",
    "    # in each category I might end up getting those bigger episodes that involve more M&As, rather than those\n",
    "    # that really reflect a certain specific reason of the M&A.\n",
    "    \n",
    "    MA_all_reasons = pd.read_csv('SCRIPT_M&A_all_reasons.csv');\n",
    "    \n",
    "    CSA_episodes['reasonMA_local_dom'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='local dominance']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_local_dom'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_expand_geo'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='expand geographically']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_expand_geo'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_ind_dom'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='industry dominance']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_ind_dom'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_syn_comb_lines'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='synergy from combining business lines']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_syn_comb_lines'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_fin_stress'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='financial stress']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_fin_stress'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_syn_cost'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='synergy from cost management']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_syn_cost'] = \"True\"\n",
    "    \n",
    "    CSA_episodes['reasonMA_diversify'] = \"False\"\n",
    "    MA_one_reasons = MA_all_reasons[MA_all_reasons['reason']=='diversify revenue sources']\n",
    "    for idx,row in CSA_episodes.iterrows():\n",
    "        mergers = row['mergers']\n",
    "        mergers['min_acquiror_target_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],mergers['target_market_share_N_avg'])\n",
    "        mergers = mergers.sort_values(['min_acquiror_target_share'],ascending=False)\n",
    "        mergers = mergers.reset_index(drop=True)[:1]\n",
    "        merged = mergers.merge(MA_one_reasons,on=['target','acquiror'])\n",
    "        if len(merged)>=1:\n",
    "            CSA_episodes.at[idx,'reasonMA_diversify'] = \"True\"\n",
    "\n",
    "    if episodes_file=='CSA_episodes_marketshare_N':\n",
    "        CSA_episodes_marketshare_N = CSA_episodes.copy()\n",
    "    if episodes_file=='CSA_episodes_impliedHHI_N':\n",
    "        CSA_episodes_impliedHHI_N = CSA_episodes.copy()\n",
    "    if episodes_file=='CSA_episodes_top5share_N':\n",
    "        CSA_episodes_top5share_N = CSA_episodes.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd1d16fc-74ad-447e-9e95-dbfddcd58255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export share of M&As for each reason\n",
    "n_total = len(MA_nonfound_reasons)+len(MA_all_reasons.drop_duplicates(['target','acquiror','year']))\n",
    "\n",
    "with open('../Draft/nums/share_reasonMA_local_dom.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='local dominance')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_expand_geo.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='expand geographically')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_ind_dom.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='industry dominance')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_syn_comb_lines.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='synergy from combining business lines')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_fin_stress.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='financial stress')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_syn_cost.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='synergy from cost management')/n_total*100,1)))\n",
    "with open('../Draft/nums/share_reasonMA_diversify.tex','w') as file:\n",
    "    file.write(str(round(np.sum(MA_all_reasons['reason']=='diversify revenue sources')/n_total*100,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04f0b1-a7b3-4f1c-bd3b-fc52ab8bbd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f20266-a3ec-439b-a336-0409161d4987",
   "metadata": {},
   "source": [
    "### 3.2.5 Export a table of M&As used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ca45815-da8a-4784-967c-6862c53cec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Export a table of M&As used\n",
    "MAs_Used = []\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "    MAs_Used = MAs_Used+[row['mergers'][['acquiror','target','sale_year']]]\n",
    "MAs_Used = pd.concat(MAs_Used).drop_duplicates()\n",
    "MAs_Used = MAs_Used.sort_values(['sale_year','acquiror'])\n",
    "MAs_Used = MAs_Used.reset_index(drop=True)\n",
    "MAs_Used['acquiror'] = MAs_Used['acquiror'].str.replace('&','\\\\&')\n",
    "MAs_Used['target'] = MAs_Used['target'].str.replace('&','\\\\&')\n",
    "\n",
    "# MA_SDC = pd.read_csv('SCRIPT_SDC_deals_cleaned.csv')\n",
    "# MAs_Used = MAs_Used.merge(MA_SDC,on=['target','acquiror','sale_year'],how='outer',indicator=True)\n",
    "# MAs_Used = MAs_Used[MAs_Used['_merge']=='left_only']\n",
    "# MAs_Used = MAs_Used.drop(columns=['_merge'])\n",
    "\n",
    "# Adjusting the format\n",
    "MAs_Used['sale_year'] = MAs_Used['sale_year'].astype(int)\n",
    "MAs_Used = MAs_Used.rename(columns={'acquiror':'Acquiror','target':'Target','sale_year':'Year'})\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('SHEARSONAMERICAN EXPRESS','SHEARSON/AMERICAN EXPRESS')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('SHEARSONAMERICAN EXPRESS','SHEARSON/AMERICAN EXPRESS')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.title()\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.title()\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace(' Of ',' of ')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace(' Of ',' of ')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Ag Becker','AG Becker')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Ag Becker','AG Becker')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace(\"American Ntnl Bank \\\\& Tr\",\"American National Bank \\\\& Trust\",regex=False)\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace(\"American Ntnl Bank \\\\& Tr\",\"American National Bank \\\\& Trust\",regex=False)\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Mcdonald','McDonald')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Mcdonald','McDonald')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Pnc Bank','PNC Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Pnc Bank','PNC Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Us Bank','US Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Us Bank','US Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Onbank','OnBank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Onbank','OnBank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bb\\\\&T','BB\\\\&T',regex=False)\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bb\\\\&T','BB\\\\&T',regex=False)\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bok Financial','BOK Financial')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bok Financial','BOK Financial')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Ubs Financial Services','UBS Financial Services')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Ubs Financial Services','UBS Financial Services')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bb\\\\&T','BB\\\\&T',regex=False)\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bb\\\\&T','BB\\\\&T',regex=False)\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Natwest Bank','NatWest Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Natwest Bank','NatWest Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('West One Bank Oregon Sb','West One Bank Oregon')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('West One Bank Oregon Sb','West One Bank Oregon')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Johnston Brown Barnett\\\\&Knight','Johnston Brown Barnett \\\\& Knight',regex=False)\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Johnston Brown Barnett\\\\&Knight','Johnston Brown Barnett \\\\& Knight',regex=False)\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Jp Morgan','JP Morgan')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Jp Morgan','JP Morgan')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Rbc Bank','RBC Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Rbc Bank','RBC Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Suntrust Bank','SunTrust Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Suntrust Bank','SunTrust Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Rrz Public Markets','RRZ Public Markets')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Rrz Public Markets','RRZ Public Markets')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Interstatejohnson Lane','Interstate/Johnson Lane',regex=False)\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Interstatejohnson Lane','Interstate/Johnson Lane',regex=False)\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Nationsbank','NationsBank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Nationsbank','NationsBank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bankboston','BankBoston')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bankboston','BankBoston')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Nbc Capital Markets Group','NBC Capital Markets Group')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Nbc Capital Markets Group','NBC Capital Markets Group')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Southtrust Securities','SouthTrust Securities')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Southtrust Securities','SouthTrust Securities')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Td Bank','TD Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Td Bank','TD Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Nationsbank','NationsBank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Nationsbank','NationsBank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Seattle Northwest Sec','Seattle Northwest Securities')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Seattle Northwest Sec','Seattle Northwest Securities')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bmo Bank','BMO Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bmo Bank','BMO Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Stonex Group','StoneX Group')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Stonex Group','StoneX Group')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Nbh Bank','NBH Bank')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Nbh Bank','NBH Bank')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Jjb Hilliard Wl Lyons','JJB Hilliard WL Lyons')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Jjb Hilliard Wl Lyons','JJB Hilliard WL Lyons')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Lj Hart','LJ Hart')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Lj Hart','LJ Hart')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Bbva Compass','BBVA Compass')\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Bbva Compass','BBVA Compass')\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Peoples United Bank',\"People's United Bank\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Peoples United Bank',\"People's United Bank\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Peoples National Bank',\"People's National Bank\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Peoples National Bank',\"People's National Bank\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Tgh Securities',\"TGH Securities\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Tgh Securities',\"TGH Securities\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Adams Mcentee',\"Adams McEntee\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Adams Mcentee',\"Adams McEntee\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Mbank Capital Mkts Dallasna',\"MBank Capital Mkts Dallas NA\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Mbank Capital Mkts Dallasna',\"MBank Capital Mkts Dallas NA\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Raffenspergerhughes & Coinc',\"Raffensperger Hughes & Co Inc\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Raffenspergerhughes & Coinc',\"Raffensperger Hughes & Co Inc\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Corestates Bank',\"CoreStates Bank\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Corestates Bank',\"CoreStates Bank\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Piper Sandler',\"Piper Jaffray\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Piper Sandler',\"Piper Jaffray\")\n",
    "MAs_Used['Acquiror'] = MAs_Used['Acquiror'].str.replace('Parkerhunter',\"Parker Hunter\")\n",
    "MAs_Used['Target'] = MAs_Used['Target'].str.replace('Parkerhunter',\"Parker Hunter\")\n",
    "\n",
    "MAs_Used_part1 = MAs_Used[:45]\n",
    "MAs_Used_part2 = MAs_Used[45:90]\n",
    "MAs_Used_part3 = MAs_Used[90:135]\n",
    "MAs_Used_part4 = MAs_Used[135:]\n",
    "\n",
    "latex_table = MAs_Used_part1.style.hide(axis=\"index\").to_latex(hrules=True)\n",
    "with open('../Draft/tabs/MAs_Used_part1.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "latex_table = MAs_Used_part2.style.hide(axis=\"index\").to_latex(hrules=True)\n",
    "with open('../Draft/tabs/MAs_Used_part2.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "latex_table = MAs_Used_part3.style.hide(axis=\"index\").to_latex(hrules=True)\n",
    "with open('../Draft/tabs/MAs_Used_part3.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "latex_table = MAs_Used_part4.style.hide(axis=\"index\").to_latex(hrules=True)\n",
    "with open('../Draft/tabs/MAs_Used_part4.tex', 'w') as f:\n",
    "    f.write(latex_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e788c-8281-4c8d-ae32-f10143210e59",
   "metadata": {},
   "source": [
    "### 3.2.6 Create a plot of map of treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e2389f4-663c-44ce-a435-ddf0e1f70774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "treat_frequency = pd.DataFrame(CSA_episodes_impliedHHI_N.value_counts(['CSA Code'])).reset_index()\n",
    "treat_frequency = treat_frequency.rename(columns={0:'frequency'})\n",
    "\n",
    "CSA_episodes_impliedHHI_N = CSA_episodes_impliedHHI_N.merge(treat_frequency,on=['CSA Code'])\n",
    "\n",
    "# Read the US states GeoJSON file\n",
    "gdf = gpd.read_file('../RawData/MSA/US-counties.geojson')\n",
    "gdf = gdf[gdf['STATE']!='02']\n",
    "gdf = gdf[gdf['STATE']!='15']\n",
    "gdf = gdf[gdf['STATE']!='72']\n",
    "gdf = gdf.rename(columns={'STATE':'FIPS State Code','COUNTY':'FIPS County Code'})\n",
    "gdf['FIPS State Code'] = gdf['FIPS State Code'].astype(int)\n",
    "gdf['FIPS County Code'] = gdf['FIPS County Code'].astype(int)\n",
    "\n",
    "treat_frequency = treat_frequency.merge(CBSAData[['CSA Code','FIPS State Code','FIPS County Code']],\n",
    "    on=['CSA Code'])\n",
    "treat_frequency = gdf.merge(treat_frequency,on=['FIPS State Code','FIPS County Code'],how='outer')\n",
    "treat_frequency.loc[pd.isnull(treat_frequency['frequency']),'frequency'] = 0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "color_dict = {0:'azure', 1:\"#fef0d9\", 2:\"#fdcc8a\", 3:\"#fc8d59\", 4:\"#e34a33\", 5:\"#b30000\"}\n",
    "\n",
    "# Merge the GeoDataFrame with data\n",
    "treat_frequency.plot(ax=ax, column='frequency', cmap=colors.ListedColormap(list(color_dict.values())), edgecolor='0.9', legend=False)\n",
    "legend_labels = [\n",
    "    'Treated 1 Time',\n",
    "    'Treated 2 Times',\n",
    "    'Treated 3 Times',\n",
    "    'Treated 4 Times',\n",
    "    'Treated 5 Times',\n",
    "    ]\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor=\"#fef0d9\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor=\"#fdcc8a\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor=\"#fc8d59\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor=\"#e34a33\"),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor=\"#b30000\"),\n",
    "    ]\n",
    "ax.legend(legend_handles, legend_labels,loc='lower center',bbox_to_anchor=(0.5, -0.15),ncol=3,fontsize='11.5')\n",
    "ax.axis(\"off\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "fig.savefig('../Draft/figs/TreatedFrequency.eps', format='eps', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba143c-f777-4956-8976-8778862c23ae",
   "metadata": {},
   "source": [
    "### 3.2.7 Illustrate the idea of Sunderam and Scharfstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "587591d3-4ed1-4478-8f81-d2bef708ea44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# M&As that are most frequently involved\n",
    "mergers = pd.DataFrame()\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "    mergers = pd.concat([mergers,row['mergers']])\n",
    "mergers_ranked = mergers.value_counts(['acquiror','target','sale_year']).reset_index()\n",
    "\n",
    "# Pick out a case\n",
    "mergers_onecase = mergers[(mergers['acquiror']=='RBC BANK')&(mergers['target']=='DAIN BOSWORTH')]\n",
    "mergers_onecase = mergers_onecase[['CSA Code','sale_year','acquiror','target']]\n",
    "mergers_onecase['acquiror_share'] = None\n",
    "mergers_onecase['target_share'] = None\n",
    "\n",
    "for idx,row in mergers_onecase.iterrows():\n",
    "\n",
    "    acquiror_share = csa_share_withinbank[\n",
    "        (csa_share_withinbank['underwriter']==row['acquiror'])&\n",
    "        (csa_share_withinbank['CSA Code']==row['CSA Code'])&\n",
    "        (csa_share_withinbank['year']<=row['sale_year'])&\n",
    "        (csa_share_withinbank['year']>=row['sale_year']-5)\n",
    "        ].reset_index()\n",
    "    acquiror_share = np.mean(acquiror_share['csa_share'])\n",
    "    mergers_onecase.at[idx,'acquiror_share'] = acquiror_share\n",
    "\n",
    "    target_share = csa_share_withinbank[\n",
    "        (csa_share_withinbank['underwriter']==row['target'])&\n",
    "        (csa_share_withinbank['CSA Code']==row['CSA Code'])&\n",
    "        (csa_share_withinbank['year']<=row['sale_year'])&\n",
    "        (csa_share_withinbank['year']>=row['sale_year']-5)\n",
    "        ].reset_index()\n",
    "    target_share = np.mean(target_share['csa_share'])\n",
    "    mergers_onecase.at[idx,'target_share'] = target_share\n",
    "\n",
    "mergers_onecase = mergers_onecase.sort_values(['acquiror_share','target_share'],ascending=False)\n",
    "\n",
    "# Merge in CSA name\n",
    "CSAs = CBSAData[['CSA Code','CSA Title']].drop_duplicates(['CSA Code'])\n",
    "mergers_onecase = mergers_onecase.merge(CSAs,on=['CSA Code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb7a2530-bf33-4429-87b4-4730a497fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "table = '../Draft/tabs/Sunderam_illustrate.tex'\n",
    "try:\n",
    "    os.remove(table)\n",
    "except:\n",
    "    pass\n",
    "with open(table, 'w') as f:\n",
    "    f.write('\\\\begin{tabular}{lccc} \\n \\\\toprule \\n')\n",
    "    f.write('& Significance of CSA & Significance of CSA \\\\\\\\ \\n')\n",
    "    f.write('CSA & for RBC Bank & for Dain Bosworth \\\\\\\\ \\n')\n",
    "    f.write('\\\\hline \\n \\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][0])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][0]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][0]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][1])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][1]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][1]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][2])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][2]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][2]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][3])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][3]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][3]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][6])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][6]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][6]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write(mergers_onecase['CSA Title'][10])\n",
    "    f.write('&'+str(format(mergers_onecase['acquiror_share'][10]*100,'.1f'))+'\\\\%')\n",
    "    f.write('&'+str(format(mergers_onecase['target_share'][10]*100,'.1f'))+'\\\\%'+'\\\\\\\\ \\n')\n",
    "\n",
    "    f.write('\\\\bottomrule \\n')\n",
    "    f.write('\\\\end{tabular}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666ea79-e656-4796-8919-ac86f05f3faa",
   "metadata": {},
   "source": [
    "### 3.2.8 Mark out if affected market neighbours major markets of merging underwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "44aae06d-f8d9-4299-816a-92738de3643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSA_Neighboring = pd.read_csv('../CleanData/Demographics/CSA_Neighboring.csv')\n",
    "\n",
    "# Add the maximum share of the treated CSA within the acquiror or the target firm's portfolio\n",
    "CSA_episodes_impliedHHI_N['max_acquiror_weight_in_neighbour'] = None\n",
    "CSA_episodes_impliedHHI_N['max_target_weight_in_neighbour'] = None\n",
    "\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Get neighbours\n",
    "    neighbours = list(CSA_Neighboring[CSA_Neighboring['CSA Code A']==row['CSA Code']]['CSA Code B'])\n",
    "\n",
    "    # Weight of acquiror\n",
    "    mergers = row['mergers'].reset_index(drop=True)\n",
    "    mergers['acquiror_weight_in_neighbour'] = None\n",
    "    for sub_idx,sub_row in mergers.iterrows():\n",
    "        acquiror_weight = csa_share_withinbank[\n",
    "            (csa_share_withinbank['underwriter']==sub_row['acquiror_parent'])&\n",
    "            (csa_share_withinbank['CSA Code'].isin(neighbours))&\n",
    "            (csa_share_withinbank['year']==sub_row['sale_year'])]\n",
    "        acquiror_weight = acquiror_weight.reset_index(drop=True)\n",
    "        if len(acquiror_weight):\n",
    "            mergers.loc[sub_idx,'acquiror_weight_in_neighbour'] = np.max(acquiror_weight['csa_share'])\n",
    "    CSA_episodes_impliedHHI_N.at[idx,'max_acquiror_weight_in_neighbour'] = np.max(mergers['acquiror_weight_in_neighbour'])\n",
    "\n",
    "    # Weight of target\n",
    "    mergers = row['mergers'].reset_index(drop=True)\n",
    "    mergers['target_weight_in_neighbour'] = None\n",
    "    for sub_idx,sub_row in mergers.iterrows():\n",
    "        acquiror_weight = csa_share_withinbank[\n",
    "            (csa_share_withinbank['underwriter']==sub_row['target_parent'])&\n",
    "            (csa_share_withinbank['CSA Code'].isin(neighbours))&\n",
    "            (csa_share_withinbank['year']==sub_row['sale_year'])]\n",
    "        acquiror_weight = acquiror_weight.reset_index(drop=True)\n",
    "        if len(acquiror_weight)>0:\n",
    "            mergers.loc[sub_idx,'target_weight_in_neighbour'] = np.max(acquiror_weight['csa_share'])\n",
    "    CSA_episodes_impliedHHI_N.at[idx,'max_target_weight_in_neighbour'] = np.max(mergers['target_weight_in_neighbour'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0cb4c1-ccc5-4def-b5b7-66081eec679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "458b1d95-18d1-4fd5-8b13-7250a19461e2",
   "metadata": {},
   "source": [
    "# 4. Placebo Tests, Using CSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16fdcb-6b4b-497d-90ac-b7d02a53c9f1",
   "metadata": {},
   "source": [
    "## 4.1 Cross-market M&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd14ac-7de6-44af-a64c-e33292f8f4ff",
   "metadata": {},
   "source": [
    "### 4.1.1 Randomly pick one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e21cbc-36e4-40f9-992a-0f7decf9f5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "np.random.seed(314)\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket = []\n",
    "\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Find the \"representative\" single M&A of this episode\n",
    "    mergers = row['mergers']\n",
    "    mergers['min_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],\n",
    "        mergers['target_market_share_N_avg']+\\\n",
    "        mergers['other_targets_market_share_N_avg'])\n",
    "    mergers = mergers.sort_values('min_share')\n",
    "    repre_merger = mergers[-1:].reset_index(drop=True)\n",
    "\n",
    "    # Find a placebo CSA for both treated and control\n",
    "\n",
    "    #---------------------#\n",
    "    # First, for acquiror #\n",
    "    #---------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>10% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['acquiror_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0.1]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket = CSA_episodes_impliedHHI_AcrossMarket+\\\n",
    "                [{'CSA Code':np.random.choice(market_share['CSA Code']),'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "    #------------------#\n",
    "    # Next, for target #\n",
    "    #------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>10% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['target_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0.1]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket = CSA_episodes_impliedHHI_AcrossMarket+\\\n",
    "                [{'CSA Code':np.random.choice(market_share['CSA Code']),'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket = pd.DataFrame(CSA_episodes_impliedHHI_AcrossMarket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc42cc-66a0-40a7-ac1b-53fcd6ca57e4",
   "metadata": {},
   "source": [
    "### 4.1.2 Randomly pick one, without requiring market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016b3322-6914-4382-8085-4bf53e27c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "np.random.seed(314)\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket_AnyShare = []\n",
    "\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Find the \"representative\" single M&A of this episode\n",
    "    mergers = row['mergers']\n",
    "    mergers['min_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],\n",
    "        mergers['target_market_share_N_avg']+\\\n",
    "        mergers['other_targets_market_share_N_avg'])\n",
    "    mergers = mergers.sort_values('min_share')\n",
    "    repre_merger = mergers[-1:].reset_index(drop=True)\n",
    "\n",
    "    # Find a placebo CSA for both treated and control\n",
    "\n",
    "    #---------------------#\n",
    "    # First, for acquiror #\n",
    "    #---------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>0% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['acquiror_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_AnyShare = CSA_episodes_impliedHHI_AcrossMarket_AnyShare+\\\n",
    "                [{'CSA Code':np.random.choice(market_share['CSA Code']),'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "    #------------------#\n",
    "    # Next, for target #\n",
    "    #------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>0% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['target_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_AnyShare = CSA_episodes_impliedHHI_AcrossMarket_AnyShare+\\\n",
    "                [{'CSA Code':np.random.choice(market_share['CSA Code']),'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket_AnyShare = pd.DataFrame(CSA_episodes_impliedHHI_AcrossMarket_AnyShare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465c606-20f4-4f5d-9821-7ad312f57786",
   "metadata": {},
   "source": [
    "### 4.1.3 Pick one with closest population size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75f3eece-906b-4e42-8304-cc5f495f8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket_SamePop = []\n",
    "\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Find the \"representative\" single M&A of this episode\n",
    "    mergers = row['mergers']\n",
    "    mergers['min_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],\n",
    "        mergers['target_market_share_N_avg']+\\\n",
    "        mergers['other_targets_market_share_N_avg'])\n",
    "    mergers = mergers.sort_values('min_share')\n",
    "    repre_merger = mergers[-1:].reset_index(drop=True)\n",
    "\n",
    "    # Find a placebo CSA for both treated and control\n",
    "\n",
    "    #---------------------#\n",
    "    # First, for acquiror #\n",
    "    #---------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>10% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['acquiror_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0.1]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        # Get dif. in population size\n",
    "        market_share = market_share.copy()\n",
    "        market_share['year'] = repre_merger['sale_year'][0]\n",
    "        market_share['focal_CSA'] = row['CSA Code']\n",
    "        market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']],on=['CSA Code','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_candidate'})\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']].\\\n",
    "            rename(columns={'CSA Code':'focal_CSA'}),on=['focal_CSA','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_focal'})\n",
    "        market_share['pop_diff'] = np.absolute(market_share['pop_focal']-market_share['pop_candidate'])\n",
    "        market_share = market_share.sort_values(['pop_diff']).reset_index(drop=True)\n",
    "        # Drop if difference is too big\n",
    "        # market_share['pop_diff_ratio'] = market_share['pop_diff']/market_share['pop_focal']\n",
    "        # market_share = market_share[market_share['pop_diff_ratio']<0.5]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_SamePop = CSA_episodes_impliedHHI_AcrossMarket_SamePop+\\\n",
    "                [{'CSA Code':market_share['CSA Code'][0],'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "    #------------------#\n",
    "    # Next, for target #\n",
    "    #------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>10% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['target_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0.1]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        # Get dif. in population size\n",
    "        market_share = market_share.copy()\n",
    "        market_share['year'] = repre_merger['sale_year'][0]\n",
    "        market_share['focal_CSA'] = row['CSA Code']\n",
    "        market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']],on=['CSA Code','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_candidate'})\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']].\\\n",
    "            rename(columns={'CSA Code':'focal_CSA'}),on=['focal_CSA','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_focal'})\n",
    "        market_share['pop_diff'] = np.absolute(market_share['pop_focal']-market_share['pop_candidate'])\n",
    "        market_share = market_share.sort_values(['pop_diff']).reset_index(drop=True)\n",
    "        # Drop if difference is too big\n",
    "        # market_share['pop_diff_ratio'] = market_share['pop_diff']/market_share['pop_focal']\n",
    "        # market_share = market_share[market_share['pop_diff_ratio']<0.5]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_SamePop = CSA_episodes_impliedHHI_AcrossMarket_SamePop+\\\n",
    "                [{'CSA Code':market_share['CSA Code'][0],'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket_SamePop = pd.DataFrame(CSA_episodes_impliedHHI_AcrossMarket_SamePop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362cc06-339a-4e98-843f-58ea5acb9198",
   "metadata": {},
   "source": [
    "### 4.1.4 Pick one with closest population size, without requiring market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aef599b-35e3-4c6a-a363-13fd0654aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare = []\n",
    "\n",
    "for idx,row in CSA_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Find the \"representative\" single M&A of this episode\n",
    "    mergers = row['mergers']\n",
    "    mergers['min_share'] = np.minimum(mergers['acquiror_market_share_N_avg'],\n",
    "        mergers['target_market_share_N_avg']+\\\n",
    "        mergers['other_targets_market_share_N_avg'])\n",
    "    mergers = mergers.sort_values('min_share')\n",
    "    repre_merger = mergers[-1:].reset_index(drop=True)\n",
    "\n",
    "    # Find a placebo CSA for both treated and control\n",
    "\n",
    "    #---------------------#\n",
    "    # First, for acquiror #\n",
    "    #---------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>0% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['acquiror_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        # Get dif. in population size\n",
    "        market_share = market_share.copy()\n",
    "        market_share['year'] = repre_merger['sale_year'][0]\n",
    "        market_share['focal_CSA'] = row['CSA Code']\n",
    "        market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']],on=['CSA Code','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_candidate'})\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']].\\\n",
    "            rename(columns={'CSA Code':'focal_CSA'}),on=['focal_CSA','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_focal'})\n",
    "        market_share['pop_diff'] = np.absolute(market_share['pop_focal']-market_share['pop_candidate'])\n",
    "        market_share = market_share.sort_values(['pop_diff']).reset_index(drop=True)\n",
    "        # Drop if difference is too big\n",
    "        # market_share['pop_diff_ratio'] = market_share['pop_diff']/market_share['pop_focal']\n",
    "        # market_share = market_share[market_share['pop_diff_ratio']<0.5]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare = CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare+\\\n",
    "                [{'CSA Code':market_share['CSA Code'][0],'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "    #------------------#\n",
    "    # Next, for target #\n",
    "    #------------------#\n",
    "    \n",
    "    # Get other CSAs where involved firm has market share>0% (a flexible threshold)\n",
    "    market_share = market_share_all_markets_byCSA[\n",
    "        (market_share_all_markets_byCSA['parent_name']==repre_merger['target_parent'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']<=repre_merger['sale_year'][0])&\n",
    "        (market_share_all_markets_byCSA['calendar_year']>=repre_merger['sale_year'][0]-3)\n",
    "        ].sort_values(['CSA Code','calendar_year'])\n",
    "    market_share = market_share.groupby(['CSA Code']).agg({'market_share_N':'mean'}).reset_index()\n",
    "    market_share = market_share[market_share['market_share_N']>0]\n",
    "\n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    market_share['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in market_share.iterrows():\n",
    "\n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=repre_merger['sale_year'][0]-1)&\n",
    "            (CSA_affected_part['sale_year']<=repre_merger['sale_year'][0]+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            market_share.at[sub_idx,'if_also_withinMA'] = True\n",
    "\n",
    "    market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "    # Randomly pick such a CSA. If there are too many potential candidates, narrow the thresholds above\n",
    "    if len(market_share)>0:\n",
    "        market_share = market_share[np.logical_not(market_share['if_also_withinMA'])]\n",
    "        # Get dif. in population size\n",
    "        market_share = market_share.copy()\n",
    "        market_share['year'] = repre_merger['sale_year'][0]\n",
    "        market_share['focal_CSA'] = row['CSA Code']\n",
    "        market_share = market_share[market_share['CSA Code']!=row['CSA Code']]\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']],on=['CSA Code','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_candidate'})\n",
    "        market_share = market_share.merge(CSA_POP[['year','CSA Code','pop']].\\\n",
    "            rename(columns={'CSA Code':'focal_CSA'}),on=['focal_CSA','year'])\n",
    "        market_share = market_share.rename(columns={'pop':'pop_focal'})\n",
    "        market_share['pop_diff'] = np.absolute(market_share['pop_focal']-market_share['pop_candidate'])\n",
    "        market_share = market_share.sort_values(['pop_diff']).reset_index(drop=True)\n",
    "        # Drop if difference is too big\n",
    "        # market_share['pop_diff_ratio'] = market_share['pop_diff']/market_share['pop_focal']\n",
    "        # market_share = market_share[market_share['pop_diff_ratio']<0.5]\n",
    "        if len(market_share)>0:\n",
    "            CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare = CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare+\\\n",
    "                [{'CSA Code':market_share['CSA Code'][0],'episode_start_year':repre_merger['sale_year'][0]}]\n",
    "\n",
    "CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare = pd.DataFrame(CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b658b3-a992-41b3-ab21-938755c69abd",
   "metadata": {},
   "source": [
    "## 4.2 Withdrawn M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d59e98-2eba-4674-b7df-e0d0e6858916",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_withdrawn = pd.read_csv(\"../CleanData/SDC/0I_MA_withdrawn.csv\")\n",
    "MA_withdrawn = MA_withdrawn.rename(columns={'announce_year':'sale_year'})[['target','acquiror','sale_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32fe4a-1262-4fe3-b19c-4fb13df57e33",
   "metadata": {},
   "source": [
    "### 4.2.1 Find CSA X Year that could be affected by withdrawn merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aac3e946-3510-4cdd-975f-4d8f0343a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Go over each merger. Check the CSAs affected by the merger (i.e., either side has business in the CSA in the year prior \n",
    "# to the merger). Check if the merger affects just one underwriter or affects multiple underwriters in this CSA.\n",
    "\n",
    "# Note that for the column \"market share of other targets\", the optimal object to put there is the market share of the other target\n",
    "# alone. Here I am instead putting in market share of the other target's parent. This should make a minimal difference.\n",
    "\n",
    "name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "def proc_list(MA_withdrawn_frag):\n",
    "\n",
    "    CSA_affected_withdrawn = []\n",
    "    MA_withdrawn_frag = MA_withdrawn_frag.reset_index(drop=True)\n",
    "    \n",
    "    for idx,row in MA_withdrawn_frag.iterrows():\n",
    "        \n",
    "        # Find CSAs that this merger affects\n",
    "        # Determine if an underwriter is active in an CSA based on activity of PRIOR years\n",
    "        GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "    \n",
    "        # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "        # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "        # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "        other_targets = \\\n",
    "            list(MA_withdrawn[(MA_withdrawn['acquiror']==row['acquiror'])&\n",
    "            (MA_withdrawn['sale_year']==row['sale_year'])&\n",
    "            (MA_withdrawn['target']!=row['target'])]['target'])\n",
    "        \n",
    "        for CSA in list(GPF_prioryears['CSA Code'].unique()):\n",
    "    \n",
    "            GPF_prioryears_oneCSA = GPF_prioryears[GPF_prioryears['CSA Code']==CSA]\n",
    "    \n",
    "            # Underwriters in this state\n",
    "            underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSA[name_GPF_colnames]))))\n",
    "            underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "            underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "            # Parents of underwriters in this state\n",
    "            parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSA[parent_name_colnames]))))\n",
    "            parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "            parents_priorMA = list(set(parents_priorMA))\n",
    "            # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "            subsidiaries_priorMA = list(GPF_names[\n",
    "                (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "    \n",
    "            # Determine if merger affects the CSA, and if both sides have business\n",
    "            IF_acquiror_active = None\n",
    "            IF_target_active = None\n",
    "            IF_other_target_active = None\n",
    "            if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                IF_acquiror_active = True\n",
    "            if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                IF_target_active = True\n",
    "            for other_target in other_targets:\n",
    "                if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                    IF_other_target_active = True\n",
    "    \n",
    "            # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "            # share \"market_share_all_markets_byCSA\" is calculated at the parent level. There are many cases where market share of a\n",
    "            # firm in an area is unavailable, which is because of no presence.\n",
    "    \n",
    "    \n",
    "    \n",
    "            #-------------------------#\n",
    "            # Market share by N deals #\n",
    "            #-------------------------#\n",
    "    \n",
    "            # (1) Market share of acquiror\n",
    "            # Determine parent of target, as \"market_share_all_markets_byCSA\" is at parent level\n",
    "            try:\n",
    "                # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                # Situation where acquiror is a parent\n",
    "                acquiror_parent = row['acquiror']\n",
    "            try:\n",
    "                acquiror_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m1 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m2 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m3 = 0\n",
    "    \n",
    "            # (2) Market share of target\n",
    "            try:\n",
    "                # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                target_parent = row['target']\n",
    "            try:\n",
    "                target_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m1 = 0\n",
    "            try:\n",
    "                target_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m2 = 0\n",
    "            try:\n",
    "                target_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCSA[\n",
    "                    (market_share_all_markets_byCSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                    &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m3 = 0\n",
    "    \n",
    "            # (3) Market share of other targets in the same transaction\n",
    "            # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "            other_targets_parents = \\\n",
    "                list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                list(other_targets)\n",
    "            other_targets_parents = list(set(other_targets_parents))\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-1)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m1 = 0\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-2)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m2 = 0\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCSA[\n",
    "                (market_share_all_markets_byCSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCSA['CSA Code']==CSA)\n",
    "                &(market_share_all_markets_byCSA['calendar_year']==row['sale_year']-3)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m3 = 0\n",
    "    \n",
    "    \n",
    "            # Record data\n",
    "            if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                CSA_affected_withdrawn = CSA_affected_withdrawn+[{\n",
    "                    'CSA Code':CSA,\n",
    "                    'sale_year':row['sale_year'],\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'target':row['target'],\n",
    "                    'other_targets':other_targets,\n",
    "                    'acquiror_parent':acquiror_parent,\n",
    "                    'target_parent':target_parent,\n",
    "                    'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                    'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                    'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                    'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                    'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                    'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                    'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                    'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                    'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                }]\n",
    "            acquiror_market_share_N_m1 = None\n",
    "            acquiror_market_share_N_m2 = None\n",
    "            acquiror_market_share_N_m3 = None\n",
    "            target_market_share_N_m1 = None\n",
    "            target_market_share_N_m2 = None\n",
    "            target_market_share_N_m3 = None\n",
    "            other_targets_market_share = None\n",
    "            other_targets_market_share_N_m1 = None\n",
    "            other_targets_market_share_N_m2 = None\n",
    "            other_targets_market_share_N_m3 = None\n",
    "    \n",
    "    CSA_affected_withdrawn = pd.DataFrame(CSA_affected_withdrawn)\n",
    "        \n",
    "    return CSA_affected_withdrawn\n",
    "\n",
    "MA_withdrawn_dd = dd.from_pandas(MA_withdrawn, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    CSA_affected_withdrawn = MA_withdrawn_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=\n",
    "    ['CSA Code','sale_year','acquiror','target',\n",
    "    'other_targets','acquiror_parent','target_parent',\n",
    "    'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "    'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "    'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "    ])).compute()\n",
    "\n",
    "# Average market share over past three years\n",
    "CSA_affected_withdrawn['acquiror_market_share_N_avg'] = \\\n",
    "    (CSA_affected_withdrawn['acquiror_market_share_N_m1']+\\\n",
    "    CSA_affected_withdrawn['acquiror_market_share_N_m2']+\\\n",
    "    CSA_affected_withdrawn['acquiror_market_share_N_m3'])/3\n",
    "CSA_affected_withdrawn['target_market_share_N_avg'] = \\\n",
    "    (CSA_affected_withdrawn['target_market_share_N_m1']+\\\n",
    "    CSA_affected_withdrawn['target_market_share_N_m2']+\\\n",
    "    CSA_affected_withdrawn['target_market_share_N_m3'])/3\n",
    "CSA_affected_withdrawn['other_targets_market_share_N_avg'] = \\\n",
    "    (CSA_affected_withdrawn['other_targets_market_share_N_m1']+\\\n",
    "    CSA_affected_withdrawn['other_targets_market_share_N_m2']+\\\n",
    "    CSA_affected_withdrawn['other_targets_market_share_N_m3'])/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3519586-958c-4836-a225-c7aeafc24721",
   "metadata": {},
   "source": [
    "### 4.2.2 Construct events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41cf25ce-14c8-4271-aab5-40874c9a17b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "# Withdrawn episodes with Delta HHI of each treshold #\n",
    "#----------------------------------------------------#\n",
    "\n",
    "for threshold in [0.01,0.005,0.003,0.002,0.001]:\n",
    "\n",
    "    # Identify episodes of mergers at the CSA level\n",
    "    \n",
    "    # Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "    # identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "    # the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "    # as in cases where two firms merge into a new one, both will get recorded in \"CSA_affected\"\n",
    "    \n",
    "    CSA_episodes_Withdrawn = []\n",
    "    \n",
    "    for CSA in list(CSA_affected_withdrawn['CSA Code'].unique()):\n",
    "    \n",
    "        CSA_affected_part = CSA_affected_withdrawn[CSA_affected_withdrawn['CSA Code']==CSA]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        \n",
    "        episode_start_year = 1900\n",
    "        for sale_year in CSA_affected_part['sale_year'].unique():\n",
    "        \n",
    "            # If this year is still within the last merger episode\n",
    "            if sale_year<=episode_start_year+4:\n",
    "                continue\n",
    "            \n",
    "            # Check intensity of M&A activities in that year and three years following\n",
    "            CSA_affected_episode = CSA_affected_part[(CSA_affected_part['sale_year']>=sale_year)&(CSA_affected_part['sale_year']<=sale_year+3)]\n",
    "            GPF_oneCSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CSA Code']==CSA)]\n",
    "            \n",
    "            # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "            \n",
    "            # Underwriters in the market\n",
    "            name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCSA_priorMA[parent_name_colnames]))))\n",
    "            name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "            name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "            name_GPFs = list(set(name_GPFs))\n",
    "            n_deals = {}\n",
    "            for item in name_GPFs:\n",
    "                n_deals[item] = 0\n",
    "            \n",
    "            # Record market shares before merger episode\n",
    "            parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "            for idx,row in GPF_oneCSA_priorMA.iterrows():\n",
    "                underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "                n_underwriters = len(underwriters_onedeal)\n",
    "                for item in underwriters_onedeal:\n",
    "                    n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "            n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "            n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "            n_deals_prior = n_deals\n",
    "            \n",
    "            # HHI prior to merger\n",
    "            hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "    \n",
    "            # Implied HHI post merger\n",
    "            CSA_affected_episode = CSA_affected_episode.reset_index(drop=True)\n",
    "            for idx,row in CSA_affected_episode.iterrows():\n",
    "                n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "            n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "            hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "            n_deals_post = n_deals\n",
    "    \n",
    "            hhi_dif = hhi_predicted-hhi_piror\n",
    "    \n",
    "            # Check if market share in the episode is high enough\n",
    "            if hhi_dif>threshold:\n",
    "                # An episode is identified\n",
    "                CSA_episodes_Withdrawn = CSA_episodes_Withdrawn+[{\n",
    "                    'episode_start_year':sale_year,\n",
    "                    'CSA Code':CSA,\n",
    "                    'mergers':CSA_affected_episode,\n",
    "                    'hhi_dif':hhi_dif,\n",
    "                    'n_deals_prior':n_deals_prior,\n",
    "                    'n_deals_post':n_deals_post,\n",
    "                    'acquiror_market_share_N_max':acquiror_market_share_N_max,\n",
    "                    'target_market_share_N_max':target_market_share_N_max,\n",
    "                    'other_targets_market_share_N_max':other_targets_market_share_N_max,\n",
    "                    }]\n",
    "                episode_start_year = sale_year\n",
    "    \n",
    "    CSA_episodes_Withdrawn = pd.DataFrame(CSA_episodes_Withdrawn)\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------#\n",
    "    # Check and rule out if affected by actual M&A #\n",
    "    #----------------------------------------------#\n",
    "    \n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    CSA_episodes_Withdrawn['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in CSA_episodes_Withdrawn.iterrows():\n",
    "    \n",
    "        # M&As in a candidate placebo CSA in [-4,+4]\n",
    "        CSA_affected_part = CSA_affected[CSA_affected['CSA Code']==sub_row['CSA Code']]\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CSA_affected_part = CSA_affected_part[\n",
    "            (CSA_affected_part['sale_year']>=sub_row['episode_start_year']-1)&\n",
    "            (CSA_affected_part['sale_year']<=sub_row['episode_start_year']+5)\n",
    "            ]\n",
    "        CSA_affected_episode = CSA_affected_part.copy()\n",
    "        CSA_affected_episode = CSA_affected_episode[(CSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CSA_affected_episode['target_market_share_N_avg']+CSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CSA_affected_episode)>0:\n",
    "            CSA_episodes_Withdrawn.at[sub_idx,'if_also_withinMA'] = True\n",
    "    \n",
    "    CSA_episodes_Withdrawn = CSA_episodes_Withdrawn[~CSA_episodes_Withdrawn['if_also_withinMA']]\n",
    "\n",
    "    if threshold==0.01:\n",
    "        CSA_episodes_Withdrawn_DeltaHHI100 = CSA_episodes_Withdrawn\n",
    "        # Number: Number of within-market CB M&As #\n",
    "        n_CSA_episodes_Withdrawn_DeltaHHI100 = '{:,}'.format(len(CSA_episodes_Withdrawn_DeltaHHI100))\n",
    "        with open('../Draft/nums/n_CSA_episodes_Withdrawn_DeltaHHI100.tex','w') as file:\n",
    "            file.write(str(n_CSA_episodes_Withdrawn_DeltaHHI100))\n",
    "    if threshold==0.005:\n",
    "        CSA_episodes_Withdrawn_DeltaHHI50 = CSA_episodes_Withdrawn\n",
    "        # Number: Number of within-market CB M&As #\n",
    "        n_CSA_episodes_Withdrawn_DeltaHHI50 = '{:,}'.format(len(CSA_episodes_Withdrawn_DeltaHHI50))\n",
    "        with open('../Draft/nums/n_CSA_episodes_Withdrawn_DeltaHHI50.tex','w') as file:\n",
    "            file.write(str(n_CSA_episodes_Withdrawn_DeltaHHI50))\n",
    "    if threshold==0.003:\n",
    "        CSA_episodes_Withdrawn_DeltaHHI30 = CSA_episodes_Withdrawn\n",
    "        # Number: Number of within-market CB M&As #\n",
    "        n_CSA_episodes_Withdrawn_DeltaHHI30 = '{:,}'.format(len(CSA_episodes_Withdrawn_DeltaHHI30))\n",
    "        with open('../Draft/nums/n_CSA_episodes_Withdrawn_DeltaHHI30.tex','w') as file:\n",
    "            file.write(str(n_CSA_episodes_Withdrawn_DeltaHHI30))\n",
    "    if threshold==0.002:\n",
    "        CSA_episodes_Withdrawn_DeltaHHI20 = CSA_episodes_Withdrawn\n",
    "        # Number: Number of within-market CB M&As #\n",
    "        n_CSA_episodes_Withdrawn_DeltaHHI20 = '{:,}'.format(len(CSA_episodes_Withdrawn_DeltaHHI20))\n",
    "        with open('../Draft/nums/n_CSA_episodes_Withdrawn_DeltaHHI20.tex','w') as file:\n",
    "            file.write(str(n_CSA_episodes_Withdrawn_DeltaHHI20))\n",
    "    if threshold==0.001:\n",
    "        CSA_episodes_Withdrawn_DeltaHHI10 = CSA_episodes_Withdrawn\n",
    "        # Number: Number of within-market CB M&As #\n",
    "        n_CSA_episodes_Withdrawn_DeltaHHI10 = '{:,}'.format(len(CSA_episodes_Withdrawn_DeltaHHI10))\n",
    "        with open('../Draft/nums/n_CSA_episodes_Withdrawn_DeltaHHI10.tex','w') as file:\n",
    "            file.write(str(n_CSA_episodes_Withdrawn_DeltaHHI10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4dbdd9-a96e-42f7-961b-9a9628b52081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a7e9ce-5074-452f-8750-ea1364b639aa",
   "metadata": {},
   "source": [
    "# 5. Assemble a Treatment-Control Matched Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087b34-8033-4e45-abb8-bdd17a9d4fc7",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- There are multiple versions of episodes definiton (by market share or HHI, cutoff on implied HHI increases, etc.). I go over each\n",
    "version here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f35822f-4577-44ae-8420-abc2ea33048e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n"
     ]
    }
   ],
   "source": [
    "print('*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bf3853f3-8e2d-4839-83ed-cbf1147ab35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_files = [\n",
    "    # [\"By Market Share in terms of N deals\",CSA_episodes_marketshare_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_marketshareByN_CountyInc.csv',\n",
    "    # ],\n",
    "    [\"By Implied HHI Increase in terms of N deals, >= 0.01\",CSA_episodes_impliedHHI_N,1,\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_Quant.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_GovFin.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_IncPop.csv',\n",
    "        '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    ],\n",
    "    # [\"By Implied Top 5 Share Increase in terms of N deals, >= 0.01\",CSA_episodes_top5share_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_top5shareByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, two match\",CSA_episodes_impliedHHI_N,2,\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_TwoMatch_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, three match\",CSA_episodes_impliedHHI_N,3,\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_ThreeMatch_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Cross-market M&A, By Implied HHI Increase in terms of N deals, >= 0.01\",CSA_episodes_impliedHHI_AcrossMarket,1,\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Cross-market M&A, By Implied HHI Increase in terms of N deals, >= 0.01\",CSA_episodes_impliedHHI_AcrossMarket_AnyShare,1,\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_AnyShare_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Cross-market M&A, By Implied HHI Increase in terms of N deals, >= 0.01\",CSA_episodes_impliedHHI_AcrossMarket_SamePop,1,\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Cross-market M&A, By Implied HHI Increase in terms of N deals, >= 0.01\",CSA_episodes_impliedHHI_AcrossMarket_SamePop_AnyShare,1,\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AcrossMarket_SamePop_AnyShare_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Withdrawn M&A, >= 0.01\",CSA_episodes_Withdrawn_DeltaHHI100,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI100_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Withdrawn M&A, >= 0.005\",CSA_episodes_Withdrawn_DeltaHHI50,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI50_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Withdrawn M&A, >= 0.003\",CSA_episodes_Withdrawn_DeltaHHI20,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI30_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Withdrawn M&A, >= 0.002\",CSA_episodes_Withdrawn_DeltaHHI20,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI20_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"Withdrawn M&A, >= 0.001\",CSA_episodes_Withdrawn_DeltaHHI10,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Withdrawn_DeltaHHI10_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, match on both level and dynamics of demographics\",CSA_episodes_impliedHHI_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Dynamics_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, match on outcome variables\",CSA_episodes_impliedHHI_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_Outcome_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, exclude cases confounded by CB M&A based on 0.01 CB HHI\",CSA_episodes_impliedHHI_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound100_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, exclude cases confounded by CB M&A based on 0.005 CB HHI\",CSA_episodes_impliedHHI_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_excludeCBConfound50_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, use all non-treated CSAs as matches\",CSA_episodes_impliedHHI_N,1000,\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_AllAsControl_episodes_impliedHHIByN_CountyInc.csv',\n",
    "    # ],\n",
    "    # [\"By Implied HHI Increase in terms of N deals, >= 0.01, require control to be never treated\",CSA_episodes_impliedHHI_N,1,\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_bondlevel.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_Quant.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_Quant_GeneralUse.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_Quant_IssuerType.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_Quant_Bid.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_GovFin.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_IncPop.csv',\n",
    "    #     '../CleanData/MAEvent/CSA_episodes_impliedHHIByN_ControlNeverTreated_CountyInc.csv',\n",
    "    # ],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81923d44-d3fb-4545-9eb3-092f6e69d650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be732d0-fd8e-43ea-a1bb-1c4dfec5d69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4e658b21-63cf-4e91-8bd2-f28ac891a541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 1 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    criteria = episodes_file[0]\n",
    "    episodes = episodes_file[1]\n",
    "    N_matches = episodes_file[2]\n",
    "    file_path = episodes_file[3]\n",
    "    file_path_bondlevel = episodes_file[4]\n",
    "    file_path_Quant = episodes_file[5]\n",
    "    file_path_Quant_GeneralUse = episodes_file[6]\n",
    "    file_path_Quant_IssuerType = episodes_file[7]\n",
    "    file_path_Quant_Bid = episodes_file[8]\n",
    "    file_path_GovFin = episodes_file[9]\n",
    "    file_path_IncPop = episodes_file[10]\n",
    "    file_path_CountyInc = episodes_file[11]\n",
    "\n",
    "    episodes = episodes.copy()\n",
    "\n",
    "\n",
    "\n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "    CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "    CSA_Data = CSA_POP.merge(CSA_INC,on=['CSA Code','year'])\n",
    "    CSA_Data = CSA_Data[['CSA Code','year','inc','pop','inc_inc_rate','pop_inc_rate']]\n",
    "    Same_State_CSA_pairs = pd.read_csv(\"../CleanData/Demographics/0C_Same_State_CSA_pairs.csv\")\n",
    "\n",
    "    #-------------------#\n",
    "    # Baseline matching #\n",
    "    #-------------------#\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CSA\n",
    "        CSA_Data_oneyear = CSA_Data[CSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CSA\n",
    "        CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "        if len(CSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "        # Remove other CSAs in the same state from potential matches\n",
    "        Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "\n",
    "        # A version of sample that exclude cases confounded by CB M&A. I require both treated and control firm\n",
    "        # to be not affected. Here I work on the requirement for control. Very soon, will remove all such potentially\n",
    "        # confounded cases from the treated firm as well\n",
    "        if \"exclude cases confounded by CB M&A\" in criteria:\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear.merge(Delta_CB_HHI,on=['CSA Code','year'])\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_m1']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_m1']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_m2']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_m2']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_m3']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_m3']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_m4']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_m4']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_p1']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_p1']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_p2']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_p2']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_p3']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_p3']))&\n",
    "                ((CSA_Data_oneyear['CB_hhi_dif_p4']<0.01)|pd.isnull(CSA_Data_oneyear['CB_hhi_dif_p4']))\n",
    "                ]\n",
    "\n",
    "        # A version of sample where I address the critique in Baker et al (2022) that estimates can be biased if \n",
    "        # previously treated units act as control\n",
    "        if \"require control to be never treated\" in criteria:\n",
    "            episodes_select_columns = episodes[['CSA Code','episode_start_year']]\n",
    "            episodes_select_columns = episodes_select_columns[episodes_select_columns['episode_start_year']<row['episode_start_year']]\n",
    "            previous_treated = list(episodes_select_columns['CSA Code'])\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(previous_treated)]\n",
    "\n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==subrow['CSA Code']]\n",
    "            CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "            # Exclude those that are treated themselves as controls\n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "\n",
    "    #-----------------------------------------------------------#\n",
    "    # Matching on both level and dynamics of local demographics #\n",
    "    #-----------------------------------------------------------#\n",
    "\n",
    "    if \"match on both level and dynamics of demographics\" in criteria:\n",
    "        \n",
    "        def calculate_distance(row,weightingmat):\n",
    "            return sp.spatial.distance.mahalanobis((row['inc'],row['pop'],row['inc_inc_rate'],row['pop_inc_rate']),\\\n",
    "                (row['treated_inc'],row['treated_pop'],row['treated_inc_inc_rate'],row['treated_pop_inc_rate']),weightingmat)\n",
    "        \n",
    "        episodes['control'] = None\n",
    "        for idx,row in episodes.iterrows():\n",
    "\n",
    "            # Income data is unavailable in recent years and is imputed\n",
    "            if row['episode_start_year']>=2020:\n",
    "                episode_start_year = 2020\n",
    "            else:\n",
    "                episode_start_year = row['episode_start_year']\n",
    "            # Find population of this CSA\n",
    "            CSA_Data_oneyear = CSA_Data[CSA_Data['year']==episode_start_year].copy()\n",
    "        \n",
    "            # Demographic data of the treated CSA\n",
    "            CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "            if len(CSA_Data_oneyear_frag)==0:\n",
    "                continue\n",
    "            episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "            episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "            episode_pop_inc_rate = CSA_Data_oneyear_frag.reset_index()['pop_inc_rate'][0]\n",
    "            episode_inc_inc_rate = CSA_Data_oneyear_frag.reset_index()['inc_inc_rate'][0]\n",
    "\n",
    "            # Find a match\n",
    "            CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "            CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "            CSA_Data_oneyear['treated_pop_inc_rate'] = episode_pop_inc_rate\n",
    "            CSA_Data_oneyear['treated_inc_inc_rate'] = episode_inc_inc_rate\n",
    "            # Get weighting matrix\n",
    "            CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['inc_inc_rate'] = winsor2(CSA_Data_oneyear['inc_inc_rate'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['pop_inc_rate'] = winsor2(CSA_Data_oneyear['pop_inc_rate'],cutoffs=[0.05,0.05])\n",
    "            cov = CSA_Data_oneyear[['inc','pop','inc_inc_rate','pop_inc_rate']].cov()\n",
    "            invcov = np.linalg.inv(cov)\n",
    "            CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "            # Remove oneself from potential matches\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "            # Remove other CSAs in the same state from potential matches\n",
    "            Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "        \n",
    "            match_counter = 0\n",
    "            control = []\n",
    "            for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "                # Years for which potential control is treated itself\n",
    "                CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==subrow['CSA Code']]\n",
    "                CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                    (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "                CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "                # Exclude those that are treated themselves as controls\n",
    "                if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                    intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "                    # This potential control is treated\n",
    "                    continue\n",
    "                else:\n",
    "                    # This potential control is not treated => Good control\n",
    "                    control = control+[subrow['CSA Code']]\n",
    "                    match_counter = match_counter+1\n",
    "                    if match_counter==N_matches:\n",
    "                        break\n",
    "        \n",
    "            episodes.at[idx,'control'] = control\n",
    "\n",
    "    #-------------------------------#\n",
    "    # Matching on outcome variables #\n",
    "    #-------------------------------#\n",
    "\n",
    "    if \"match on outcome variables\" in criteria:\n",
    "\n",
    "        CSA_Data = CSA_Data.merge(CSACharsForMatch.rename(columns={'sale_year':'year'}),on=['CSA Code','year'],how='outer')\n",
    "\n",
    "        def calculate_distance(row,weightingmat):\n",
    "            return sp.spatial.distance.mahalanobis((row['inc'],row['pop'],row['gross_spread'],row['avg_yield']),\\\n",
    "                (row['treated_inc'],row['treated_pop'],row['treated_gross_spread'],row['treated_avg_yield']),weightingmat)\n",
    "        \n",
    "        episodes['control'] = None\n",
    "        for idx,row in episodes.iterrows():\n",
    "\n",
    "            # Income data is unavailable in recent years and is imputed\n",
    "            if row['episode_start_year']>=2020:\n",
    "                episode_start_year = 2020\n",
    "            else:\n",
    "                episode_start_year = row['episode_start_year']\n",
    "            # Find population of this CSA\n",
    "            CSA_Data_oneyear = CSA_Data[CSA_Data['year']==episode_start_year].copy()\n",
    "        \n",
    "            # Demographic data of the treated CSA\n",
    "            CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "            if len(CSA_Data_oneyear_frag)==0:\n",
    "                continue\n",
    "            episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "            episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "            episode_gross_spread = CSA_Data_oneyear_frag.reset_index()['gross_spread'][0]\n",
    "            episode_avg_yield = CSA_Data_oneyear_frag.reset_index()['avg_yield'][0]\n",
    "\n",
    "            # Find a match\n",
    "            CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "            CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "            CSA_Data_oneyear['treated_gross_spread'] = episode_gross_spread\n",
    "            CSA_Data_oneyear['treated_avg_yield'] = episode_avg_yield\n",
    "            # Get weighting matrix\n",
    "            # If data is missing which hinders construction of weighting matrix, skip\n",
    "            if np.sum(~pd.isnull(CSA_Data_oneyear['inc']))==0:\n",
    "                continue\n",
    "            if np.sum(~pd.isnull(CSA_Data_oneyear['pop']))==0:\n",
    "                continue\n",
    "            if np.sum(~pd.isnull(CSA_Data_oneyear['gross_spread']))==0:\n",
    "                continue\n",
    "            if np.sum(~pd.isnull(CSA_Data_oneyear['avg_yield']))==0:\n",
    "                continue\n",
    "            CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['gross_spread'] = winsor2(CSA_Data_oneyear['gross_spread'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['avg_yield'] = winsor2(CSA_Data_oneyear['avg_yield'],cutoffs=[0.05,0.05])\n",
    "            cov = CSA_Data_oneyear[['inc','pop','gross_spread','avg_yield']].cov()\n",
    "            invcov = np.linalg.inv(cov)\n",
    "            CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "            # Remove oneself from potential matches\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "            # Remove other CSAs in the same state from potential matches\n",
    "            Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "        \n",
    "            match_counter = 0\n",
    "            control = []\n",
    "            for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "                # Years for which potential control is treated itself\n",
    "                CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==subrow['CSA Code']]\n",
    "                CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                    (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "                CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "                # \n",
    "                if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                    intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "                    # This potential control is treated\n",
    "                    continue\n",
    "                else:\n",
    "                    # This potential control is not treated => Good control\n",
    "                    control = control+[subrow['CSA Code']]\n",
    "                    match_counter = match_counter+1\n",
    "                    if match_counter==N_matches:\n",
    "                        break\n",
    "        \n",
    "            episodes.at[idx,'control'] = control\n",
    "\n",
    "    #-------------------------------------------------------------#\n",
    "    # A version of sample that exclude cases confounded by CB M&A #\n",
    "    #-------------------------------------------------------------#\n",
    "\n",
    "    if \"exclude cases confounded by CB M&A based on 0.01 CB HHI\" in criteria:\n",
    "    \n",
    "        Delta_CB_HHI_renamed = Delta_CB_HHI.rename(columns={'year':'episode_start_year'})\n",
    "        episodes = episodes.merge(Delta_CB_HHI_renamed,on=['CSA Code','episode_start_year'])\n",
    "        episodes = episodes[\n",
    "            ((episodes['CB_hhi_dif']<0.01)|pd.isnull(episodes['CB_hhi_dif']))&\n",
    "            ((episodes['CB_hhi_dif_m1']<0.01)|pd.isnull(episodes['CB_hhi_dif_m1']))&\n",
    "            ((episodes['CB_hhi_dif_m2']<0.01)|pd.isnull(episodes['CB_hhi_dif_m2']))&\n",
    "            ((episodes['CB_hhi_dif_m3']<0.01)|pd.isnull(episodes['CB_hhi_dif_m3']))&\n",
    "            ((episodes['CB_hhi_dif_m4']<0.01)|pd.isnull(episodes['CB_hhi_dif_m4']))&\n",
    "            ((episodes['CB_hhi_dif_p1']<0.01)|pd.isnull(episodes['CB_hhi_dif_p1']))&\n",
    "            ((episodes['CB_hhi_dif_p2']<0.01)|pd.isnull(episodes['CB_hhi_dif_p2']))&\n",
    "            ((episodes['CB_hhi_dif_p3']<0.01)|pd.isnull(episodes['CB_hhi_dif_p3']))&\n",
    "            ((episodes['CB_hhi_dif_p4']<0.01)|pd.isnull(episodes['CB_hhi_dif_p4']))\n",
    "            ]\n",
    "\n",
    "    if \"exclude cases confounded by CB M&A based on 0.005 CB HHI\" in criteria:\n",
    "    \n",
    "        Delta_CB_HHI_renamed = Delta_CB_HHI.rename(columns={'year':'episode_start_year'})\n",
    "        episodes = episodes.merge(Delta_CB_HHI_renamed,on=['CSA Code','episode_start_year'])\n",
    "        episodes = episodes[\n",
    "            ((episodes['CB_hhi_dif']<0.005)|pd.isnull(episodes['CB_hhi_dif']))&\n",
    "            ((episodes['CB_hhi_dif_m1']<0.005)|pd.isnull(episodes['CB_hhi_dif_m1']))&\n",
    "            ((episodes['CB_hhi_dif_m2']<0.005)|pd.isnull(episodes['CB_hhi_dif_m2']))&\n",
    "            ((episodes['CB_hhi_dif_m3']<0.005)|pd.isnull(episodes['CB_hhi_dif_m3']))&\n",
    "            ((episodes['CB_hhi_dif_m4']<0.005)|pd.isnull(episodes['CB_hhi_dif_m4']))&\n",
    "            ((episodes['CB_hhi_dif_p1']<0.005)|pd.isnull(episodes['CB_hhi_dif_p1']))&\n",
    "            ((episodes['CB_hhi_dif_p2']<0.005)|pd.isnull(episodes['CB_hhi_dif_p2']))&\n",
    "            ((episodes['CB_hhi_dif_p3']<0.005)|pd.isnull(episodes['CB_hhi_dif_p3']))&\n",
    "            ((episodes['CB_hhi_dif_p4']<0.005)|pd.isnull(episodes['CB_hhi_dif_p4']))\n",
    "            ]\n",
    "\n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "    # Update the \"episodes\" files with match information\n",
    "    if episodes_file[0]==\"By Market Share in terms of N deals\":\n",
    "        CSA_episodes_marketshare_N = episodes.copy()\n",
    "    if episodes_file[0]==\"By Implied HHI Increase in terms of N deals, >= 0.01\":\n",
    "        CSA_episodes_impliedHHI_N = episodes.copy()\n",
    "\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,11))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "\n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "\n",
    "        if 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "\n",
    "        if 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "\n",
    "        if 'top5share_dif' in episodes_Exploded.columns:\n",
    "            top5share_dif = row['top5share_dif']\n",
    "        else:\n",
    "            top5share_dif = None\n",
    "\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "\n",
    "        # Event characteristics - importance for merging firms\n",
    "        if 'max_acquiror_weight' in episodes_Exploded.columns:\n",
    "            max_acquiror_weight = row['max_acquiror_weight']\n",
    "        else:\n",
    "            max_acquiror_weight = None\n",
    "\n",
    "        if 'max_target_weight' in episodes_Exploded.columns:\n",
    "            max_target_weight = row['max_target_weight']\n",
    "        else:\n",
    "            max_target_weight = None\n",
    "\n",
    "        # Event characteristics - importance of neighbouring CSAs for merging firms\n",
    "        if 'max_acquiror_weight_in_neighbour' in episodes_Exploded.columns:\n",
    "            max_acquiror_weight_in_neighbour = row['max_acquiror_weight_in_neighbour']\n",
    "        else:\n",
    "            max_acquiror_weight_in_neighbour = None\n",
    "\n",
    "        if 'max_target_weight_in_neighbour' in episodes_Exploded.columns:\n",
    "            max_target_weight_in_neighbour = row['max_target_weight_in_neighbour']\n",
    "        else:\n",
    "            max_target_weight_in_neighbour = None\n",
    "\n",
    "        # Event characteristics - driving reasons\n",
    "        if 'reasonMA_endo_possible' in episodes_Exploded.columns:\n",
    "            reasonMA_endo_possible = row['reasonMA_endo_possible']\n",
    "        else:\n",
    "            reasonMA_endo_possible = None\n",
    "\n",
    "        if 'reasonMA_local_dom' in episodes_Exploded.columns:\n",
    "            reasonMA_local_dom = row['reasonMA_local_dom']\n",
    "        else:\n",
    "            reasonMA_local_dom = None\n",
    "        if 'reasonMA_expand_geo' in episodes_Exploded.columns:\n",
    "            reasonMA_expand_geo = row['reasonMA_expand_geo']\n",
    "        else:\n",
    "            reasonMA_expand_geo = None\n",
    "        if 'reasonMA_ind_dom' in episodes_Exploded.columns:\n",
    "            reasonMA_ind_dom = row['reasonMA_ind_dom']\n",
    "        else:\n",
    "            reasonMA_ind_dom = None\n",
    "        if 'reasonMA_syn_comb_lines' in episodes_Exploded.columns:\n",
    "            reasonMA_syn_comb_lines = row['reasonMA_syn_comb_lines']\n",
    "        else:\n",
    "            reasonMA_syn_comb_lines = None\n",
    "        if 'reasonMA_fin_stress' in episodes_Exploded.columns:\n",
    "            reasonMA_fin_stress = row['reasonMA_fin_stress']\n",
    "        else:\n",
    "            reasonMA_fin_stress = None\n",
    "        if 'reasonMA_syn_cost' in episodes_Exploded.columns:\n",
    "            reasonMA_syn_cost = row['reasonMA_syn_cost']\n",
    "        else:\n",
    "            reasonMA_syn_cost = None\n",
    "        if 'reasonMA_diversify' in episodes_Exploded.columns:\n",
    "            reasonMA_diversify = row['reasonMA_diversify']\n",
    "        else:\n",
    "            reasonMA_diversify = None\n",
    "\n",
    "        # Frequency of treatment throughout the sample period\n",
    "        if 'frequency' in episodes_Exploded.columns:\n",
    "            frequency = row['frequency']\n",
    "        else:\n",
    "            frequency = None\n",
    "\n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==row['CSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CSA Code','sale_year','State','County',\n",
    "            'issuer_type','Issuer',\n",
    "            'avg_maturity','amount',\n",
    "            'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "            'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "            'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "            'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "            'underpricing_15to60','underpricing_15to30',\n",
    "            'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "            'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "            'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "            'if_callable','CB_Eligible',\n",
    "            'num_relationship','TBB_n_bidders',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        #------------------------------------#\n",
    "        # Some cross-sectional heterogeneity #\n",
    "        #------------------------------------#\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers\n",
    "        mergers = CSA_affected[\n",
    "            (CSA_affected['CSA Code']==row['CSA Code'])&\n",
    "            (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        \n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        #---------------------------------------------------------------------#\n",
    "        # Whether any of the merging banks is an expert in a certain subfield #\n",
    "        #---------------------------------------------------------------------#\n",
    "\n",
    "        if (\"Cross-market\" in criteria) or (\"Withdrawn\" in criteria) or (\"all non-treated CSAs as matches\" in criteria):\n",
    "            x = 1 # Do nothing\n",
    "        else:\n",
    "            if_merging_banks_expert_US = {}\n",
    "            if_merging_banks_expert_CSA = {}\n",
    "            \n",
    "            # Initialize the dictionary\n",
    "            for sorting_var in ['Bid','amount_bracket','mat_bracket','use_short','has_ratings']:\n",
    "                if sorting_var=='Bid':\n",
    "                    categories = ['N','C','P']\n",
    "                if sorting_var=='amount_bracket':\n",
    "                    categories = ['small','med','large','mega']\n",
    "                if sorting_var=='mat_bracket':\n",
    "                    categories = ['short','med','long']\n",
    "                if sorting_var=='use_short':\n",
    "                    categories = ['gp','edu','util','house','health','ed','tsp','pollute']\n",
    "                if sorting_var=='has_ratings':\n",
    "                    categories = ['False','True']\n",
    "                for category in categories:\n",
    "                    for threshold in ['5','10','25','50']:\n",
    "                        if_merging_banks_expert_US['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category] = False\n",
    "                        if_merging_banks_expert_CSA['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category] = False\n",
    "            \n",
    "            # As an episode involves many banks, if in any merger both sides are experts, set indicator to 1\n",
    "            mergers = row['mergers']\n",
    "            for sub_idx,sub_row in mergers.iterrows():\n",
    "    \n",
    "                RankBankWithinCategoryUS_acquiror = pd.DataFrame()\n",
    "                try:\n",
    "                    RankBankWithinCategoryUS_acquiror = RankBankWithinCategoryUS_gb.get_group(sub_row['acquiror'])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "                RankBankWithinCategoryUS_target = pd.DataFrame()\n",
    "                try:\n",
    "                    RankBankWithinCategoryUS_target = RankBankWithinCategoryUS_gb.get_group(sub_row['target'])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "                RankBankWithinCategoryCSA_acquiror = pd.DataFrame()\n",
    "                try:\n",
    "                    RankBankWithinCategoryCSA_acquiror = RankBankWithinCategoryCSA_gb.get_group(sub_row['acquiror'])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "                RankBankWithinCategoryCSA_target = pd.DataFrame()\n",
    "                try:\n",
    "                    RankBankWithinCategoryCSA_target = RankBankWithinCategoryCSA_gb.get_group(sub_row['target'])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "                for sorting_var in ['Bid','amount_bracket','mat_bracket','use_short','has_ratings']:\n",
    "    \n",
    "                    if sorting_var=='Bid':\n",
    "                        categories = ['N','C','P']\n",
    "                    if sorting_var=='amount_bracket':\n",
    "                        categories = ['small','med','large','mega']\n",
    "                    if sorting_var=='mat_bracket':\n",
    "                        categories = ['short','med','long']\n",
    "                    if sorting_var=='use_short':\n",
    "                        categories = ['gp','edu','util','house','health','ed','tsp','pollute']\n",
    "                    if sorting_var=='has_ratings':\n",
    "                        categories = ['False','True']\n",
    "    \n",
    "                    for category in categories:\n",
    "                        if len(RankBankWithinCategoryUS_acquiror)>0 and len(RankBankWithinCategoryUS_target)>0:\n",
    "                            for threshold in ['5','10','25','50']:\n",
    "                                if \\\n",
    "                                if_merging_banks_expert_US['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category]:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if_acquiror_bank_top = \\\n",
    "                                        np.any(RankBankWithinCategoryUS_acquiror[ \\\n",
    "                                        (RankBankWithinCategoryUS_acquiror['sale_year']==sub_row['sale_year'])]\\\n",
    "                                        ['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category])\n",
    "                                    if_target_bank_top = \\\n",
    "                                        np.any(RankBankWithinCategoryUS_target[\n",
    "                                        (RankBankWithinCategoryUS_target['sale_year']==sub_row['sale_year'])]\\\n",
    "                                        ['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category])\n",
    "                                    if_merging_banks_top = if_acquiror_bank_top and if_target_bank_top\n",
    "                                    if_merging_banks_expert_US['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category] = \\\n",
    "                                        if_merging_banks_top\n",
    "    \n",
    "                        if len(RankBankWithinCategoryCSA_acquiror)>0 and len(RankBankWithinCategoryCSA_target)>0:\n",
    "                            for threshold in ['5','10','25','50']:\n",
    "                                if \\\n",
    "                                if_merging_banks_expert_CSA['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category]:\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    if_acquiror_bank_top = \\\n",
    "                                        np.any(RankBankWithinCategoryCSA_acquiror[\n",
    "                                        (RankBankWithinCategoryCSA_acquiror['CSA Code']==row['CSA Code'])& \\\n",
    "                                        (RankBankWithinCategoryCSA_acquiror['sale_year']==sub_row['sale_year'])]\\\n",
    "                                        ['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category])\n",
    "                                    if_target_bank_top = \\\n",
    "                                        np.any(RankBankWithinCategoryCSA_target[\n",
    "                                        (RankBankWithinCategoryCSA_target['CSA Code']==row['CSA Code'])& \\\n",
    "                                        (RankBankWithinCategoryCSA_target['sale_year']==sub_row['sale_year'])]\\\n",
    "                                        ['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category])\n",
    "                                    if_merging_banks_top = if_acquiror_bank_top and if_target_bank_top\n",
    "                                    if_merging_banks_expert_CSA['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category] = \\\n",
    "                                        if_merging_banks_top\n",
    "    \n",
    "            # Whether the merging underwriter is an expert in underwriting for the specific issue, using multiple ways of\n",
    "            # dividing expertise\n",
    "            for sorting_var in ['Bid','amount_bracket','mat_bracket','use_short','has_ratings']:\n",
    "                if sorting_var=='Bid':\n",
    "                    categories = ['N','C','P']\n",
    "                if sorting_var=='amount_bracket':\n",
    "                    categories = ['small','med','large','mega']\n",
    "                if sorting_var=='mat_bracket':\n",
    "                    categories = ['short','med','long']\n",
    "                if sorting_var=='use_short':\n",
    "                    categories = ['gp','edu','util','house','health','ed','tsp','pollute']\n",
    "                if sorting_var=='has_ratings':\n",
    "                    categories = ['False','True']\n",
    "                for threshold in ['5','10','25','50']:\n",
    "                    GPF_Seg['if_US_expert_'+sorting_var+'_'+threshold] = False\n",
    "                    for category in categories:\n",
    "                        if \\\n",
    "                        if_merging_banks_expert_US['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category]:\n",
    "                            GPF_Seg.loc[GPF_Seg[sorting_var]==category,'if_US_expert_'+sorting_var+'_'+threshold] = True\n",
    "                    GPF_Seg['if_CSA_expert_'+sorting_var+'_'+threshold] = False\n",
    "                    for category in categories:\n",
    "                        if \\\n",
    "                        if_merging_banks_expert_CSA['BankAttribute_top'+threshold+'_'+sorting_var+'_'+category]:\n",
    "                            GPF_Seg.loc[GPF_Seg[sorting_var]==category,'if_CSA_expert_'+sorting_var+'_'+threshold] = True\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['treated_csa'] = row['CSA Code'] # Used for constructing cohort X issuer FEs\n",
    "        # Event characteristics - strength\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg['top5share_dif'] = top5share_dif\n",
    "        # Event characteristics - importance for merging firms\n",
    "        GPF_Seg['max_acquiror_weight'] = max_acquiror_weight\n",
    "        GPF_Seg['max_target_weight'] = max_target_weight\n",
    "        GPF_Seg['max_acquiror_weight_in_neighbour'] = max_acquiror_weight_in_neighbour\n",
    "        GPF_Seg['max_target_weight_in_neighbour'] = max_target_weight_in_neighbour\n",
    "        # Event characteristics - driving reasons\n",
    "        GPF_Seg['reasonMA_endo_possible'] = reasonMA_endo_possible\n",
    "        GPF_Seg['reasonMA_local_dom'] = reasonMA_local_dom\n",
    "        GPF_Seg['reasonMA_expand_geo'] = reasonMA_expand_geo\n",
    "        GPF_Seg['reasonMA_ind_dom'] = reasonMA_ind_dom\n",
    "        GPF_Seg['reasonMA_syn_comb_lines'] = reasonMA_syn_comb_lines\n",
    "        GPF_Seg['reasonMA_fin_stress'] = reasonMA_fin_stress\n",
    "        GPF_Seg['reasonMA_syn_cost'] = reasonMA_syn_cost\n",
    "        GPF_Seg['reasonMA_diversify'] = reasonMA_diversify\n",
    "        GPF_Seg['frequency'] = frequency\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # For the sample with all non-treated as control, restrict variables to limit file size\n",
    "        if \"use all non-treated CSAs as matches\" in criteria:\n",
    "            GPF_Seg_Treated = GPF_Seg_Treated[[\n",
    "                'CSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'treated','episode_start_year','year_to_merger','calendar_year','treated_csa',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                ]]\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship','TBB_n_bidders',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            if (\"Cross-market\" in criteria) or (\"Withdrawn\" in criteria)  or (\"all non-treated CSAs as matches\" in criteria):\n",
    "                x = 1 # Do nothing\n",
    "            else:\n",
    "                # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the control areas\n",
    "                mergers = CSA_affected[\n",
    "                    (CSA_affected['CSA Code']==item)&\n",
    "                    (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                    (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                    ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                    'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "                mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "    \n",
    "            # Whether the merging underwriter is an expert in underwriting for the specific issue, using multiple ways of\n",
    "            # dividing expertise. Note that I set it all to 0 for control areas\n",
    "            if (\"Cross-market\" in criteria) or (\"Withdrawn\" in criteria)  or (\"all non-treated CSAs as matches\" in criteria):\n",
    "                x = 1 # Do nothing\n",
    "            else:\n",
    "                for sorting_var in ['Bid','amount_bracket','mat_bracket','use_short','has_ratings']:\n",
    "                    if sorting_var=='Bid':\n",
    "                        categories = ['N','C','P']\n",
    "                    if sorting_var=='amount_bracket':\n",
    "                        categories = ['small','med','large','mega']\n",
    "                    if sorting_var=='mat_bracket':\n",
    "                        categories = ['short','med','long']\n",
    "                    if sorting_var=='use_short':\n",
    "                        categories = ['gp','edu','util','house','health','ed','tsp','pollute']\n",
    "                    if sorting_var=='has_ratings':\n",
    "                        categories = ['False','True']\n",
    "                    for threshold in ['5','10','25','50']:\n",
    "                        GPF_Seg['if_US_expert_'+sorting_var+'_'+threshold] = False\n",
    "                        GPF_Seg['if_CSA_expert_'+sorting_var+'_'+threshold] = False\n",
    "\n",
    "            if (\"Cross-market\" in criteria) or (\"Withdrawn\" in criteria)  or (\"all non-treated CSAs as matches\" in criteria):\n",
    "                x = 1 # Do nothing\n",
    "            else:\n",
    "                # Whether the underwriter is the target bank in M&A\n",
    "                GPF_Seg['bank_is_target'] = False\n",
    "                for column in name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_target'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                    |(GPF_Seg['bank_is_target'])\n",
    "                for column in parent_name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_target'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                    |(GPF_Seg['bank_is_target'])\n",
    "                # Whether the underwriter is the acquiror bank in M&A\n",
    "                GPF_Seg['bank_is_acquiror'] = False\n",
    "                for column in name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_acquiror'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                    |(GPF_Seg['bank_is_acquiror'])\n",
    "                for column in parent_name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_acquiror'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                    |(GPF_Seg['bank_is_acquiror'])\n",
    "                \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_csa'] = row['CSA Code'] # The treated CSA corresponding to this control unit\n",
    "            # Event characteristics - strength\n",
    "            GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "            GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "            GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg['top5share_dif'] = top5share_dif\n",
    "            # Event characteristics - importance for merging firms\n",
    "            GPF_Seg['max_acquiror_weight'] = max_acquiror_weight\n",
    "            GPF_Seg['max_target_weight'] = max_target_weight\n",
    "            GPF_Seg['max_acquiror_weight_in_neighbour'] = max_acquiror_weight_in_neighbour\n",
    "            GPF_Seg['max_target_weight_in_neighbour'] = max_target_weight_in_neighbour\n",
    "            # Event characteristics - driving reasons\n",
    "            GPF_Seg['reasonMA_endo_possible'] = reasonMA_endo_possible\n",
    "            GPF_Seg['reasonMA_local_dom'] = reasonMA_local_dom\n",
    "            GPF_Seg['reasonMA_expand_geo'] = reasonMA_expand_geo\n",
    "            GPF_Seg['reasonMA_ind_dom'] = reasonMA_ind_dom\n",
    "            GPF_Seg['reasonMA_syn_comb_lines'] = reasonMA_syn_comb_lines\n",
    "            GPF_Seg['reasonMA_fin_stress'] = reasonMA_fin_stress\n",
    "            GPF_Seg['reasonMA_syn_cost'] = reasonMA_syn_cost\n",
    "            GPF_Seg['reasonMA_diversify'] = reasonMA_diversify\n",
    "            GPF_Seg['frequency'] = frequency\n",
    "\n",
    "            # For the sample with all non-treated as control, restrict variables to limit file size\n",
    "            if \"use all non-treated CSAs as matches\" in criteria:\n",
    "                GPF_Seg = GPF_Seg[[\n",
    "                    'CSA Code','sale_year','State','County',\n",
    "                    'issuer_type','Issuer',\n",
    "                    'treated','episode_start_year','year_to_merger','calendar_year','treated_csa',\n",
    "                    'avg_maturity','amount',\n",
    "                    'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                    'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                    'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                    'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                    ]]\n",
    "\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "\n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    reg_sample = reg_sample.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample = reg_sample.merge(CSA_INC.rename(columns={'year':'calendar_year'}),on=['CSA Code','calendar_year'],\n",
    "        how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])    \n",
    "    reg_sample.to_csv(file_path)\n",
    "\n",
    "\n",
    "    # #-----------------------#\n",
    "    # # Bond level, using GPF #\n",
    "    # #-----------------------#\n",
    "\n",
    "    # if 'mergers' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['mergers'])\n",
    "    # if 'n_deals_prior' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['n_deals_prior'])\n",
    "    # if 'n_deals_post' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['n_deals_post'])\n",
    "    \n",
    "    # def proc_list(reg_sample):\n",
    "    #     reg_sample_bond_level = []\n",
    "    #     for idx,row in reg_sample.iterrows():\n",
    "    #         row_dict = reg_sample.loc[idx].to_dict()\n",
    "    #         if str(row['yield_by_maturity_list'])!='nan':\n",
    "    #             yield_by_maturity_list = eval(row['yield_by_maturity_list'])\n",
    "    #             if str(row['spread_by_maturity_list'])!='nan':\n",
    "    #                 spread_by_maturity_list = eval(row['spread_by_maturity_list'])\n",
    "    #             else:\n",
    "    #                 spread_by_maturity_list = [None for item in yield_by_maturity_list]\n",
    "    #             maturity_by_maturity_list = eval(row['maturity_by_maturity_list'])\n",
    "    #             amount_by_maturity_list = eval(row['amount_by_maturity_list'])\n",
    "    #             for bond_idx in range(0,len(yield_by_maturity)):\n",
    "    #                 row_dict['yield_one_bond'] = yield_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['spread_one_bond'] = spread_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['maturity_one_bond'] = maturity_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['amount_one_bond'] = amount_by_maturity_list[bond_idx]\n",
    "    #                 reg_sample_bond_level = reg_sample_bond_level+[row_dict]\n",
    "    #     reg_sample_bond_level = pd.DataFrame(reg_sample_bond_level)\n",
    "    #     return reg_sample_bond_level\n",
    "\n",
    "    # meta_columns = list(proc_list(reg_sample.sample(10)).columns)\n",
    "    # reg_sample_dd = dd.from_pandas(reg_sample, npartitions=20)\n",
    "    # with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    #     reg_sample_bond_level = reg_sample_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "    # dropped_columns =\\\n",
    "    #     [item for item in list(reg_sample_bond_level.columns) if item[:11]=='parent_name']+\\\n",
    "    #     [item for item in list(reg_sample_bond_level.columns) if item[:8]=='name_GPF']+\\\n",
    "    #     ['avg_yield','avg_spread','avg_maturity','maturity_by_maturity_list','amount_by_maturity_list','yield_by_maturity_list','spread_by_maturity_list']\n",
    "    # reg_sample_bond_level = reg_sample_bond_level.drop(columns=dropped_columns)\n",
    "    # reg_sample_bond_level.to_csv(file_path_bondlevel)\n",
    "    \n",
    "    #--------------------#\n",
    "    # Sample of quantity #\n",
    "    #--------------------#\n",
    "\n",
    "    # Markout treated CSA for further inclusion of cohort X issuer fixed effects\n",
    "    episodes_Exploded['treated_csa'] = episodes_Exploded['CSA Code']\n",
    "    \n",
    "    if 'mergers' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['mergers'])\n",
    "    if 'n_deals_prior' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['n_deals_prior'])\n",
    "    if 'n_deals_post' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['n_deals_post'])\n",
    "    \n",
    "    # Generate \"episodes_Exploded_QSample\", an indexing file with time to event dimension and has separate observations for both treated\n",
    "    # and control CSAs\n",
    "    \n",
    "    episodes_Exploded_QSample = episodes_Exploded.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_QSample.iterrows():\n",
    "        if str(episodes_Exploded_QSample.at[idx,'control'])!='None' and str(episodes_Exploded_QSample.at[idx,'control'])!='nan':\n",
    "            episodes_Exploded_QSample.at[idx,'num_control'] = len(row['control'])\n",
    "    num_control = int(np.max(episodes_Exploded_QSample['num_control']))\n",
    "    for ctrl_ind in range(0,num_control):\n",
    "        episodes_Exploded_QSample['control_'+str(ctrl_ind)] = None\n",
    "        for idx,row in episodes_Exploded_QSample.iterrows():\n",
    "            if str(episodes_Exploded_QSample.at[idx,'control'])!='None' and \\\n",
    "                str(episodes_Exploded_QSample.at[idx,'control'])!='nan':\n",
    "                # Add variables \"control_0\", \"control_1\", etc, for each control CSA\n",
    "                if ctrl_ind<len(row['control']):\n",
    "                    episodes_Exploded_QSample.at[idx,'control_'+str(ctrl_ind)] = row['control'][ctrl_ind]\n",
    "\n",
    "    # Treated observations\n",
    "    COLs_control = [item for item in episodes_Exploded_QSample.columns if item[:8]=='control_']\n",
    "    episodes_Exploded_QSample_Treated = episodes_Exploded_QSample.drop(columns=COLs_control+['num_control'])\n",
    "    episodes_Exploded_QSample_Treated['Treated'] = 1\n",
    "\n",
    "    # Control observations\n",
    "    episodes_Exploded_QSample_Control = pd.DataFrame()\n",
    "    for ctrl_ind in range(0,num_control):\n",
    "        episodes_Exploded_QSample_OneControl = episodes_Exploded_QSample.drop(columns={'CSA Code'}).\\\n",
    "            rename(columns={'control_'+str(ctrl_ind):'CSA Code'})\n",
    "        COLs_control = [item for item in episodes_Exploded_QSample_OneControl.columns if item[:8]=='control_']\n",
    "        episodes_Exploded_QSample_OneControl = episodes_Exploded_QSample_OneControl.drop(columns=COLs_control+['num_control'])\n",
    "        episodes_Exploded_QSample_Control = pd.concat([episodes_Exploded_QSample_Control,episodes_Exploded_QSample_OneControl])\n",
    "    episodes_Exploded_QSample_Control['Treated'] = 0\n",
    "    episodes_Exploded_QSample_Control = episodes_Exploded_QSample_Control[~pd.isnull(episodes_Exploded_QSample_Control['CSA Code'])]\n",
    "\n",
    "    # A cohort X firm x year to treatment level dataset\n",
    "    episodes_Exploded_QSample = pd.concat([episodes_Exploded_QSample_Treated,episodes_Exploded_QSample_Control])\n",
    "\n",
    "    #''''''''''''''''''#\n",
    "    # Overall quantity #\n",
    "    #..................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCounty.merge(CBSAData[['CSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample.merge(CountyQuant,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Overall['calendar_year'] = episodes_Exploded_QSample_Overall['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall[episodes_Exploded_QSample_Overall['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_Overall.to_csv(file_path_Quant)\n",
    "    \n",
    "    #''''''''''''''''''''''#\n",
    "    # By main use quantity #\n",
    "    #......................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCountyXUsageGeneral.merge(CBSAData[['CSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample.merge(CountyQuant,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_UsageGeneral['calendar_year'] = episodes_Exploded_QSample_UsageGeneral['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral[episodes_Exploded_QSample_UsageGeneral['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_UsageGeneral.to_csv(file_path_Quant_GeneralUse)\n",
    "    \n",
    "    #'''''''''''''''''''''''''#\n",
    "    # By issuer type quantity #\n",
    "    #.........................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCountyXIssuerType.merge(CBSAData[['CSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample.merge(CountyQuant,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_IssuerType['calendar_year'] = episodes_Exploded_QSample_IssuerType['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType[episodes_Exploded_QSample_IssuerType['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_IssuerType.to_csv(file_path_Quant_IssuerType)\n",
    "\n",
    "    #''''''''''''''''''''''#\n",
    "    # By bid type quantity #\n",
    "    #......................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCountyXBid.merge(CBSAData[['CSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Bid = episodes_Exploded_QSample.merge(CountyQuant,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Bid['calendar_year'] = episodes_Exploded_QSample_Bid['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_Bid = episodes_Exploded_QSample_Bid.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_Bid = episodes_Exploded_QSample_Bid[episodes_Exploded_QSample_Bid['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_Bid = episodes_Exploded_QSample_Bid.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_Bid.to_csv(file_path_Quant_Bid)\n",
    "\n",
    "    #-------------------------------#\n",
    "    # Sample of government finances #\n",
    "    #-------------------------------#\n",
    "    \n",
    "    GovFinData = pd.read_csv('../CleanData/GovFinSurvey/0G_GovFinData.csv',low_memory=False)\n",
    "    GovFinData = GovFinData.rename(columns={'Year4':'calendar_year'})\n",
    "    episodes_Exploded_GovFinSample = episodes_Exploded_QSample.merge(GovFinData,on=['CSA Code','calendar_year'])\n",
    "    episodes_Exploded_GovFinSample = episodes_Exploded_GovFinSample.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "    episodes_Exploded_GovFinSample.to_csv(file_path_GovFin)\n",
    "\n",
    "    #---------------------------------#\n",
    "    # Sample of population and income #\n",
    "    #---------------------------------#\n",
    "    \n",
    "    episodes_Exploded_QSample['calendar_year'] = episodes_Exploded_QSample['calendar_year'].astype(int)\n",
    "    episodes_Exploded_IncPop = episodes_Exploded_QSample.merge(CSA_INC.rename(columns={'year':'calendar_year'}),\n",
    "        on=['CSA Code','calendar_year'])\n",
    "    episodes_Exploded_IncPop = episodes_Exploded_IncPop.merge(CSA_POP.rename(columns={'year':'calendar_year'}),\n",
    "        on=['CSA Code','calendar_year'])\n",
    "    episodes_Exploded_IncPop.to_csv(file_path_IncPop)\n",
    "\n",
    "    episodes_Exploded_QSample = episodes_Exploded_QSample[['episode_start_year','CSA Code','year_to_merger','calendar_year','Treated']]\n",
    "    inc_by_county_long = pd.read_csv(\"../CleanData/Demographics/0C_County_Inc.csv\")\n",
    "    inc_by_county_long = inc_by_county_long.rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_CountyInc = episodes_Exploded_QSample.merge(inc_by_county_long,on=['CSA Code','calendar_year'])\n",
    "    episodes_Exploded_CountyInc.to_csv(file_path_CountyInc)\n",
    "\n",
    "    print('Exported regression sample for '+episodes_file[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c8a19-bae3-437e-b78a-38bbc92459d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef2afc-bfb4-4051-a9bc-a36c032c7095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25e185-242e-43a3-af75-f01cd696501a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3fa92a-059a-4ef9-a741-b40993e3cbd2",
   "metadata": {},
   "source": [
    "### 5.1.4 Plot treated-control matched sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b55f27-189d-479d-ba78-46f068b1f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "episodes['mergers'][20][['CSA Code','sale_year','acquiror','target','other_targets','acquiror_parent',\n",
    "    'target_parent','acquiror_market_share_N_avg','target_market_share_N_avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb80c8-2420-4cf0-bb5b-294141e4b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "CSADict = CBSAData.drop_duplicates(subset=['CSA Code'])[['CSA Code','CSA Title']]\n",
    "CSADict = CSADict.set_index('CSA Code')['CSA Title'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa387b-30dc-4f22-892d-c4e7ac3614a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Plot how control-treated are assembled\n",
    "\n",
    "episodes = CSA_episodes_impliedHHI_N\n",
    "\n",
    "# Read the US states GeoJSON file\n",
    "gdf = gpd.read_file('../RawData/MSA/US-counties.geojson')\n",
    "gdf = gdf[gdf['STATE']!='02']\n",
    "gdf = gdf[gdf['STATE']!='15']\n",
    "gdf = gdf[gdf['STATE']!='72']\n",
    "gdf = gdf.rename(columns={'STATE':'FIPS State Code','COUNTY':'FIPS County Code'})\n",
    "\n",
    "# Sometimes treated and control are adjacent: Improved by requiring treated and control to be not in the same state. Note that further\n",
    "# improvement can be made\n",
    "\n",
    "counties = pd.DataFrame()\n",
    "\n",
    "episode_ind = 30\n",
    "CSA_treated = episodes['CSA Code'][episode_ind]\n",
    "CSA_control = episodes['control'][episode_ind][0]\n",
    "CSA_treated1, CSA_control1 = CSA_treated, CSA_control\n",
    "counties_treated = CBSAData[(CBSAData['CSA Code']==CSA_treated)][['FIPS State Code','FIPS County Code']]\n",
    "counties_treated['value'] = 1\n",
    "counties_control = CBSAData[(CBSAData['CSA Code']==CSA_control)][['FIPS State Code','FIPS County Code']]\n",
    "counties_control['value'] = 2\n",
    "counties = pd.concat([counties,counties_treated,counties_control])\n",
    "\n",
    "episode_ind = 20\n",
    "CSA_treated = episodes['CSA Code'][episode_ind]\n",
    "CSA_control = episodes['control'][episode_ind][0]\n",
    "CSA_treated2, CSA_control2 = CSA_treated, CSA_control\n",
    "counties_treated = CBSAData[(CBSAData['CSA Code']==CSA_treated)][['FIPS State Code','FIPS County Code']]\n",
    "counties_treated['value'] = 3\n",
    "counties_control = CBSAData[(CBSAData['CSA Code']==CSA_control)][['FIPS State Code','FIPS County Code']]\n",
    "counties_control['value'] = 4\n",
    "counties = pd.concat([counties,counties_treated,counties_control])\n",
    "\n",
    "episode_ind = 110\n",
    "CSA_treated = episodes['CSA Code'][episode_ind]\n",
    "CSA_control = episodes['control'][episode_ind][0]\n",
    "CSA_treated3, CSA_control3 = CSA_treated, CSA_control\n",
    "counties_treated = CBSAData[(CBSAData['CSA Code']==CSA_treated)][['FIPS State Code','FIPS County Code']]\n",
    "counties_treated['value'] = 5\n",
    "counties_control = CBSAData[(CBSAData['CSA Code']==CSA_control)][['FIPS State Code','FIPS County Code']]\n",
    "counties_control['value'] = 6\n",
    "counties = pd.concat([counties,counties_treated,counties_control])\n",
    "\n",
    "counties['FIPS State Code'] = counties['FIPS State Code'].astype(int)\n",
    "counties['FIPS County Code'] = counties['FIPS County Code'].astype(int)\n",
    "gdf['FIPS State Code'] = gdf['FIPS State Code'].astype(int)\n",
    "gdf['FIPS County Code'] = gdf['FIPS County Code'].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "color_dict = {0:'azure', 1:'darkgreen', 2:'lime', 3:'darkblue', 4:'cornflowerblue', 5:'darkred', 6:'lightcoral'}\n",
    "\n",
    "# Merge the GeoDataFrame with data\n",
    "merged = gdf.merge(counties,on=['FIPS State Code','FIPS County Code'],how='outer')\n",
    "merged.loc[pd.isnull(merged['value']),'value'] = 0\n",
    "merged.plot(ax=ax, column='value', cmap=colors.ListedColormap(list(color_dict.values())), edgecolor='0.9', legend=False)\n",
    "legend_labels = [\n",
    "    'Treated CSA 1: '+CSADict[CSA_treated1],\n",
    "    'Treated CSA 2: '+CSADict[CSA_treated2],\n",
    "    'Treated CSA 3: '+CSADict[CSA_treated3],\n",
    "    'Control CSA 1: '+CSADict[CSA_control1],\n",
    "    'Control CSA 2: '+CSADict[CSA_control2],\n",
    "    'Control CSA 3: '+CSADict[CSA_control3],\n",
    "    ]\n",
    "legend_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='darkgreen'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='darkblue'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='darkred'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='lime'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='cornflowerblue'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markersize=10, markerfacecolor='lightcoral'),\n",
    "    ]\n",
    "ax.legend(legend_handles, legend_labels,loc='lower center',bbox_to_anchor=(0.5, -0.15),ncol=2,fontsize='11.5')\n",
    "ax.axis(\"off\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "fig.savefig('../Draft/figs/TreatedControlSample.eps', format='eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4d367-22f0-4478-bea3-b628ac889809",
   "metadata": {},
   "source": [
    "# 6. Construct a Dataset for What Predicts Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187aea95-8995-4812-b71d-acb9545fafe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Identify, for each CSA X year, what would be the implied increase in HHI based on M&As in the next three years\n",
    "\n",
    "ImpliedDeltaHHI = []\n",
    "\n",
    "for year in range(1970,2020):\n",
    "    \n",
    "    GPF_priorMA = GPF[(GPF['sale_year']<=year-1)&(GPF['sale_year']>=year-3)]\n",
    "    CSAs = list(GPF_priorMA['CSA Code'].unique())\n",
    "    CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "\n",
    "    for CSA in CSAs:\n",
    "\n",
    "        GPF_oneCSA_priorMA = GPF_priorMA[GPF_priorMA['CSA Code']==CSA]\n",
    "\n",
    "        # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCSA_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneCSA_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # HHI prior to merger\n",
    "        hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "\n",
    "        # Implied HHI post merger\n",
    "        MA_post = MA[(MA['sale_year']>=year)&(MA['sale_year']<=year+3)]\n",
    "        for idx,row in MA_post.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        hhi_dif = hhi_predicted-hhi_piror\n",
    "\n",
    "        # Also record prior issuance amount, which will be an explanatory variable\n",
    "        GPF_oneCSA_priorMA = GPF_oneCSA_priorMA[~pd.isnull(GPF_oneCSA_priorMA['amount'])]\n",
    "        amount = np.sum(GPF_oneCSA_priorMA['amount'])\n",
    "\n",
    "        ImpliedDeltaHHI = ImpliedDeltaHHI+[{'CSA Code':CSA,'year':year,'hhi_dif':hhi_dif,'hhi_piror':hhi_piror,'amount':amount}]\n",
    "\n",
    "ImpliedDeltaHHI = pd.DataFrame(ImpliedDeltaHHI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffdf99-4b78-4611-9a0a-9be1e2ee2aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add in explanatory variables:\n",
    "# Income, income growth\n",
    "# Population, population growth\n",
    "# Population age\n",
    "# Minority ratio\n",
    "# Current degree of concentration\n",
    "# Past issuance per-capita\n",
    "\n",
    "ImpliedDeltaHHI = ImpliedDeltaHHI.merge(CSA_POP,on=['CSA Code','year'])\n",
    "ImpliedDeltaHHI = ImpliedDeltaHHI.merge(CSA_INC,on=['CSA Code','year'])\n",
    "ImpliedDeltaHHI.to_csv('../CleanData/MAEvent/ImpliedDeltaHHI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12f35e-b886-452c-b9ae-077f64b35bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
