{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cc9fb9-7f29-4575-a1a4-a3984c87327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff8e08-5148-463f-bb78-5c30bc55537c",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97241d97-8d76-42fc-8bd2-dd1587d0ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPF\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "\n",
    "# Parent relationship\n",
    "GPF_names = pd.read_parquet('../CleanData/SDC/0H_GPF_Parent.parquet')\n",
    "\n",
    "# HHI and market share of each underwriter\n",
    "HHI_byCSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCSA.csv')\n",
    "market_share_all_markets_byCSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCSA.csv')\n",
    "HHI_byCBSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCBSA.csv')\n",
    "market_share_all_markets_byCBSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCBSA.csv')\n",
    "\n",
    "# All M&As\n",
    "MA = pd.read_parquet('../CleanData/SDC/0B_M&A.parquet')\n",
    "MA = MA.reset_index(drop=True)\n",
    "\n",
    "# Withdrawn M&As\n",
    "MA_withdrawn = pd.read_csv(\"../CleanData/SDC/0I_MA_withdrawn.csv\")\n",
    "\n",
    "# Quantity of issuance\n",
    "StateXCountyXBid = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXBid.parquet\")\n",
    "StateXCountyXUsageBB = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageBB.parquet\")\n",
    "StateXCountyXUsageGeneral = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageGeneral.parquet\")\n",
    "StateXCountyXUsageMain = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageMain.parquet\")\n",
    "StateXCountyXIssuerType = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXIssuerType.parquet\")\n",
    "\n",
    "StateXCounty = StateXCountyXBid.groupby(['State','County','sale_year']).agg({'amount':sum})\n",
    "StateXCounty = StateXCounty.reset_index()\n",
    "\n",
    "# Demographics\n",
    "CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "CBSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "CBSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "\n",
    "#-------------#\n",
    "# Import CBSA #\n",
    "#-------------#\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "# \"CSA\" is for metropolitan and \"CBSA\" includes also those micropolitan\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSAData = CBSAData.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSAData = CBSAData.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSAData = CBSAData[CBSAData['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSAData['County'] = CBSAData['County'].str.upper()\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' COUNTY','')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' AND ',' & ')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace('.','',regex=False)\n",
    "CBSAData['CSA Code'] = CBSAData['CSA Code'].astype(float)\n",
    "CBSAData['CBSA Code'] = CBSAData['CBSA Code'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5239ce-98bc-41ee-a4d7-c5f2a568c163",
   "metadata": {},
   "source": [
    "# 2. Construct Events of M&As, Using CBSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b99c39c-2c94-40e3-af4f-2c6bb480c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 1min, total: 2min 33s\n",
      "Wall time: 1h 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# %%script false --no-raise-error\n",
    "\n",
    "def proc_list(MA_frag):\n",
    "    \n",
    "    raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "    name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    \n",
    "    CBSA_affected = []\n",
    "    MA_frag = MA_frag.reset_index(drop=True)\n",
    "    \n",
    "    for idx,row in MA_frag.iterrows():\n",
    "        \n",
    "        # Find CBSAs that this merger affects\n",
    "        # Determine if an underwriter is active in an CBSA based on activity of PRIOR years\n",
    "        GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "\n",
    "        # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "        # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "        # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "        other_targets = \\\n",
    "            list(MA[(MA['acquiror']==row['acquiror'])&\n",
    "            (MA['sale_year']==row['sale_year'])&\n",
    "            (MA['target']!=row['target'])]['target'])\n",
    "        \n",
    "        for CBSA in list(GPF_prioryears['CBSA Code'].unique()):\n",
    "\n",
    "            GPF_prioryears_oneCBSA = GPF_prioryears[GPF_prioryears['CBSA Code']==CBSA]\n",
    "\n",
    "            # Underwriters in this state\n",
    "            underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCBSA[name_GPF_colnames]))))\n",
    "            underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "            underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "            # Parents of underwriters in this state\n",
    "            parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCBSA[parent_name_colnames]))))\n",
    "            parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "            parents_priorMA = list(set(parents_priorMA))\n",
    "            # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "            subsidiaries_priorMA = list(GPF_names[\n",
    "                (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "\n",
    "            # Determine if merger affects the CBSA, and if both sides have business\n",
    "            IF_acquiror_active = None\n",
    "            IF_target_active = None\n",
    "            IF_other_target_active = None\n",
    "            if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                IF_acquiror_active = True\n",
    "            if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                IF_target_active = True\n",
    "            for other_target in other_targets:\n",
    "                if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                    IF_other_target_active = True\n",
    "\n",
    "            # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "            # share \"market_share_all_markets_byCBSA\" is calculated at the parent level. There are many cases where market share of a\n",
    "            # firm in an area is unavailable, which is because of no presence.\n",
    "\n",
    "\n",
    "\n",
    "            #-------------------------#\n",
    "            # Market share by N deals #\n",
    "            #-------------------------#\n",
    "\n",
    "            # (1) Market share of acquiror\n",
    "            # Determine parent of target, as \"market_share_all_markets_byCBSA\" is at parent level\n",
    "            try:\n",
    "                # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                # Situation where acquiror is a parent\n",
    "                acquiror_parent = row['acquiror']\n",
    "            try:\n",
    "                acquiror_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m1 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m2 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m3 = 0\n",
    "\n",
    "            # (2) Market share of target\n",
    "            try:\n",
    "                # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                target_parent = row['target']\n",
    "            try:\n",
    "                target_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m1 = 0\n",
    "            try:\n",
    "                target_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m2 = 0\n",
    "            try:\n",
    "                target_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m3 = 0\n",
    "\n",
    "            # (3) Market share of other targets in the same transaction\n",
    "            # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "            other_targets_parents = \\\n",
    "                list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                list(other_targets)\n",
    "            other_targets_parents = list(set(other_targets_parents))\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m1 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m2 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m3 = 0\n",
    "\n",
    "\n",
    "\n",
    "            # Record data\n",
    "            if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                CBSA_affected = CBSA_affected+[{\n",
    "                    'CBSA Code':CBSA,\n",
    "                    'sale_year':row['sale_year'],\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'target':row['target'],\n",
    "                    'other_targets':other_targets,\n",
    "                    'acquiror_parent':acquiror_parent,\n",
    "                    'target_parent':target_parent,\n",
    "                    'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                    'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                    'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                    'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                    'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                    'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                    'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                    'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                    'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                }]\n",
    "            acquiror_market_share_N_m1 = None\n",
    "            acquiror_market_share_N_m2 = None\n",
    "            acquiror_market_share_N_m3 = None\n",
    "            target_market_share_N_m1 = None\n",
    "            target_market_share_N_m2 = None\n",
    "            target_market_share_N_m3 = None\n",
    "            other_targets_market_share = None\n",
    "            other_targets_market_share_N_m1 = None\n",
    "            other_targets_market_share_N_m2 = None\n",
    "            other_targets_market_share_N_m3 = None\n",
    "    \n",
    "    CBSA_affected = pd.DataFrame(CBSA_affected)\n",
    "    return CBSA_affected\n",
    "\n",
    "MA_dd = dd.from_pandas(MA, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    CBSA_affected = MA_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=\n",
    "    ['CBSA Code','sale_year','acquiror','target',\n",
    "    'other_targets','acquiror_parent','target_parent',\n",
    "    'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "    'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "    'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "    ])).compute()\n",
    "\n",
    "# Average market share over past three years\n",
    "CBSA_affected['acquiror_market_share_N_avg'] = \\\n",
    "    (CBSA_affected['acquiror_market_share_N_m1']+\\\n",
    "    CBSA_affected['acquiror_market_share_N_m2']+\\\n",
    "    CBSA_affected['acquiror_market_share_N_m3'])/3\n",
    "CBSA_affected['target_market_share_N_avg'] = \\\n",
    "    (CBSA_affected['target_market_share_N_m1']+\\\n",
    "    CBSA_affected['target_market_share_N_m2']+\\\n",
    "    CBSA_affected['target_market_share_N_m3'])/3\n",
    "CBSA_affected['other_targets_market_share_N_avg'] = \\\n",
    "    (CBSA_affected['other_targets_market_share_N_m1']+\\\n",
    "    CBSA_affected['other_targets_market_share_N_m2']+\\\n",
    "    CBSA_affected['other_targets_market_share_N_m3'])/3\n",
    "\n",
    "# As this step takes significant time, export output\n",
    "CBSA_affected.to_parquet('../CleanData/MAEvent/1B_CBSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7072c55e-35df-4376-b68a-a57b1a43866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBSA_affected = pd.read_parquet('../CleanData/MAEvent/1B_CBSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1dd36-0c83-4848-8808-e1337bba214e",
   "metadata": {},
   "source": [
    "## 2.2 Identify merger episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac31d8-c22e-43fd-8445-aaee692eb9d7",
   "metadata": {},
   "source": [
    "### 2.2.1 Method 1: By market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c132c3d-412e-497c-8c5b-93cb816affc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------#\n",
    "# Market share by N of deals #\n",
    "#----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CBSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CBSA_affected\"\n",
    "\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "CBSA_episodes_marketshare_N = []\n",
    "\n",
    "for CBSA in list(CBSA_affected['CBSA Code'].unique()):\n",
    "\n",
    "    CBSA_affected_part = CBSA_affected[CBSA_affected['CBSA Code']==CBSA]\n",
    "    CBSA_affected_part = CBSA_affected_part[\n",
    "        (CBSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CBSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CBSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    years = CBSA_affected_part['sale_year'].unique()\n",
    "    years = sorted(years)\n",
    "    for sale_year in years:\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CBSA_affected_episode = CBSA_affected_part[(CBSA_affected_part['sale_year']>=sale_year)&(CBSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        # When a firm acquires multiple firms, market share of other firms are in \"other_targets_market_share_N_avg\", so just keeping one record\n",
    "        # is sufficient\n",
    "        CBSA_affected_episode = CBSA_affected_episode.drop_duplicates(['acquiror','sale_year'])\n",
    "        # Alternative aggregation methods might be more reasonable. Also, this does not account for that target tends to be smaller so threshold\n",
    "        # for them should be smaller too. Even better, can compute the implied-HHI change (based on historical data) of this merger, and put threshold\n",
    "        # on that, which is definitely more powerful.\n",
    "        acquiror_market_share_N_avg = np.sum(CBSA_affected_episode['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_avg = np.sum(CBSA_affected_episode['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_avg = np.sum(CBSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Out of all mergers in this episode, calculate\n",
    "        # (1) the max of sum of market shares of merging entities\n",
    "        max_sum_share = \\\n",
    "            np.max(CBSA_affected_episode['acquiror_market_share_N_avg']+CBSA_affected_episode['target_market_share_N_avg']\n",
    "                +CBSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "        # (2) the max of min of market shares of merging entities\n",
    "        max_min_share = \\\n",
    "            np.max(np.minimum(CBSA_affected_episode['acquiror_market_share_N_avg'],\n",
    "                CBSA_affected_episode['target_market_share_N_avg']+CBSA_affected_episode['other_targets_market_share_N_avg']))\n",
    "        # (3) the mean of sum of market shares of merging entities\n",
    "        mean_sum_share = \\\n",
    "            np.mean(CBSA_affected_episode['acquiror_market_share_N_avg']+CBSA_affected_episode['target_market_share_N_avg']\n",
    "                +CBSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if acquiror_market_share_N_avg>0.05 and target_market_share_N_avg+other_targets_market_share_N_avg>0.05:\n",
    "            # An episode is identified\n",
    "            CBSA_episodes_marketshare_N = CBSA_episodes_marketshare_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CBSA Code':CBSA,\n",
    "                'mergers':CBSA_affected_episode,\n",
    "                'acquiror_market_share_N_avg':acquiror_market_share_N_avg,\n",
    "                'target_market_share_N_avg':target_market_share_N_avg,\n",
    "                'other_targets_market_share_N_avg':other_targets_market_share_N_avg,\n",
    "                'max_sum_share':max_sum_share,\n",
    "                'max_min_share':max_min_share,\n",
    "                'mean_sum_share':mean_sum_share,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CBSA_episodes_marketshare_N = pd.DataFrame(CBSA_episodes_marketshare_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb792d6b-469a-4f76-a9a6-0171096775e4",
   "metadata": {},
   "source": [
    "### 2.2.2 Method 2: By implied rise in HHI due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42600530-0441-42d6-958d-e2b1276a46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------#\n",
    "# Change in HHI by N of deals #\n",
    "#-----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CBSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CBSA_affected\"\n",
    "\n",
    "CBSA_episodes_impliedHHI_N = []\n",
    "\n",
    "for CBSA in list(CBSA_affected['CBSA Code'].unique()):\n",
    "\n",
    "    CBSA_affected_part = CBSA_affected[CBSA_affected['CBSA Code']==CBSA]\n",
    "    CBSA_affected_part = CBSA_affected_part[\n",
    "        (CBSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CBSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CBSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in CBSA_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CBSA_affected_episode = CBSA_affected_part[(CBSA_affected_part['sale_year']>=sale_year)&(CBSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneCBSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CBSA Code']==CBSA)]\n",
    "        \n",
    "        # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCBSA_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneCBSA_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # HHI prior to merger\n",
    "        hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "\n",
    "        # Implied HHI post merger\n",
    "        CBSA_affected_episode = CBSA_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in CBSA_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        hhi_dif = hhi_predicted-hhi_piror\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if hhi_dif>0.01:\n",
    "            # An episode is identified\n",
    "            CBSA_episodes_impliedHHI_N = CBSA_episodes_impliedHHI_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CBSA Code':CBSA,\n",
    "                'mergers':CBSA_affected_episode,\n",
    "                'hhi_dif':hhi_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CBSA_episodes_impliedHHI_N = pd.DataFrame(CBSA_episodes_impliedHHI_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1216c06-92c0-49d1-be86-6139a0232547",
   "metadata": {},
   "source": [
    "### 2.2.3 Method 3: By implied rise in top 5 share due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d0cacc-06b8-4330-af3d-20ffd435dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------#\n",
    "# Change in top 5 share by N of deals #\n",
    "#-------------------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the CBSA level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"CBSA_affected\"\n",
    "\n",
    "CBSA_episodes_top5share_N = []\n",
    "\n",
    "for CBSA in list(CBSA_affected['CBSA Code'].unique()):\n",
    "\n",
    "    CBSA_affected_part = CBSA_affected[CBSA_affected['CBSA Code']==CBSA]\n",
    "    CBSA_affected_part = CBSA_affected_part[\n",
    "        (CBSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((CBSA_affected_part['target_market_share_N_avg']>0)|\n",
    "        (CBSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in CBSA_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        CBSA_affected_episode = CBSA_affected_part[(CBSA_affected_part['sale_year']>=sale_year)&(CBSA_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneCBSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CBSA Code']==CBSA)]\n",
    "        \n",
    "        # Calculate (1) Top 5 share (by parent firm) in the three years prior (2) Predicted top 5 share after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCBSA_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneCBSA_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # Top 5 share prior to merger\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_prior = 1\n",
    "        else:\n",
    "            top5share_prior = np.sum(n_deals['marketshare'][:5])\n",
    "\n",
    "        # Implied top 5 share post merger\n",
    "        CBSA_affected_episode = CBSA_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in CBSA_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_post = 1\n",
    "        else:\n",
    "            top5share_post = np.sum(n_deals['marketshare'][:5])\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        top5share_dif = top5share_post-top5share_prior\n",
    "\n",
    "        # Market shares used in summary statistics\n",
    "        CBSA_affected_episode['min_share'] = np.minimum(CBSA_affected_episode['acquiror_market_share_N_avg'],\n",
    "            CBSA_affected_episode['target_market_share_N_avg']+\\\n",
    "            CBSA_affected_episode['other_targets_market_share_N_avg'])\n",
    "        CBSA_affected_episode = CBSA_affected_episode.sort_values('min_share')\n",
    "        CBSA_affected_episode_topshare = CBSA_affected_episode[-1:]\n",
    "        acquiror_market_share_N_max = np.max(CBSA_affected_episode_topshare['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_max = np.max(CBSA_affected_episode_topshare['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_max = np.max(CBSA_affected_episode_topshare['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if top5share_dif>0.05:\n",
    "            # An episode is identified\n",
    "            CBSA_episodes_top5share_N = CBSA_episodes_top5share_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'CBSA Code':CBSA,\n",
    "                'mergers':CBSA_affected_episode,\n",
    "                'top5share_dif':top5share_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                'acquiror_market_share_N_max':acquiror_market_share_N_max,\n",
    "                'target_market_share_N_max':target_market_share_N_max,\n",
    "                'other_targets_market_share_N_max':other_targets_market_share_N_max,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "CBSA_episodes_top5share_N = pd.DataFrame(CBSA_episodes_top5share_N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178c8c1-8bc6-4447-b9f4-8449cf37a700",
   "metadata": {},
   "source": [
    "# 3. Placebo Tests, Using CSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b363f-77b8-42cb-a490-8cce21b62921",
   "metadata": {},
   "source": [
    "## 3.1 Withdrawn M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f684243-7e22-4022-9868-276c6a874265",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_withdrawn = pd.read_csv(\"../CleanData/SDC/0I_MA_withdrawn.csv\")\n",
    "MA_withdrawn = MA_withdrawn.rename(columns={'announce_year':'sale_year'})[['target','acquiror','sale_year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d0605-2c89-41b9-95e9-bfc8afb70685",
   "metadata": {},
   "source": [
    "### 3.2.1 Find CSA X Year that could be affected by withdrawn merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4209da-b8aa-4181-bfa9-d5e50f5c3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find CBSA X Year that could be affected by withdrawn merger\n",
    "\n",
    "# Go over each merger. Check the CBSAs affected by the merger (i.e., either side has business in the CBSA in the year prior \n",
    "# to the merger). Check if the merger affects just one underwriter or affects multiple underwriters in this CBSA.\n",
    "\n",
    "# Note that for the column \"market share of other targets\", the optimal object to put there is the market share of the other target\n",
    "# alone. Here I am instead putting in market share of the other target's parent. This should make a minimal difference.\n",
    "\n",
    "name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "def proc_list(MA_withdrawn_frag):\n",
    "\n",
    "    CBSA_affected_withdrawn = []\n",
    "    MA_withdrawn_frag = MA_withdrawn_frag.reset_index(drop=True)\n",
    "    \n",
    "    for idx,row in MA_withdrawn_frag.iterrows():\n",
    "        \n",
    "        # Find CBSAs that this merger affects\n",
    "        # Determine if an underwriter is active in an CBSA based on activity of PRIOR years\n",
    "        GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "    \n",
    "        # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "        # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "        # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "        other_targets = \\\n",
    "            list(MA_withdrawn[(MA_withdrawn['acquiror']==row['acquiror'])&\n",
    "            (MA_withdrawn['sale_year']==row['sale_year'])&\n",
    "            (MA_withdrawn['target']!=row['target'])]['target'])\n",
    "        \n",
    "        for CBSA in list(GPF_prioryears['CBSA Code'].unique()):\n",
    "    \n",
    "            GPF_prioryears_oneCBSA = GPF_prioryears[GPF_prioryears['CBSA Code']==CBSA]\n",
    "    \n",
    "            # Underwriters in this state\n",
    "            underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCBSA[name_GPF_colnames]))))\n",
    "            underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "            underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "            # Parents of underwriters in this state\n",
    "            parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCBSA[parent_name_colnames]))))\n",
    "            parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "            parents_priorMA = list(set(parents_priorMA))\n",
    "            # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "            subsidiaries_priorMA = list(GPF_names[\n",
    "                (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "    \n",
    "            # Determine if merger affects the CBSA, and if both sides have business\n",
    "            IF_acquiror_active = None\n",
    "            IF_target_active = None\n",
    "            IF_other_target_active = None\n",
    "            if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                IF_acquiror_active = True\n",
    "            if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                IF_target_active = True\n",
    "            for other_target in other_targets:\n",
    "                if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                    IF_other_target_active = True\n",
    "    \n",
    "            # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "            # share \"market_share_all_markets_byCBSA\" is calculated at the parent level. There are many cases where market share of a\n",
    "            # firm in an area is unavailable, which is because of no presence.\n",
    "    \n",
    "    \n",
    "    \n",
    "            #-------------------------#\n",
    "            # Market share by N deals #\n",
    "            #-------------------------#\n",
    "    \n",
    "            # (1) Market share of acquiror\n",
    "            # Determine parent of target, as \"market_share_all_markets_byCBSA\" is at parent level\n",
    "            try:\n",
    "                # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                # Situation where acquiror is a parent\n",
    "                acquiror_parent = row['acquiror']\n",
    "            try:\n",
    "                acquiror_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m1 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m2 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m3 = 0\n",
    "    \n",
    "            # (2) Market share of target\n",
    "            try:\n",
    "                # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                target_parent = row['target']\n",
    "            try:\n",
    "                target_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m1 = 0\n",
    "            try:\n",
    "                target_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m2 = 0\n",
    "            try:\n",
    "                target_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byCBSA[\n",
    "                    (market_share_all_markets_byCBSA['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                    &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m3 = 0\n",
    "    \n",
    "            # (3) Market share of other targets in the same transaction\n",
    "            # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "            other_targets_parents = \\\n",
    "                list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                list(other_targets)\n",
    "            other_targets_parents = list(set(other_targets_parents))\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-1)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m1 = 0\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-2)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m2 = 0\n",
    "    \n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byCBSA[\n",
    "                (market_share_all_markets_byCBSA['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byCBSA['CBSA Code']==CBSA)\n",
    "                &(market_share_all_markets_byCBSA['calendar_year']==row['sale_year']-3)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m3 = 0\n",
    "    \n",
    "    \n",
    "            # Record data\n",
    "            if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                CBSA_affected_withdrawn = CBSA_affected_withdrawn+[{\n",
    "                    'CBSA Code':CBSA,\n",
    "                    'sale_year':row['sale_year'],\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'target':row['target'],\n",
    "                    'other_targets':other_targets,\n",
    "                    'acquiror_parent':acquiror_parent,\n",
    "                    'target_parent':target_parent,\n",
    "                    'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                    'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                    'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                    'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                    'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                    'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                    'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                    'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                    'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                }]\n",
    "            acquiror_market_share_N_m1 = None\n",
    "            acquiror_market_share_N_m2 = None\n",
    "            acquiror_market_share_N_m3 = None\n",
    "            target_market_share_N_m1 = None\n",
    "            target_market_share_N_m2 = None\n",
    "            target_market_share_N_m3 = None\n",
    "            other_targets_market_share = None\n",
    "            other_targets_market_share_N_m1 = None\n",
    "            other_targets_market_share_N_m2 = None\n",
    "            other_targets_market_share_N_m3 = None\n",
    "    \n",
    "    CBSA_affected_withdrawn = pd.DataFrame(CBSA_affected_withdrawn)\n",
    "        \n",
    "    return CBSA_affected_withdrawn\n",
    "\n",
    "MA_withdrawn_dd = dd.from_pandas(MA_withdrawn, npartitions=40)\n",
    "with dask.config.set(scheduler='processes',num_workers=40):\n",
    "    CBSA_affected_withdrawn = MA_withdrawn_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=\n",
    "    ['CBSA Code','sale_year','acquiror','target',\n",
    "    'other_targets','acquiror_parent','target_parent',\n",
    "    'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "    'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "    'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "    ])).compute()\n",
    "\n",
    "# Average market share over past three years\n",
    "CBSA_affected_withdrawn['acquiror_market_share_N_avg'] = \\\n",
    "    (CBSA_affected_withdrawn['acquiror_market_share_N_m1']+\\\n",
    "    CBSA_affected_withdrawn['acquiror_market_share_N_m2']+\\\n",
    "    CBSA_affected_withdrawn['acquiror_market_share_N_m3'])/3\n",
    "CBSA_affected_withdrawn['target_market_share_N_avg'] = \\\n",
    "    (CBSA_affected_withdrawn['target_market_share_N_m1']+\\\n",
    "    CBSA_affected_withdrawn['target_market_share_N_m2']+\\\n",
    "    CBSA_affected_withdrawn['target_market_share_N_m3'])/3\n",
    "CBSA_affected_withdrawn['other_targets_market_share_N_avg'] = \\\n",
    "    (CBSA_affected_withdrawn['other_targets_market_share_N_m1']+\\\n",
    "    CBSA_affected_withdrawn['other_targets_market_share_N_m2']+\\\n",
    "    CBSA_affected_withdrawn['other_targets_market_share_N_m3'])/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad83f0f-8ada-4445-a0c8-af75e93ab1d1",
   "metadata": {},
   "source": [
    "### 3.1.2 Construct events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5114cbfb-af9e-4578-939c-19333159c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------#\n",
    "# Withdrawn episodes with Delta HHI of each treshold #\n",
    "#----------------------------------------------------#\n",
    "\n",
    "for threshold in [0.01,0.005,0.003,0.002,0.001]:\n",
    "\n",
    "    # Identify episodes of mergers at the CBSA level\n",
    "    \n",
    "    # Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "    # identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "    # the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "    # as in cases where two firms merge into a new one, both will get recorded in \"CBSA_affected\"\n",
    "    \n",
    "    CBSA_episodes_Withdrawn = []\n",
    "    \n",
    "    for CBSA in list(CBSA_affected_withdrawn['CBSA Code'].unique()):\n",
    "    \n",
    "        CBSA_affected_part = CBSA_affected_withdrawn[CBSA_affected_withdrawn['CBSA Code']==CBSA]\n",
    "        CBSA_affected_part = CBSA_affected_part[\n",
    "            (CBSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CBSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CBSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        \n",
    "        episode_start_year = 1900\n",
    "        for sale_year in CBSA_affected_part['sale_year'].unique():\n",
    "        \n",
    "            # If this year is still within the last merger episode\n",
    "            if sale_year<=episode_start_year+4:\n",
    "                continue\n",
    "            \n",
    "            # Check intensity of M&A activities in that year and three years following\n",
    "            CBSA_affected_episode = CBSA_affected_part[(CBSA_affected_part['sale_year']>=sale_year)&(CBSA_affected_part['sale_year']<=sale_year+3)]\n",
    "            GPF_oneCBSA_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['CBSA Code']==CBSA)]\n",
    "            \n",
    "            # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "            \n",
    "            # Underwriters in the market\n",
    "            name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCBSA_priorMA[parent_name_colnames]))))\n",
    "            name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "            name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "            name_GPFs = list(set(name_GPFs))\n",
    "            n_deals = {}\n",
    "            for item in name_GPFs:\n",
    "                n_deals[item] = 0\n",
    "            \n",
    "            # Record market shares before merger episode\n",
    "            parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "            for idx,row in GPF_oneCBSA_priorMA.iterrows():\n",
    "                underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "                n_underwriters = len(underwriters_onedeal)\n",
    "                for item in underwriters_onedeal:\n",
    "                    n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "            n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "            n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "            n_deals_prior = n_deals\n",
    "            \n",
    "            # HHI prior to merger\n",
    "            hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "    \n",
    "            # Implied HHI post merger\n",
    "            CBSA_affected_episode = CBSA_affected_episode.reset_index(drop=True)\n",
    "            for idx,row in CBSA_affected_episode.iterrows():\n",
    "                n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "            n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "            hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "            n_deals_post = n_deals\n",
    "    \n",
    "            hhi_dif = hhi_predicted-hhi_piror\n",
    "    \n",
    "            # Check if market share in the episode is high enough\n",
    "            if hhi_dif>threshold:\n",
    "                # An episode is identified\n",
    "                CBSA_episodes_Withdrawn = CBSA_episodes_Withdrawn+[{\n",
    "                    'episode_start_year':sale_year,\n",
    "                    'CBSA Code':CBSA,\n",
    "                    'mergers':CBSA_affected_episode,\n",
    "                    'hhi_dif':hhi_dif,\n",
    "                    'n_deals_prior':n_deals_prior,\n",
    "                    'n_deals_post':n_deals_post,\n",
    "                    'acquiror_market_share_N_max':acquiror_market_share_N_max,\n",
    "                    'target_market_share_N_max':target_market_share_N_max,\n",
    "                    'other_targets_market_share_N_max':other_targets_market_share_N_max,\n",
    "                    }]\n",
    "                episode_start_year = sale_year\n",
    "    \n",
    "    CBSA_episodes_Withdrawn = pd.DataFrame(CBSA_episodes_Withdrawn)\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------#\n",
    "    # Check and rule out if affected by actual M&A #\n",
    "    #----------------------------------------------#\n",
    "    \n",
    "    # Check if there is any M&A in between [-4,+4] that has both sides market share above 0.01\n",
    "    CBSA_episodes_Withdrawn['if_also_withinMA'] = False\n",
    "    for sub_idx,sub_row in CBSA_episodes_Withdrawn.iterrows():\n",
    "    \n",
    "        # M&As in a candidate placebo CBSA in [-4,+4]\n",
    "        CBSA_affected_part = CBSA_affected[CBSA_affected['CBSA Code']==sub_row['CBSA Code']]\n",
    "        CBSA_affected_part = CBSA_affected_part[\n",
    "            (CBSA_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CBSA_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CBSA_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "        CBSA_affected_part = CBSA_affected_part[\n",
    "            (CBSA_affected_part['sale_year']>=sub_row['episode_start_year']-1)&\n",
    "            (CBSA_affected_part['sale_year']<=sub_row['episode_start_year']+5)\n",
    "            ]\n",
    "        CBSA_affected_episode = CBSA_affected_part.copy()\n",
    "        CBSA_affected_episode = CBSA_affected_episode[(CBSA_affected_episode['acquiror_market_share_N_avg']>0.01)&\n",
    "            (CBSA_affected_episode['target_market_share_N_avg']+CBSA_affected_episode['other_targets_market_share_N_avg']>0.01)]\n",
    "        if len(CBSA_affected_episode)>0:\n",
    "            CBSA_episodes_Withdrawn.at[sub_idx,'if_also_withinMA'] = True\n",
    "    \n",
    "    CBSA_episodes_Withdrawn = CBSA_episodes_Withdrawn[~CBSA_episodes_Withdrawn['if_also_withinMA']]\n",
    "\n",
    "    if threshold==0.01:\n",
    "        CBSA_episodes_Withdrawn_DeltaHHI100 = CBSA_episodes_Withdrawn\n",
    "    if threshold==0.005:\n",
    "        CBSA_episodes_Withdrawn_DeltaHHI50 = CBSA_episodes_Withdrawn\n",
    "    if threshold==0.003:\n",
    "        CBSA_episodes_Withdrawn_DeltaHHI30 = CBSA_episodes_Withdrawn\n",
    "    if threshold==0.002:\n",
    "        CBSA_episodes_Withdrawn_DeltaHHI20 = CBSA_episodes_Withdrawn\n",
    "    if threshold==0.001:\n",
    "        CBSA_episodes_Withdrawn_DeltaHHI10 = CBSA_episodes_Withdrawn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92101d7c-f3f3-4986-82dc-2cf522c5a653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e5822-165d-45df-8707-51dc9cd209e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbed779-23aa-40b3-87f2-ca93789528fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13fd5615-e2f4-4411-832f-eb70b751b76b",
   "metadata": {},
   "source": [
    "# 4. Assemble a Treatment-Control Matched Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8948d6-a8a1-4ddb-8ca9-9349edca0c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 3 episodes.\n",
      "Exported regression sample for By Market Share in terms of N deals\n",
      "A control cannot be found for 2 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 1 episodes.\n",
      "Exported regression sample for By Implied Top 5 Share Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for Withdrawn M&A, >= 0.01\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for Withdrawn M&A, >= 0.005\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for Withdrawn M&A, >= 0.003\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for Withdrawn M&A, >= 0.002\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for Withdrawn M&A, >= 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 0 episodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported regression sample for Withdrawn M&A, >= 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 0 episodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported regression sample for Withdrawn M&A, >= 0.002\n"
     ]
    }
   ],
   "source": [
    "episodes_files = [\n",
    "    [\"By Market Share in terms of N deals\",CBSA_episodes_marketshare_N,1,\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_marketshareByN_GovFin.csv',\n",
    "    ],\n",
    "    [\"By Implied HHI Increase in terms of N deals, >= 0.01\",CBSA_episodes_impliedHHI_N,1,\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_impliedHHIByN_GovFin.csv',\n",
    "    ],\n",
    "    [\"By Implied Top 5 Share Increase in terms of N deals, >= 0.01\",CBSA_episodes_top5share_N,1,\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_episodes_top5shareByN_GovFin.csv',\n",
    "    ],\n",
    "    [\"Withdrawn M&A, >= 0.01\",CBSA_episodes_Withdrawn_DeltaHHI100,1,\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI100_GovFin.csv',\n",
    "    ],\n",
    "    [\"Withdrawn M&A, >= 0.005\",CBSA_episodes_Withdrawn_DeltaHHI50,1,\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI50_GovFin.csv',\n",
    "    ],\n",
    "    [\"Withdrawn M&A, >= 0.003\",CBSA_episodes_Withdrawn_DeltaHHI30,1,\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI30_GovFin.csv',\n",
    "    ],\n",
    "    [\"Withdrawn M&A, >= 0.002\",CBSA_episodes_Withdrawn_DeltaHHI20,1,\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI20_GovFin.csv',\n",
    "    ],\n",
    "    [\"Withdrawn M&A, >= 0.001\",CBSA_episodes_Withdrawn_DeltaHHI10,1,\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10_bondlevel.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10_Quant.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10_Quant_GeneralUse.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10_Quant_IssuerType.csv',\n",
    "        '../CleanData/MAEvent/CBSA_Withdrawn_DeltaHHI10_GovFin.csv',\n",
    "    ],\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    criteria = episodes_file[0]\n",
    "    episodes = episodes_file[1]\n",
    "    N_matches = episodes_file[2]\n",
    "    file_path = episodes_file[3]\n",
    "    file_path_bondlevel = episodes_file[4]\n",
    "    file_path_Quant = episodes_file[5]\n",
    "    file_path_Quant_GeneralUse = episodes_file[6]\n",
    "    file_path_Quant_IssuerType = episodes_file[7]\n",
    "    file_path_GovFin = episodes_file[8]\n",
    "\n",
    "    episodes = episodes.copy()\n",
    "    \n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CBSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "    CBSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "    CBSA_Data = CBSA_POP.merge(CBSA_INC,on=['CBSA Code','year'])\n",
    "    CBSA_Data = CBSA_Data[['CBSA Code','year','inc','pop']]\n",
    "    Same_State_CBSA_pairs = pd.read_csv(\"../CleanData/Demographics/0C_Same_State_CBSA_pairs.csv\")\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CBSA\n",
    "        CBSA_Data_oneyear = CBSA_Data[CBSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CBSA\n",
    "        CBSA_Data_oneyear_frag = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']==row['CBSA Code']].copy()\n",
    "        if len(CBSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CBSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CBSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CBSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CBSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CBSA_Data_oneyear['inc'] = winsor2(CBSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CBSA_Data_oneyear['pop'] = winsor2(CBSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CBSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CBSA_Data_oneyear['dist'] = CBSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']!=row['CBSA Code']]\n",
    "        # Remove other CBSAs in the same state from potential matches\n",
    "        Same_State_CBSAs = list(Same_State_CBSA_pairs[Same_State_CBSA_pairs['CBSA_1']==row['CBSA Code']]['CBSA_2'])\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[~CBSA_Data_oneyear['CBSA Code'].isin(Same_State_CBSAs)]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CBSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==subrow['CBSA Code']]\n",
    "            CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CBSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "    \n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,11))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "\n",
    "        if 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "\n",
    "        if 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==row['CBSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CBSA Code','sale_year','State','County',\n",
    "            'issuer_type','Issuer',\n",
    "            'avg_maturity','amount',\n",
    "            'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "            'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "            'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "            'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "            'underpricing_15to60','underpricing_15to30',\n",
    "            'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "            'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "            'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "            'if_callable','CB_Eligible',\n",
    "            'num_relationship',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        #------------------------------------#\n",
    "        # Some cross-sectional heterogeneity #\n",
    "        #------------------------------------#\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "        # code block)\n",
    "        mergers = CBSA_affected[\n",
    "            (CBSA_affected['CBSA Code']==row['CBSA Code'])&\n",
    "            (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['treated_cbsa'] = row['CBSA Code'] # Used for constructing cohort X issuer FEs\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CBSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "            mergers = CBSA_affected[\n",
    "                (CBSA_affected['CBSA Code']==item)&\n",
    "                (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_cbsa'] = row['CBSA Code'] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    reg_sample = reg_sample.merge(HHI_byCBSA,on=['CBSA Code','calendar_year'])\n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n",
    "\n",
    "    # #-----------------------#\n",
    "    # # Bond level, using GPF #\n",
    "    # #-----------------------#\n",
    "\n",
    "    # if 'mergers' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['mergers'])\n",
    "    # if 'n_deals_prior' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['n_deals_prior'])\n",
    "    # if 'n_deals_post' in reg_sample.columns:\n",
    "    #     reg_sample = reg_sample.drop(columns=['n_deals_post'])\n",
    "    \n",
    "    # def proc_list(reg_sample):\n",
    "    #     reg_sample_bond_level = []\n",
    "    #     for idx,row in reg_sample.iterrows():\n",
    "    #         row_dict = reg_sample.loc[idx].to_dict()\n",
    "    #         if str(row['yield_by_maturity_list'])!='nan':\n",
    "    #             yield_by_maturity_list = eval(row['yield_by_maturity_list'])\n",
    "    #             if str(row['spread_by_maturity_list'])!='nan':\n",
    "    #                 spread_by_maturity_list = eval(row['spread_by_maturity_list'])\n",
    "    #             else:\n",
    "    #                 spread_by_maturity_list = [None for item in yield_by_maturity_list]\n",
    "    #             maturity_by_maturity_list = eval(row['maturity_by_maturity_list'])\n",
    "    #             amount_by_maturity_list = eval(row['amount_by_maturity_list'])\n",
    "    #             for bond_idx in range(0,len(yield_by_maturity)):\n",
    "    #                 row_dict['yield_one_bond'] = yield_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['spread_one_bond'] = spread_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['maturity_one_bond'] = maturity_by_maturity_list[bond_idx]\n",
    "    #                 row_dict['amount_one_bond'] = amount_by_maturity_list[bond_idx]\n",
    "    #                 reg_sample_bond_level = reg_sample_bond_level+[row_dict]\n",
    "    #     reg_sample_bond_level = pd.DataFrame(reg_sample_bond_level)\n",
    "    #     return reg_sample_bond_level\n",
    "\n",
    "    # meta_columns = list(proc_list(reg_sample.sample(10)).columns)\n",
    "    # reg_sample_dd = dd.from_pandas(reg_sample, npartitions=20)\n",
    "    # with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    #     reg_sample_bond_level = reg_sample_dd.map_partitions(proc_list,meta=pd.DataFrame(columns=meta_columns)).compute()\n",
    "    # dropped_columns =\\\n",
    "    #     [item for item in list(reg_sample_bond_level.columns) if item[:11]=='parent_name']+\\\n",
    "    #     [item for item in list(reg_sample_bond_level.columns) if item[:8]=='name_GPF']+\\\n",
    "    #     ['avg_yield','avg_spread','avg_maturity','maturity_by_maturity_list','amount_by_maturity_list','yield_by_maturity_list','spread_by_maturity_list']\n",
    "    # reg_sample_bond_level = reg_sample_bond_level.drop(columns=dropped_columns)\n",
    "    # reg_sample_bond_level.to_csv(file_path_bondlevel)\n",
    "    \n",
    "    #--------------------#\n",
    "    # Sample of quantity #\n",
    "    #--------------------#\n",
    "\n",
    "    if 'mergers' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['mergers'])\n",
    "    if 'n_deals_prior' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['n_deals_prior'])\n",
    "    if 'n_deals_post' in episodes_Exploded.columns:\n",
    "        episodes_Exploded = episodes_Exploded.drop(columns=['n_deals_post'])\n",
    "    \n",
    "    # Start from CBSA level sample\n",
    "    episodes_Exploded_QSample = episodes_Exploded.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_QSample.iterrows():\n",
    "        if str(episodes_Exploded_QSample.at[idx,'control'])!='None' and str(episodes_Exploded_QSample.at[idx,'control'])!='nan':\n",
    "            episodes_Exploded_QSample.at[idx,'num_control'] = len(row['control'])\n",
    "    num_control = int(np.max(episodes_Exploded_QSample['num_control']))\n",
    "    for ctrl_ind in range(0,num_control):\n",
    "        episodes_Exploded_QSample['control_'+str(ctrl_ind)] = None\n",
    "        for idx,row in episodes_Exploded_QSample.iterrows():\n",
    "            if str(episodes_Exploded_QSample.at[idx,'control'])!='None' and \\\n",
    "                str(episodes_Exploded_QSample.at[idx,'control'])!='nan':\n",
    "                episodes_Exploded_QSample.at[idx,'control_'+str(ctrl_ind)] = row['control'][ctrl_ind]\n",
    "        episodes_Exploded_QSample['control_'+str(ctrl_ind)] = episodes_Exploded_QSample['control_'+str(ctrl_ind)].astype(int)\n",
    "    \n",
    "    COLs_control = [item for item in episodes_Exploded_QSample.columns if item[:8]=='control_']\n",
    "    episodes_Exploded_QSample_Treated = episodes_Exploded_QSample.drop(columns=COLs_control+['num_control'])\n",
    "    episodes_Exploded_QSample_Treated['Treated'] = 1\n",
    "    \n",
    "    episodes_Exploded_QSample_Control = pd.DataFrame()\n",
    "    for ctrl_ind in range(0,num_control):\n",
    "        episodes_Exploded_QSample_OneControl = episodes_Exploded_QSample.drop(columns={'CBSA Code'}).\\\n",
    "            rename(columns={'control_'+str(ctrl_ind):'CBSA Code'})\n",
    "        COLs_control = [item for item in episodes_Exploded_QSample_OneControl.columns if item[:8]=='control_']\n",
    "        episodes_Exploded_QSample_OneControl = episodes_Exploded_QSample_OneControl.drop(columns=COLs_control+['num_control'])\n",
    "        episodes_Exploded_QSample_Control = pd.concat([episodes_Exploded_QSample_Control,episodes_Exploded_QSample_OneControl])\n",
    "    episodes_Exploded_QSample_Control['Treated'] = 0\n",
    "    \n",
    "    episodes_Exploded_QSample = pd.concat([episodes_Exploded_QSample_Treated,episodes_Exploded_QSample_Control])\n",
    "    \n",
    "    #''''''''''''''''''#\n",
    "    # Overall quantity #\n",
    "    #..................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCounty.merge(CBSAData[['CBSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample.merge(CountyQuant,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_Overall['calendar_year'] = episodes_Exploded_QSample_Overall['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall[episodes_Exploded_QSample_Overall['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_Overall = episodes_Exploded_QSample_Overall.merge(HHI_byCBSA,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_Overall.to_csv(file_path_Quant)\n",
    "    \n",
    "    #''''''''''''''''''''''#\n",
    "    # By main use quantity #\n",
    "    #......................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCountyXUsageGeneral.merge(CBSAData[['CBSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample.merge(CountyQuant,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_UsageGeneral['calendar_year'] = episodes_Exploded_QSample_UsageGeneral['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral[episodes_Exploded_QSample_UsageGeneral['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_UsageGeneral = episodes_Exploded_QSample_UsageGeneral.merge(HHI_byCBSA,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_UsageGeneral.to_csv(file_path_Quant_GeneralUse)\n",
    "    \n",
    "    #'''''''''''''''''''''''''#\n",
    "    # By issuer type quantity #\n",
    "    #.........................#\n",
    "    \n",
    "    # Add the county dimension into the data, along with amount of issue\n",
    "    CountyQuant = StateXCountyXIssuerType.merge(CBSAData[['CBSA Code','State','County']],on=['State','County']).\\\n",
    "        rename(columns={'sale_year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample.merge(CountyQuant,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    episodes_Exploded_QSample_IssuerType['calendar_year'] = episodes_Exploded_QSample_IssuerType['calendar_year'].astype(int)\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType[episodes_Exploded_QSample_IssuerType['_merge']!='right_only']\n",
    "    episodes_Exploded_QSample_IssuerType = episodes_Exploded_QSample_IssuerType.merge(HHI_byCBSA,on=['CBSA Code','calendar_year'])\n",
    "    \n",
    "    episodes_Exploded_QSample_IssuerType.to_csv(file_path_Quant_IssuerType)\n",
    "\n",
    "    #-------------------------------#\n",
    "    # Sample of government finances #\n",
    "    #-------------------------------#\n",
    "    \n",
    "    GovFinData = pd.read_csv('../CleanData/GovFinSurvey/0G_GovFinData.csv',low_memory=False)\n",
    "    GovFinData = GovFinData.rename(columns={'Year4':'calendar_year'})\n",
    "    episodes_Exploded_GovFinSample = episodes_Exploded_QSample.merge(GovFinData,on=['CBSA Code','calendar_year'])\n",
    "    episodes_Exploded_GovFinSample = episodes_Exploded_GovFinSample.merge(HHI_byCBSA,on=['CBSA Code','calendar_year'])\n",
    "    episodes_Exploded_GovFinSample.to_csv(file_path_GovFin)\n",
    "    \n",
    "    print('Exported regression sample for '+episodes_file[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a641e-4f97-4bd2-84bb-0ed524ce135f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
