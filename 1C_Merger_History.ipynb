{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82a5804-290d-48a9-ae4d-78666ab634a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "try:\n",
    "    del(FUN_proc_name)\n",
    "except:\n",
    "    pass\n",
    "import FUN_proc_name\n",
    "importlib.reload(FUN_proc_name)\n",
    "from FUN_proc_name import FUN_proc_name\n",
    "\n",
    "# Whether to run the step of merging GPF underwriters to SDC M&A deals. Note that M&As from SDC has been hand-cleaned and \n",
    "# recorded in \"SCRIPT_SDC_deals_cleaned.csv\"\n",
    "IF_match_SDC_MA = False\n",
    "IF_fuzzy_match = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e35083-0212-4ce7-b97e-f53c2bb2ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GPF\n",
    "GPF = pd.read_csv(\"../RawData/SDC/GPF.csv\",low_memory=False)\n",
    "GPF = GPF.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Generate cleaned names\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "idx = 0\n",
    "for column in raw_name_GPF_colnames:\n",
    "    GPF['name_GPF_'+str(idx)] = GPF[column].apply(FUN_proc_name)\n",
    "    idx = idx+1\n",
    "name_GPF_colnames = ['name_GPF_'+str(idx) for idx in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "# Hand corrections\n",
    "# (1) Change \"CHEMICAL BANK\" to \"CHEMICAL BANK MICHIGAN\" for deals in MI\n",
    "for column in raw_name_GPF_colnames:\n",
    "    GPF.loc[(GPF['raw_name_GPF_0'].str.contains('Chemical Bank'))&(GPF['State']=='MI'),column] = 'CHEMICAL BANK MICHIGAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d9b7e6-d11d-4ff0-8347-f8c9abf23373",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPF_names = pd.read_parquet('../CleanData/SDC/GPF_names.parquet')\n",
    "MA = pd.read_parquet('../CleanData/SDC/M&A.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6898dd39-a0f2-47a4-9543-c9983530fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_fate(state,begin_year,end_year,begin_rank,end_rank):\n",
    "\n",
    "    name_GPFs = list(chain.from_iterable(list(np.array(\n",
    "        GPF[(GPF['sale_year']>=begin_year)&(GPF['sale_year']<=end_year)&(GPF['State']==state)][name_GPF_colnames]))))\n",
    "    name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "    name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "    name_GPFs = Counter(name_GPFs).most_common()\n",
    "    \n",
    "    table_data = []\n",
    "    for item in name_GPFs[begin_rank:end_rank]:\n",
    "        MA_frag = MA[MA['target']==item[0]].reset_index()\n",
    "        # Whether it is ever aquired\n",
    "        if len(MA_frag)==0:\n",
    "            table_data = table_data+[[str(item[1]),item[0].replace('&',' ')]]\n",
    "        elif len(MA_frag)==1:\n",
    "            acquiror = MA_frag['acquiror'][0]\n",
    "            MA_frag = MA[MA['target']==acquiror].reset_index()\n",
    "            # Check if parent is acquired again\n",
    "            if len(MA_frag)==0:\n",
    "                one_bank_data = [str(item[1]),item[0].replace('&',' ')+' $\\\\rightarrow$',acquiror.replace('&',' ')]\n",
    "                table_data = table_data+[one_bank_data]\n",
    "            else:\n",
    "                second_acquiror = MA_frag['acquiror'][0]\n",
    "                MA_frag = MA[MA['target']==second_acquiror].reset_index()\n",
    "                # Check if parent is acquired again\n",
    "                if len(MA_frag)==0:\n",
    "                    one_bank_data = [str(item[1]),item[0].replace('&',' ')+' $\\\\rightarrow$',acquiror.replace('&',' ')+\\\n",
    "                        ' $\\\\rightarrow$',second_acquiror.replace('&',' ')]\n",
    "                    table_data = table_data+[one_bank_data]\n",
    "                else:\n",
    "                    third_acquiror = MA_frag['acquiror'][0]\n",
    "                    MA_frag = MA[MA['target']==third_acquiror].reset_index()\n",
    "                    # Check if parent is acquired again\n",
    "                    if len(MA_frag)==0:\n",
    "                        one_bank_data = [str(item[1]),item[0].replace('&',' ')+' $\\\\rightarrow$',acquiror.replace('&',' ')+\\\n",
    "                            ' $\\\\rightarrow$',second_acquiror.replace('&',' ')+' $\\\\rightarrow$',third_acquiror.replace('&',' ')]\n",
    "                        table_data = table_data+[one_bank_data]\n",
    "                    else:\n",
    "                        fourth_acquiror = MA_frag['acquiror'][0]\n",
    "                        one_bank_data = [str(item[1]),item[0].replace('&',' ')+' $\\\\rightarrow$',acquiror.replace('&',' ')+\\\n",
    "                            ' $\\\\rightarrow$',second_acquiror.replace('&',' ')+' $\\\\rightarrow$',third_acquiror.replace('&',' ')+\\\n",
    "                            ' $\\\\rightarrow$',fourth_acquiror.replace('&',' ')]\n",
    "                        table_data = table_data+[one_bank_data]\n",
    "        elif len(MA_frag)==2:\n",
    "            pass\n",
    "    table_data = [row + [\"\"]*(5-len(row)) for row in table_data]\n",
    "    table_data = [['N Deals','Underwriter','First Acquiror','Second Acquiror','Third Acquiror','Fourth Acquiror']]+table_data\n",
    "    \n",
    "    latex_code = \"\"\"\n",
    "        \"\"\"\n",
    "    for row in table_data[:1]:\n",
    "        latex_code += \"\\\\vspace{2mm}\"\n",
    "        latex_code += \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "    latex_code += \"\\\\hline \\\\\\\\\\n\"\n",
    "    for row in table_data[1:]:\n",
    "        latex_code += \"\\\\vspace{2mm}\"\n",
    "        latex_code += \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    # Save the LaTeX code to a .tex file\n",
    "    with open(\"../Slides/tabs/fate_\"+state+\"_\"+str(begin_year)+\"_\"+str(end_year)+\"_\"+str(begin_rank)+\"_\"+str(end_rank)+\".tex\", \"w\") as f:\n",
    "        f.write(latex_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709a30a9-9a73-40f7-80b3-9bcf1931cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_fate('MO',1980,1990,0,10)\n",
    "export_fate('MO',1980,1990,10,20)\n",
    "export_fate('MO',1990,2000,0,10)\n",
    "export_fate('MO',1990,2000,10,20)\n",
    "export_fate('MO',2000,2010,0,10)\n",
    "export_fate('MO',2000,2010,10,20)\n",
    "export_fate('MO',2010,2020,0,10)\n",
    "export_fate('MO',2010,2020,10,20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
