{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2023a33d-359f-4520-9bab-54e82b735f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c3c95-8406-4fe8-8f75-365c796cf07e",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4994ea8-c5df-4d65-a432-b45a9d30e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPF\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "\n",
    "# CBs in SOD\n",
    "SOD = pd.read_csv('../CleanData/FDIC/0I_SOD.csv')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].str.replace(',','')\n",
    "SOD['DEPSUMBR'] = SOD['DEPSUMBR'].astype(int)\n",
    "\n",
    "# M&As among CBs in SOD\n",
    "SNL_in_SOD = pd.read_csv('../CleanData/FDIC/0I_SNL_in_SOD.csv')\n",
    "SNL_in_SOD['year'] = SNL_in_SOD['Completion Date'].str[:4].astype(int)\n",
    "SNL_in_SOD = SNL_in_SOD[SNL_in_SOD['Target']!=SNL_in_SOD['Buyer']]\n",
    "SNL_in_SOD = SNL_in_SOD[['Target','Buyer','year']]\n",
    "\n",
    "# Areas affected by underwriter M&A\n",
    "CSA_affected = pd.read_parquet('../CleanData/MAEvent/1B_CSA_affected.parquet')\n",
    "CBSA_affected = pd.read_parquet('../CleanData/MAEvent/1B_CBSA_affected.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc03d96-c700-4340-a746-af6c7b4e5083",
   "metadata": {},
   "source": [
    "# 2. Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d81a3fe-7933-4121-b2d4-af6cc3dec5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 2.89 s, total: 13.2 s\n",
      "Wall time: 55.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SNL_in_SOD_withchars = SNL_in_SOD.copy()\n",
    "\n",
    "SNL_in_SOD_withchars['both_active'] = False\n",
    "SNL_in_SOD_withchars['both_active_overlap_CSA'] = False\n",
    "\n",
    "# M&As where both underwrite municipal bonds right before merger\n",
    "def proc_list(SNL_in_SOD_withchars):\n",
    "    for idx,row in SNL_in_SOD_withchars.iterrows():\n",
    "        SOD_oneyear = SOD[SOD['year']==row['year']-1]\n",
    "        names = list(chain.from_iterable(list(np.array(SOD_oneyear[['name']]))))\n",
    "        names = list(set(names))\n",
    "        if (row['Target'] in names) and \\\n",
    "            (row['Buyer'] in names):\n",
    "            SNL_in_SOD_withchars.at[idx,'both_active'] = True\n",
    "    return SNL_in_SOD_withchars\n",
    "\n",
    "output_columns = proc_list(SNL_in_SOD_withchars[:3]).columns # Process one year to get columns\n",
    "SNL_in_SOD_withchars_dd = dd.from_pandas(SNL_in_SOD_withchars, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    SNL_in_SOD_withchars = SNL_in_SOD_withchars_dd.map_partitions(proc_list, \\\n",
    "        meta=pd.DataFrame(columns=output_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa07c4a-47d1-4da1-83d2-aff41ce09466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M&As where both underwirte municipal bonds before merger and have market overlap in terms of CSA\n",
    "def proc_list(SNL_in_SOD_withchars):\n",
    "    for idx,row in SNL_in_SOD_withchars.iterrows():\n",
    "        SOD_oneyear = SOD[SOD['year']==row['year']-1]\n",
    "        CSAs = list(SOD_oneyear['CSA Code'].unique())\n",
    "        CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "        for CSA in CSAs:\n",
    "            SOD_oneyearCSA = SOD_oneyear[SOD_oneyear['CSA Code']==CSA]\n",
    "            names = list(chain.from_iterable(list(np.array(SOD_oneyearCSA[['name']]))))\n",
    "            names = list(set(names))\n",
    "            # If for any CSA there is overlap, then there is overlap\n",
    "            if (row['Target'] in names) and \\\n",
    "                (row['Buyer'] in names):\n",
    "                SNL_in_SOD_withchars.at[idx,'both_active_overlap_CSA'] = True\n",
    "    return SNL_in_SOD_withchars\n",
    "\n",
    "output_columns = proc_list(SNL_in_SOD_withchars[:3]).columns # Process one year to get columns\n",
    "SNL_in_SOD_withchars_dd = dd.from_pandas(SNL_in_SOD_withchars, npartitions=20)\n",
    "with dask.config.set(scheduler='processes',num_workers=20):\n",
    "    SNL_in_SOD_withchars = SNL_in_SOD_withchars_dd.map_partitions(proc_list, \\\n",
    "        meta=pd.DataFrame(columns=output_columns)).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a82b12-7800-4b40-9f2e-6102f908c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CBMA_both_active = np.sum(SNL_in_SOD_withchars['both_active']==True)\n",
    "n_CBMA_both_active_overlap_CSA = np.sum(SNL_in_SOD_withchars['both_active_overlap_CSA']==True)\n",
    "\n",
    "# Number: Number of M&As where both sides are active #\n",
    "n_CBMA_both_active = '{:,}'.format(n_CBMA_both_active)\n",
    "with open('../Draft/nums/n_CBMA_both_active.tex','w') as file:\n",
    "    file.write(str(n_CBMA_both_active))\n",
    "\n",
    "# Number: Number of M&As where both sides are active and have geographic overlap #\n",
    "n_CBMA_both_active_overlap_CSA = '{:,}'.format(n_CBMA_both_active_overlap_CSA)\n",
    "with open('../Draft/nums/n_CBMA_both_active_overlap_CSA.tex','w') as file:\n",
    "    file.write(str(n_CBMA_both_active_overlap_CSA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb4b9c8-c243-4e62-b37b-0132d21c4f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,424'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_CBMA_both_active_overlap_CSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71a84b-7158-4c12-b1c2-c4751d1fa207",
   "metadata": {},
   "source": [
    "# 3. Identify CB Merger Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3650e9bf-e895-4ba0-8d85-55a72ded4606",
   "metadata": {},
   "source": [
    "## 3.1 Using CSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27fd174a-a8e0-4b25-b179-c756e4fcceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------#\n",
    "# Version 1: CSA, Delta HHI > 20 #\n",
    "#--------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.002:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI20 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#--------------------------------#\n",
    "# Version 2: CSA, Delta HHI > 30 #\n",
    "#--------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.003:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI30 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#--------------------------------#\n",
    "# Version 3: CSA, Delta HHI > 50 #\n",
    "#--------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.005:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#---------------------------------#\n",
    "# Version 4: CSA, Delta HHI > 100 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CSAs = SOD['CSA Code'].unique()\n",
    "CSAs = [item for item in CSAs if str(item)!='nan']\n",
    "for CSA in CSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CSA Code']==CSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.01:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CSA Code':CSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100 = pd.DataFrame(CB_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327602d5-5438-47e8-a560-6f5f71796928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number: Number of within-market CB M&As #\n",
    "n_CB_CSA_episodes_DeltaHHI100 = '{:,}'.format(len(CB_CSA_episodes_DeltaHHI100))\n",
    "with open('../Draft/nums/n_CB_CSA_episodes_DeltaHHI100.tex','w') as file:\n",
    "    file.write(str(n_CB_CSA_episodes_DeltaHHI100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2de2c-416d-40a4-a1d3-648b302f9595",
   "metadata": {},
   "source": [
    "## 3.2 Using CBSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d438ee27-db9b-4a18-83ca-afc1f1d5fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------#\n",
    "# Version 1: CBSA, Delta HHI > 20 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.002:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI20 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#---------------------------------#\n",
    "# Version 2: CBSA, Delta HHI > 30 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.003:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI30 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#---------------------------------#\n",
    "# Version 3: CBSA, Delta HHI > 50 #\n",
    "#---------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.005:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50 = pd.DataFrame(CB_episodes)\n",
    "\n",
    "#----------------------------------#\n",
    "# Version 4: CBSA, Delta HHI > 100 #\n",
    "#----------------------------------#\n",
    "\n",
    "# Note that deposits are not noisy. Use HHI implied by just one year.\n",
    "\n",
    "CB_episodes = []\n",
    "CBSAs = SOD['CBSA Code'].unique()\n",
    "CBSAs = [item for item in CBSAs if str(item)!='nan']\n",
    "for CBSA in CBSAs:\n",
    "\n",
    "    episode_start_year = 1900\n",
    "\n",
    "    for year in range(1995,2023):\n",
    "\n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # HHI in the year prior to M&A\n",
    "        SOD_prior = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "        \n",
    "        SOD_prior = SOD_prior.reset_index()\n",
    "        hhi_prior = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "        \n",
    "        # Get SNL deals of interest\n",
    "        Banks_in_SOD = SOD[(SOD['CBSA Code']==CBSA)&(SOD['year']==year-1)].copy()\n",
    "        Banks_in_SOD = list(Banks_in_SOD['name'].unique())\n",
    "        SNL_in_SOD_relevant = SNL_in_SOD[\n",
    "            (SNL_in_SOD['Target'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['Buyer'].isin(Banks_in_SOD))\n",
    "            &(SNL_in_SOD['year']>=year)\n",
    "            &(SNL_in_SOD['year']<=year+3)]\n",
    "\n",
    "        if len(SNL_in_SOD_relevant)>0:\n",
    "            for idx,row in SNL_in_SOD_relevant.iterrows():\n",
    "                SOD_prior.loc[SOD_prior['name']==row['Target'],'name'] = row['Buyer']\n",
    "            SOD_prior = SOD_prior.groupby('name').agg({'DEPSUMBR':sum})\n",
    "            hhi_post = np.sum((SOD_prior['DEPSUMBR']/np.sum(SOD_prior['DEPSUMBR']))**2)\n",
    "            if hhi_post-hhi_prior>0.01:\n",
    "                episode_start_year = year\n",
    "                CB_episodes = CB_episodes+[{'CBSA Code':CBSA,'episode_start_year':year,'hhi_dif':hhi_post-hhi_prior}]\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100 = pd.DataFrame(CB_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76acd3-52b6-40eb-af87-72dcb8127b34",
   "metadata": {},
   "source": [
    "## 3.3 Apply the restriction criteria: No significant IB mergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409d882a-b3f2-4525-8e35-6bf5b891c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CSA_episodes_DeltaHHI20['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI20.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI20.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI20 = CB_CSA_episodes_DeltaHHI20[~CB_CSA_episodes_DeltaHHI20['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI30['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI30.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI30.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI30 = CB_CSA_episodes_DeltaHHI30[~CB_CSA_episodes_DeltaHHI30['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI50.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI50.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI50 = CB_CSA_episodes_DeltaHHI50[~CB_CSA_episodes_DeltaHHI50['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100['if_contaminated'] = False\n",
    "for idx,row in CB_CSA_episodes_DeltaHHI100.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==row['CSA Code']]\n",
    "    CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CSA_episodes_DeltaHHI100.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CSA_episodes_DeltaHHI100 = CB_CSA_episodes_DeltaHHI100[~CB_CSA_episodes_DeltaHHI100['if_contaminated']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c6ee9d-819b-4b82-a8c5-4a8bc565508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CBSA_episodes_DeltaHHI20['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI20.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI20.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI20 = CB_CBSA_episodes_DeltaHHI20[~CB_CBSA_episodes_DeltaHHI20['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI30['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI30.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI30.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI30 = CB_CBSA_episodes_DeltaHHI30[~CB_CBSA_episodes_DeltaHHI30['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI50.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI50.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI50 = CB_CBSA_episodes_DeltaHHI50[~CB_CBSA_episodes_DeltaHHI50['if_contaminated']]\n",
    "\n",
    "\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100['if_contaminated'] = False\n",
    "for idx,row in CB_CBSA_episodes_DeltaHHI100.iterrows():\n",
    "    # Years for which potential control is treated itself\n",
    "    CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==row['CBSA Code']]\n",
    "    CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "        (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "    CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "    # Note that market share is on a rolling basis of every three years, so I only need to start from year -1\n",
    "    if len(set(list(range(row['episode_start_year']-1,row['episode_start_year']+5))).\\\n",
    "        intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "        # This potential control is treated\n",
    "        CB_CBSA_episodes_DeltaHHI100.at[idx,'if_contaminated'] = True\n",
    "\n",
    "CB_CBSA_episodes_DeltaHHI100 = CB_CBSA_episodes_DeltaHHI100[~CB_CBSA_episodes_DeltaHHI100['if_contaminated']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd32a15-00b5-48f1-865a-b7dde1c0c4a9",
   "metadata": {},
   "source": [
    "# 4. Assemble a Treatment-Control Matched Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3854d62-8d19-422a-9755-7019b180f97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 0 episodes.\n",
      "A control cannot be found for 0 episodes.\n",
      "A control cannot be found for 0 episodes.\n",
      "A control cannot be found for 0 episodes.\n"
     ]
    }
   ],
   "source": [
    "# There are multiple versions of episodes definiton (by market share or HHI, cutoff on implied HHI increases, etc.). I go over each\n",
    "# version here\n",
    "\n",
    "episodes_files = [\n",
    "    [CB_CSA_episodes_DeltaHHI20,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI20.csv',\n",
    "    ],\n",
    "    [CB_CSA_episodes_DeltaHHI30,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI30.csv',\n",
    "    ],\n",
    "    [CB_CSA_episodes_DeltaHHI50,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI50.csv',\n",
    "    ],\n",
    "    [CB_CSA_episodes_DeltaHHI100,1,\n",
    "        '../CleanData/MAEvent/CB_CSA_episodes_DeltaHHI100.csv',\n",
    "    ],\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    episodes = episodes_file[0].copy()\n",
    "    N_matches = episodes_file[1]\n",
    "    file_path = episodes_file[2]\n",
    "\n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "    CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "    CSA_Data = CSA_POP.merge(CSA_INC,on=['CSA Code','year'])\n",
    "    CSA_Data = CSA_Data[['CSA Code','year','inc','pop']]\n",
    "    Same_State_CSA_pairs = pd.read_csv(\"../CleanData/Demographics/0C_Same_State_CSA_pairs.csv\")\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CSA\n",
    "        CSA_Data_oneyear = CSA_Data[CSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CSA\n",
    "        CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "        if len(CSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "        # Remove other CSAs in the same state from potential matches\n",
    "        Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "        CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CSA_affected_frag = CSA_affected[CSA_affected['CSA Code']==subrow['CSA Code']]\n",
    "            CSA_affected_frag = CSA_affected_frag[(CSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CSA_affected_frag['target_market_share_N_avg']+CSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CSA_affected_frag_affected_years = list(CSA_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,5))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_Dollar_avg']\n",
    "        elif 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "        if 'target_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_Dollar_avg']\n",
    "        elif 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "        if 'other_targets_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_Dollar_avg']\n",
    "        elif 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==row['CSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CSA Code','sale_year','State','County',\n",
    "            'issuer_type','Issuer',\n",
    "            'avg_maturity','amount',\n",
    "            'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "            'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "            'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "            'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "            'underpricing_15to60','underpricing_15to30',\n",
    "            'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "            'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "            'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "            'if_callable','CB_Eligible',\n",
    "            'num_relationship',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        #------------------------------------#\n",
    "        # Some cross-sectional heterogeneity #\n",
    "        #------------------------------------#\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "        # code block)\n",
    "        mergers = CSA_affected[\n",
    "            (CSA_affected['CSA Code']==row['CSA Code'])&\n",
    "            (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['treated_csa'] = row['CSA Code'] # Used for constructing cohort X issuer FEs\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "            mergers = CSA_affected[\n",
    "                (CSA_affected['CSA Code']==item)&\n",
    "                (CSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_csa'] = row['CSA Code'] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "\n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa5ffbd-283f-4db7-9da1-89f4d07bf14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 4 episodes.\n",
      "A control cannot be found for 3 episodes.\n",
      "A control cannot be found for 3 episodes.\n",
      "A control cannot be found for 2 episodes.\n"
     ]
    }
   ],
   "source": [
    "# There are multiple versions of episodes definiton (by market share or HHI, cutoff on implied HHI increases, etc.). I go over each\n",
    "# version here\n",
    "\n",
    "episodes_files = [\n",
    "    [CB_CBSA_episodes_DeltaHHI20,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI20.csv',\n",
    "    ],\n",
    "    [CB_CBSA_episodes_DeltaHHI30,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI30.csv',\n",
    "    ],\n",
    "    [CB_CBSA_episodes_DeltaHHI50,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI50.csv',\n",
    "    ],\n",
    "    [CB_CBSA_episodes_DeltaHHI100,1,\n",
    "        '../CleanData/MAEvent/CB_CBSA_episodes_DeltaHHI100.csv',\n",
    "    ],\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    episodes = episodes_file[0].copy()\n",
    "    N_matches = episodes_file[1]\n",
    "    file_path = episodes_file[2]\n",
    "\n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    CBSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "    CBSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "    CBSA_Data = CBSA_POP.merge(CBSA_INC,on=['CBSA Code','year'])\n",
    "    CBSA_Data = CBSA_Data[['CBSA Code','year','inc','pop']]\n",
    "    Same_State_CBSA_pairs = pd.read_csv(\"../CleanData/Demographics/0C_Same_State_CBSA_pairs.csv\")\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this CBSA\n",
    "        CBSA_Data_oneyear = CBSA_Data[CBSA_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated CBSA\n",
    "        CBSA_Data_oneyear_frag = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']==row['CBSA Code']].copy()\n",
    "        if len(CBSA_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = CBSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = CBSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        CBSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "        CBSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        CBSA_Data_oneyear['inc'] = winsor2(CBSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        CBSA_Data_oneyear['pop'] = winsor2(CBSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = CBSA_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        CBSA_Data_oneyear['dist'] = CBSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[CBSA_Data_oneyear['CBSA Code']!=row['CBSA Code']]\n",
    "        # Remove other CBSAs in the same state from potential matches\n",
    "        Same_State_CBSAs = list(Same_State_CBSA_pairs[Same_State_CBSA_pairs['CBSA_1']==row['CBSA Code']]['CBSA_2'])\n",
    "        CBSA_Data_oneyear = CBSA_Data_oneyear[~CBSA_Data_oneyear['CBSA Code'].isin(Same_State_CBSAs)]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in CBSA_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            CBSA_affected_frag = CBSA_affected[CBSA_affected['CBSA Code']==subrow['CBSA Code']]\n",
    "            CBSA_affected_frag = CBSA_affected_frag[(CBSA_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (CBSA_affected_frag['target_market_share_N_avg']+CBSA_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            CBSA_affected_frag_affected_years = list(CBSA_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(CBSA_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['CBSA Code']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,5))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_Dollar_avg']\n",
    "        elif 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "        if 'target_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_Dollar_avg']\n",
    "        elif 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "        if 'other_targets_market_share_Dollar_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_Dollar_avg']\n",
    "        elif 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==row['CBSA Code'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CBSA Code','sale_year','State','County',\n",
    "            'issuer_type','Issuer',\n",
    "            'avg_maturity','amount',\n",
    "            'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "            'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "            'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "            'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "            'underpricing_15to60','underpricing_15to30',\n",
    "            'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "            'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "            'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "            'if_callable','CB_Eligible',\n",
    "            'num_relationship',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        #------------------------------------#\n",
    "        # Some cross-sectional heterogeneity #\n",
    "        #------------------------------------#\n",
    "\n",
    "        # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "        # code block)\n",
    "        mergers = CBSA_affected[\n",
    "            (CBSA_affected['CBSA Code']==row['CBSA Code'])&\n",
    "            (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "            (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "            ][['acquiror','target','acquiror_parent','target_parent',\n",
    "            'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "        mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "        # Whether the underwriter is the target bank in M&A\n",
    "        GPF_Seg['bank_is_target'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_target'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "            |(GPF_Seg['bank_is_target'])\n",
    "        # Whether the underwriter is the acquiror bank in M&A\n",
    "        GPF_Seg['bank_is_acquiror'] = False\n",
    "        for column in name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "        for column in parent_name_GPF_colnames:\n",
    "            GPF_Seg['bank_is_acquiror'] = \\\n",
    "            (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "            |(GPF_Seg['bank_is_acquiror'])\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['treated_cbsa'] = row['CBSA Code'] # Used for constructing cohort X issuer FEs\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['CBSA Code']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CBSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "            # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "            mergers = CBSA_affected[\n",
    "                (CBSA_affected['CBSA Code']==item)&\n",
    "                (CBSA_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CBSA_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_cbsa'] = row['CBSA Code'] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
