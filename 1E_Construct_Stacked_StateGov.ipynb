{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e900c01-94d5-47b1-8b57-58a50b110fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:07:31.553359Z",
     "iopub.status.busy": "2024-03-03T09:07:31.552957Z",
     "iopub.status.idle": "2024-03-03T09:07:32.477404Z",
     "shell.execute_reply": "2024-03-03T09:07:32.477138Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n",
    "\n",
    "try:\n",
    "    del(FUN_1E_CBSA_to_State_hhidif)\n",
    "except:\n",
    "    pass\n",
    "import FUN_1E_CBSA_to_State_hhidif\n",
    "importlib.reload(FUN_1E_CBSA_to_State_hhidif)\n",
    "from FUN_1E_CBSA_to_State_hhidif import FUN_1E_CBSA_to_State_hhidif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc7c82-cdca-4efc-9224-319c3d6ddc88",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c44eae5-cdab-4c05-81ea-abae84955299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:07:32.479176Z",
     "iopub.status.busy": "2024-03-03T09:07:32.478999Z",
     "iopub.status.idle": "2024-03-03T09:07:43.108299Z",
     "shell.execute_reply": "2024-03-03T09:07:43.107919Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPF\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "\n",
    "# Parent relationship\n",
    "GPF_names = pd.read_parquet('../CleanData/SDC/0H_GPF_Parent.parquet')\n",
    "\n",
    "# All M&As\n",
    "MA = pd.read_parquet('../CleanData/SDC/0B_M&A.parquet')\n",
    "MA = MA.merge(GPF_names.rename(columns={'name_GPF':'acquiror','parent_name':'acquiror_parent'}),on=['acquiror','sale_year'])\n",
    "MA = MA.reset_index(drop=True)\n",
    "\n",
    "#-------------#\n",
    "# Import CBSA #\n",
    "#-------------#\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "# \"CSA\" is for metropolitan and \"CBSA\" includes also those micropolitan\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSAData = CBSAData.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSAData = CBSAData.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSAData = CBSAData[CBSAData['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSAData['County'] = CBSAData['County'].str.upper()\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' COUNTY','')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' AND ',' & ')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace('.','',regex=False)\n",
    "CBSAData['CSA Code'] = CBSAData['CSA Code'].astype(float)\n",
    "CBSAData['CBSA Code'] = CBSAData['CBSA Code'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4bd74d-2431-4262-9ef0-ab638d118e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:07:43.110039Z",
     "iopub.status.busy": "2024-03-03T09:07:43.109907Z",
     "iopub.status.idle": "2024-03-03T09:08:00.628501Z",
     "shell.execute_reply": "2024-03-03T09:08:00.628094Z"
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------------------#\n",
    "# State Gov Fin data based on local government survey #\n",
    "#-----------------------------------------------------#\n",
    "\n",
    "# Construct a state X year amount of transfers to school districts/special districts/other governments\n",
    "GovFinData = pd.read_csv('../CleanData/GovFinSurvey/0G_GovFinData.csv', low_memory=False)\n",
    "\n",
    "GovFinData_part = GovFinData[GovFinData['Type Code']==4]\n",
    "SpecialDistFund = GovFinData_part.groupby(['State','Year4']).agg({'Total State IG Revenue':sum,'Total LTD Issued':sum,'Total Expenditure':sum})\n",
    "SpecialDistFund = SpecialDistFund.reset_index()\n",
    "SpecialDistFund['Year4'] = SpecialDistFund['Year4'].astype(int)\n",
    "SpecialDistFund = SpecialDistFund.rename(\n",
    "    columns={'Year4':'year','Total State IG Revenue':'Transfer to Special Dist',\n",
    "    'Total LTD Issued':'Total LTD Issued by Special Dist','Total Expenditure':'Total Expenditure by Special Dist'})\n",
    "\n",
    "GovFinData_part = GovFinData[GovFinData['Type Code']==5]\n",
    "SchoolDistFund = GovFinData_part.groupby(['State','Year4']).agg({'Total State IG Revenue':sum})\n",
    "SchoolDistFund = SchoolDistFund.reset_index()\n",
    "SchoolDistFund['Year4'] = SchoolDistFund['Year4'].astype(int)\n",
    "SchoolDistFund = SchoolDistFund.rename(columns={'Year4':'year','Total State IG Revenue':'Transfer to School Dist'})\n",
    "\n",
    "GovFinData_part = GovFinData[(GovFinData['Type Code']==1)|(GovFinData['Type Code']==2)|(GovFinData['Type Code']==3)]\n",
    "MTCFund = GovFinData_part.groupby(['State','Year4']).agg({'Total State IG Revenue':sum})\n",
    "MTCFund = MTCFund.reset_index()\n",
    "MTCFund['Year4'] = MTCFund['Year4'].astype(int)\n",
    "MTCFund = MTCFund.rename(columns={'Year4':'year','Total State IG Revenue':'Transfer to MTC'})\n",
    "\n",
    "States = list(GPF['State'].unique())\n",
    "pop_by_CBSA = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "pop_by_CBSA = pop_by_CBSA.rename(columns={'year':'sale_year'})\n",
    "pop_by_CSA = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "pop_by_CSA = pop_by_CSA.rename(columns={'year':'sale_year'})\n",
    "\n",
    "States = [State for State in States if State!='HI']\n",
    "States = [State for State in States if State!=None]\n",
    "States = [State for State in States if str(State)!='nan']\n",
    "States = [State for State in States if State!='N']\n",
    "States = [State for State in States if State!='F']\n",
    "States = [State for State in States if State!='N\\nN']\n",
    "\n",
    "#-----------------------------------------------------#\n",
    "# State Gov Fin data based on state government survey #\n",
    "#-----------------------------------------------------#\n",
    "\n",
    "StateGovFinData = pd.read_csv('../CleanData/GovFinSurvey/0G_StateGovFinData.csv',low_memory=False)\n",
    "StateGovFinData['Year4'] = StateGovFinData['Year4'].astype(int)\n",
    "StateGovFinData = StateGovFinData.rename(columns={'Year4':'calendar_year'})\n",
    "StateGovFinData = StateGovFinData.merge(SpecialDistFund.rename(columns={'year':'calendar_year'}),on=['State','calendar_year'])\n",
    "StateGovFinData = StateGovFinData.merge(SchoolDistFund.rename(columns={'year':'calendar_year'}),on=['State','calendar_year'])\n",
    "StateGovFinData = StateGovFinData.merge(MTCFund.rename(columns={'year':'calendar_year'}),on=['State','calendar_year'])\n",
    "\n",
    "#--------------------------------------------------------------#\n",
    "# Amount of debt issued by state authorities, according to GPF #\n",
    "#--------------------------------------------------------------#\n",
    "\n",
    "GPF_Raw = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "GPF_State = GPF_Raw[(GPF_Raw['County']=='STATE AUTHORITY')|(GPF_Raw['County']=='STATE')].groupby(['State','sale_year']).agg({'amount':sum}).reset_index()\n",
    "GPF_State = GPF_State.rename(columns={'sale_year':'calendar_year'})\n",
    "GPF_State = GPF_State.rename(columns={'amount':'amount_GPF'})\n",
    "StateGovFinData = StateGovFinData.merge(GPF_State,on=['State','calendar_year'],how='outer',indicator=True)\n",
    "StateGovFinData = StateGovFinData[StateGovFinData['_merge']!='right_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c3daa-c52d-4e3e-a835-9ca406c02598",
   "metadata": {},
   "source": [
    "# 2. Construct the state-level implied increase in HHI, Using CBSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546560d6-c5a0-43ad-bf30-7792b8f6e28b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- For each state $\\times$ year, calculate the weighted average implied Delta HHI using the prior three years' data. A state might have multiple CBSAs, each with certain implied Delta HHI. I calculate the average using the population as the weight. Then, I look for state $\\times$ year where this Delta HHI is above a certain threshold. I say such states are treated in those years.\n",
    "- I use CBSA instead of CBSA as CBSA provides a more complete coverage of all places in a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab422837-d36e-42d5-9881-3efa58241fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:08:00.630329Z",
     "iopub.status.busy": "2024-03-03T09:08:00.630226Z",
     "iopub.status.idle": "2024-03-03T09:10:37.641585Z",
     "shell.execute_reply": "2024-03-03T09:10:37.641185Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.5 s, sys: 6.33 s, total: 46.9 s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate increase in HHI for every state X year\n",
    "\n",
    "GPF = GPF[~pd.isnull(GPF['sale_year'])]\n",
    "input_list = [(State,GPF,MA,raw_name_GPF_colnames,pop_by_CBSA) for State in States]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes = 10) as p:\n",
    "        CBSA_all_state_hhi_dif = p.starmap(FUN_1E_CBSA_to_State_hhidif, input_list)\n",
    "CBSA_all_state_hhi_dif = pd.concat(CBSA_all_state_hhi_dif)\n",
    "\n",
    "CBSA_all_state_hhi_dif = CBSA_all_state_hhi_dif[~pd.isnull(CBSA_all_state_hhi_dif['state_hhi_dif'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42d991-75a4-488d-9886-c5dac9ccf9d0",
   "metadata": {},
   "source": [
    "# 3. State-level episode based on CBSA to state level HHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1769dab-22c8-43a4-8071-15bbb187f8d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:10:37.643342Z",
     "iopub.status.busy": "2024-03-03T09:10:37.643237Z",
     "iopub.status.idle": "2024-03-03T09:10:38.089166Z",
     "shell.execute_reply": "2024-03-03T09:10:38.088911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find state level M&A episodes\n",
    "\n",
    "CBSA_all_state_hhi_dif = pd.DataFrame(CBSA_all_state_hhi_dif)\n",
    "\n",
    "State_episodes_impliedHHI_N = []\n",
    "\n",
    "for State in States:\n",
    "    \n",
    "    episode_start_year = 1970\n",
    "    for year in range(1970,2023):\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if year<=episode_start_year+4:\n",
    "            continue\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        state_hhi_dif = CBSA_all_state_hhi_dif[(CBSA_all_state_hhi_dif['State']==State)&(CBSA_all_state_hhi_dif['year']==year)]\n",
    "        state_hhi_dif = state_hhi_dif.reset_index(drop=True)\n",
    "        if len(state_hhi_dif)==1:\n",
    "            state_hhi_dif = state_hhi_dif['state_hhi_dif'][0]\n",
    "        else:\n",
    "            state_hhi_dif = 0\n",
    "        if state_hhi_dif>0.01:\n",
    "            # An episode is identified\n",
    "            State_episodes_impliedHHI_N = State_episodes_impliedHHI_N+[{\n",
    "                'episode_start_year':year,\n",
    "                'State':State,\n",
    "                'state_hhi_dif':state_hhi_dif,\n",
    "                }]\n",
    "            episode_start_year = year\n",
    "\n",
    "State_episodes_impliedHHI_N = pd.DataFrame(State_episodes_impliedHHI_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9f7f40-381f-4c97-ab5b-1ea52b64c673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:10:38.090627Z",
     "iopub.status.busy": "2024-03-03T09:10:38.090556Z",
     "iopub.status.idle": "2024-03-03T09:10:41.006929Z",
     "shell.execute_reply": "2024-03-03T09:10:41.006528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate state level income and population\n",
    "\n",
    "pop_by_CBSA = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "inc_by_CBSA = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "popincinfo = []\n",
    "for State in States:\n",
    "    CBSAsinState = CBSAData[CBSAData['State']==State][['CBSA Code']].drop_duplicates()\n",
    "    for year in range(1970,2023):\n",
    "        one_stateyear = CBSAsinState.merge(inc_by_CBSA[inc_by_CBSA['year']==year],on='CBSA Code')\n",
    "        one_stateyear = one_stateyear.merge(pop_by_CBSA[pop_by_CBSA['year']==year],on='CBSA Code')\n",
    "        if len(one_stateyear)==0:\n",
    "            continue\n",
    "        pop = np.sum(one_stateyear['pop'])\n",
    "        inc = np.dot(one_stateyear['pop'],one_stateyear['inc'])/pop\n",
    "        popincinfo = popincinfo+[{'State':State,'year':year,'pop':pop,'inc':inc}]\n",
    "popincinfo = pd.DataFrame(popincinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7251fb-574f-4b37-aed2-d44c69abfcb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-03T09:10:41.008569Z",
     "iopub.status.busy": "2024-03-03T09:10:41.008473Z",
     "iopub.status.idle": "2024-03-03T09:10:41.673814Z",
     "shell.execute_reply": "2024-03-03T09:10:41.673472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 76 episodes.\n",
      "Match cannot be found for 0 episodes.\n"
     ]
    }
   ],
   "source": [
    "# Number of matches to use\n",
    "n_matches = 1\n",
    "\n",
    "def calculate_distance(row,weightingmat):\n",
    "    return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "        (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "\n",
    "State_episodes_impliedHHI_N['control'] = None\n",
    "for idx,row in State_episodes_impliedHHI_N.iterrows():\n",
    "\n",
    "    # Pop/inc information for a certain year\n",
    "    one_year = popincinfo[popincinfo['year']==row['episode_start_year']].copy()\n",
    "\n",
    "    # Demographic data of the treated state\n",
    "    stateyear = one_year[one_year['State']==row['State']].copy()\n",
    "    if len(stateyear)==0:\n",
    "        continue\n",
    "    episode_pop = stateyear.reset_index()['pop'][0]\n",
    "    episode_inc = stateyear.reset_index()['inc'][0]\n",
    "    \n",
    "    # Find a match\n",
    "    one_year['treated_pop'] = episode_pop\n",
    "    one_year['treated_inc'] = episode_inc\n",
    "    # Get weighting matrix\n",
    "    one_year['inc'] = winsor2(one_year['inc'],cutoffs=[0.05,0.05])\n",
    "    one_year['pop'] = winsor2(one_year['pop'],cutoffs=[0.05,0.05])\n",
    "    cov = one_year[['inc','pop']].cov()\n",
    "    invcov = np.linalg.inv(cov)\n",
    "    one_year['dist'] = one_year.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "    one_year = one_year.sort_values('dist').reset_index(drop=True)\n",
    "    # Remove oneself from potential matches\n",
    "    one_year = one_year[one_year['State']!=row['State']]\n",
    "\n",
    "    match_counter = 0\n",
    "    control = []\n",
    "    for subidx,subrow in one_year.iterrows():\n",
    "        # Years for which potential control is treated itself\n",
    "        one_state_hhi_dif = CBSA_all_state_hhi_dif[CBSA_all_state_hhi_dif['State']==subrow['State']]\n",
    "        one_state_hhi_dif = one_state_hhi_dif[(one_state_hhi_dif['state_hhi_dif']>0.001)]\n",
    "        one_state_affectedyears = list(one_state_hhi_dif['year'].unique())\n",
    "        if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "            intersection(set(one_state_affectedyears)))>0:\n",
    "            # This potential control is treated\n",
    "            continue\n",
    "        else:\n",
    "            # This potential control is not treated => Good control\n",
    "            control = control+[subrow['State']]\n",
    "            match_counter = match_counter+1\n",
    "            if match_counter==n_matches:\n",
    "                break\n",
    "\n",
    "    if len(control)>0:\n",
    "        State_episodes_impliedHHI_N.at[idx,'control'] = control\n",
    "    else:\n",
    "        State_episodes_impliedHHI_N.at[idx,'control'] = None\n",
    "\n",
    "print(\"There are a total of \"+str(len(State_episodes_impliedHHI_N))+\" episodes.\")\n",
    "print(\"Match cannot be found for \"+str(np.sum(pd.isnull(State_episodes_impliedHHI_N['control'])))+\" episodes.\")\n",
    "State_episodes_impliedHHI_N = State_episodes_impliedHHI_N[~pd.isnull(State_episodes_impliedHHI_N['control'])]\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Expand to include an event time dimension #\n",
    "#############################################\n",
    "\n",
    "State_episodes_impliedHHI_N['cohort_idx'] = np.array(range(0,len(State_episodes_impliedHHI_N)))\n",
    "episodes_Exploded = State_episodes_impliedHHI_N\n",
    "episodes_Exploded['year_to_merger'] = [list(range(-4,5))]*len(episodes_Exploded)\n",
    "episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "\n",
    "################################\n",
    "# Assemble a regression sample #\n",
    "################################\n",
    "\n",
    "episodes_Exploded_Treated = episodes_Exploded[['episode_start_year','State','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "episodes_Exploded_Treated['Treated'] = 1\n",
    "episodes_Exploded_Treated = episodes_Exploded_Treated.reset_index(drop=True)\n",
    "if n_matches==1:\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][0]\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded = pd.concat([episodes_Exploded_Treated,episodes_Exploded_Control])\n",
    "elif n_matches==2:\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][0]\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded_Control_1 = episodes_Exploded_Control\n",
    "\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        if len(episodes_Exploded_Control.at[idx,'control'])==2:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][1]\n",
    "        else:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = None\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control[~pd.isnull(episodes_Exploded_Control['State'])]\n",
    "    episodes_Exploded_Control_2 = episodes_Exploded_Control\n",
    "elif n_matches==3:\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][0]\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded_Control_1 = episodes_Exploded_Control\n",
    "\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        if len(episodes_Exploded_Control.at[idx,'control'])>=2:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][1]\n",
    "        else:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = None\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control[~pd.isnull(episodes_Exploded_Control['State'])]\n",
    "    episodes_Exploded_Control_2 = episodes_Exploded_Control\n",
    "\n",
    "    episodes_Exploded_Control = episodes_Exploded[['episode_start_year','control','state_hhi_dif','year_to_merger','calendar_year','cohort_idx']]\n",
    "    episodes_Exploded_Control['Treated'] = 0\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.reset_index(drop=True)\n",
    "    for idx,row in episodes_Exploded_Control.iterrows():\n",
    "        if len(episodes_Exploded_Control.at[idx,'control'])==3:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = episodes_Exploded_Control.at[idx,'control'][2]\n",
    "        else:\n",
    "            episodes_Exploded_Control.at[idx,'control'] = None\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control.rename(columns={'control':'State'})\n",
    "    episodes_Exploded_Control = episodes_Exploded_Control[~pd.isnull(episodes_Exploded_Control['State'])]\n",
    "    episodes_Exploded_Control_3 = episodes_Exploded_Control\n",
    "\n",
    "    episodes_Exploded = pd.concat([episodes_Exploded_Treated,episodes_Exploded_Control_1,episodes_Exploded_Control_2,episodes_Exploded_Control_3])\n",
    "\n",
    "# Export data\n",
    "episodes_Exploded = episodes_Exploded.merge(StateGovFinData,on=['State','calendar_year'])\n",
    "episodes_Exploded.to_csv('../CleanData/MAEvent/GovFin_State_from_CBSA.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
