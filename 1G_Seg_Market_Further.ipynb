{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec4e4ae-7eeb-4a4b-bc00-cbf9c4c9f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import itertools\n",
    "from itertools import chain\n",
    "from math import sqrt, floor, ceil, isnan\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "import importlib\n",
    "from importlib import reload\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_colwidth = 400\n",
    "\n",
    "# A customized winsorisation function that handles None values correctly\n",
    "# The percentiles are taken and winsorisation are done on non-None values only\n",
    "def winsor2(series,cutoffs):\n",
    "\n",
    "    import numpy as np\n",
    "    import scipy as sp\n",
    "    \n",
    "    IsNone = np.isnan(series).copy()\n",
    "    IsNotNone = np.logical_not(IsNone).copy()\n",
    "    series_NotNonePart = sp.stats.mstats.winsorize(series[IsNotNone],limits=(cutoffs[0],cutoffs[1]))\n",
    "    series_new = series.copy()\n",
    "    series_new[IsNone] = np.nan\n",
    "    series_new[IsNotNone] = series_NotNonePart\n",
    "\n",
    "    return series_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb758e8-fd6c-4746-adb8-96e901b4ec81",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f56786-fb8f-4d9a-8cae-d0bde7ea8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPF\n",
    "GPF = pd.read_csv(\"../CleanData/SDC/0A_GPF.csv\",low_memory=False)\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = [column for column in GPF.columns if column[:9]=='name_GPF_']\n",
    "parent_name_GPF_colnames = [column for column in GPF.columns if 'parent_name_' in column]\n",
    "\n",
    "# Parent relationship\n",
    "GPF_names = pd.read_parquet('../CleanData/SDC/0H_GPF_Parent.parquet')\n",
    "\n",
    "# HHI and market share of each underwriter\n",
    "HHI_byCSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCSA.csv')\n",
    "market_share_all_markets_byCSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCSA.csv')\n",
    "HHI_byCBSA = pd.read_csv('../CleanData/SDC/1A_HHI_byCBSA.csv')\n",
    "market_share_all_markets_byCBSA = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCBSA.csv')\n",
    "HHI_byState = pd.read_csv('../CleanData/SDC/1A_HHI_byState.csv')\n",
    "market_share_all_markets_byState = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byState.csv')\n",
    "\n",
    "# Portfolio weights of CSAs within underwriter\n",
    "csa_share_withinbank = pd.read_csv('../CleanData/SDC/1A_csa_share_withinbank.csv')\n",
    "\n",
    "# All M&As\n",
    "MA = pd.read_parquet('../CleanData/SDC/0B_M&A.parquet')\n",
    "MA = MA.reset_index(drop=True)\n",
    "\n",
    "# Withdrawn M&As\n",
    "MA_withdrawn = pd.read_csv(\"../CleanData/SDC/0I_MA_withdrawn.csv\")\n",
    "\n",
    "# Quantity of issuance\n",
    "StateXCountyXBid = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXBid.parquet\")\n",
    "StateXCountyXUsageBB = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageBB.parquet\")\n",
    "StateXCountyXUsageGeneral = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageGeneral.parquet\")\n",
    "StateXCountyXUsageMain = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXUsageMain.parquet\")\n",
    "StateXCountyXIssuerType = pd.read_parquet(\"../CleanData/SDC/0A_StateXCountyXIssuerType.parquet\")\n",
    "\n",
    "StateXCounty = StateXCountyXBid.groupby(['State','County','sale_year']).agg({'amount':sum})\n",
    "StateXCounty = StateXCounty.reset_index()\n",
    "\n",
    "# Demographics\n",
    "CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "CBSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Pop.csv\")\n",
    "CBSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CBSA_Inc.csv\")\n",
    "State_POP = pd.read_csv(\"../CleanData/Demographics/0C_State_Pop.csv\")\n",
    "State_INC = pd.read_csv(\"../CleanData/Demographics/0C_State_Inc.csv\")\n",
    "\n",
    "#-------------#\n",
    "# Import CBSA #\n",
    "#-------------#\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "# \"CSA\" is for metropolitan and \"CBSA\" includes also those micropolitan\n",
    "CBSAData = pd.read_excel(\"../RawData/MSA/CBSA.xlsx\",skiprows=[0,1])\n",
    "CBSAData = CBSAData[~pd.isnull(CBSAData['County/County Equivalent'])]\n",
    "\n",
    "# Add state abbreviations\n",
    "us_state_to_abbrev = pd.DataFrame.from_dict(us_state_to_abbrev,orient='index').reset_index()\n",
    "us_state_to_abbrev.columns = ['State Name','State']\n",
    "CBSAData = CBSAData.rename(columns={'County/County Equivalent':'County'})\n",
    "CBSAData = CBSAData.merge(us_state_to_abbrev,on='State Name',how='outer',indicator=True)\n",
    "CBSAData = CBSAData[CBSAData['_merge']=='both'].drop(columns=['_merge'])\n",
    "# Merge is perfect\n",
    "CBSAData['County'] = CBSAData['County'].str.upper()\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' COUNTY','')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace(' AND ',' & ')\n",
    "CBSAData['County'] = CBSAData['County'].str.replace('.','',regex=False)\n",
    "CBSAData['CSA Code'] = CBSAData['CSA Code'].astype(float)\n",
    "CBSAData['CBSA Code'] = CBSAData['CBSA Code'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe0c96-73fd-4418-975c-769a35abee57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222e1376-6d33-4441-900d-f4fb5cc0ab80",
   "metadata": {},
   "source": [
    "# 2. Construct Events of M&As, Using State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5bc32-5d80-46b1-9f55-f6300bb412a0",
   "metadata": {},
   "source": [
    "## 2.1 Find states affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c523d1-0395-4a94-bb78-d8606f539ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# %%script false --no-raise-error\n",
    "\n",
    "def proc_list(MA_frag):\n",
    "    \n",
    "    raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "    name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "    \n",
    "    State_affected = []\n",
    "    MA_frag = MA_frag.reset_index(drop=True)\n",
    "    \n",
    "    for idx,row in MA_frag.iterrows():\n",
    "        \n",
    "        # Find States that this merger affects\n",
    "        # Determine if an underwriter is active in an State based on activity of PRIOR years\n",
    "        GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "\n",
    "        # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "        # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "        # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "        other_targets = \\\n",
    "            list(MA[(MA['acquiror']==row['acquiror'])&\n",
    "            (MA['sale_year']==row['sale_year'])&\n",
    "            (MA['target']!=row['target'])]['target'])\n",
    "        \n",
    "        for State in list(GPF_prioryears['State'].unique()):\n",
    "\n",
    "            GPF_prioryears_oneState = GPF_prioryears[GPF_prioryears['State']==State]\n",
    "\n",
    "            # Underwriters in this state\n",
    "            underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneState[name_GPF_colnames]))))\n",
    "            underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "            underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "            # Parents of underwriters in this state\n",
    "            parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneState[parent_name_colnames]))))\n",
    "            parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "            parents_priorMA = list(set(parents_priorMA))\n",
    "            # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "            subsidiaries_priorMA = list(GPF_names[\n",
    "                (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "\n",
    "            # Determine if merger affects the State, and if both sides have business\n",
    "            IF_acquiror_active = None\n",
    "            IF_target_active = None\n",
    "            IF_other_target_active = None\n",
    "            if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                IF_acquiror_active = True\n",
    "            if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                IF_target_active = True\n",
    "            for other_target in other_targets:\n",
    "                if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                    IF_other_target_active = True\n",
    "\n",
    "            # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "            # share \"market_share_all_markets_byState\" is calculated at the parent level. There are many cases where market share of a\n",
    "            # firm in an area is unavailable, which is because of no presence.\n",
    "\n",
    "\n",
    "\n",
    "            #-------------------------#\n",
    "            # Market share by N deals #\n",
    "            #-------------------------#\n",
    "\n",
    "            # (1) Market share of acquiror\n",
    "            # Determine parent of target, as \"market_share_all_markets_byState\" is at parent level\n",
    "            try:\n",
    "                # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                # Situation where acquiror is a parent\n",
    "                acquiror_parent = row['acquiror']\n",
    "            try:\n",
    "                acquiror_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m1 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m2 = 0\n",
    "            try:\n",
    "                acquiror_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==acquiror_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                acquiror_market_share_N_m3 = 0\n",
    "\n",
    "            # (2) Market share of target\n",
    "            try:\n",
    "                # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['parent_name'][0]\n",
    "            except:\n",
    "                target_parent = row['target']\n",
    "            try:\n",
    "                target_market_share_N_m1 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-1)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m1 = 0\n",
    "            try:\n",
    "                target_market_share_N_m2 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-2)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m2 = 0\n",
    "            try:\n",
    "                target_market_share_N_m3 = \\\n",
    "                    market_share_all_markets_byState[\n",
    "                    (market_share_all_markets_byState['parent_name']==target_parent)\n",
    "                    &(market_share_all_markets_byState['State']==State)\n",
    "                    &(market_share_all_markets_byState['calendar_year']==row['sale_year']-3)]\\\n",
    "                    .reset_index()['market_share_N'][0]\n",
    "            except:\n",
    "                target_market_share_N_m3 = 0\n",
    "\n",
    "            # (3) Market share of other targets in the same transaction\n",
    "            # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "            other_targets_parents = \\\n",
    "                list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                list(other_targets)\n",
    "            other_targets_parents = list(set(other_targets_parents))\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byState[\n",
    "                (market_share_all_markets_byState['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byState['State']==State)\n",
    "                &(market_share_all_markets_byState['calendar_year']==row['sale_year']-1)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m1 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byState[\n",
    "                (market_share_all_markets_byState['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byState['State']==State)\n",
    "                &(market_share_all_markets_byState['calendar_year']==row['sale_year']-2)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m2 = 0\n",
    "\n",
    "            other_targets_market_share_N = \\\n",
    "                market_share_all_markets_byState[\n",
    "                (market_share_all_markets_byState['parent_name'].isin(other_targets_parents))\n",
    "                &(market_share_all_markets_byState['State']==State)\n",
    "                &(market_share_all_markets_byState['calendar_year']==row['sale_year']-3)]\n",
    "            if len(other_targets_market_share_N)>0:\n",
    "                other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "            else:\n",
    "                other_targets_market_share_N_m3 = 0\n",
    "\n",
    "\n",
    "\n",
    "            # Record data\n",
    "            if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                State_affected = State_affected+[{\n",
    "                    'State':State,\n",
    "                    'sale_year':row['sale_year'],\n",
    "                    'acquiror':row['acquiror'],\n",
    "                    'target':row['target'],\n",
    "                    'other_targets':other_targets,\n",
    "                    'acquiror_parent':acquiror_parent,\n",
    "                    'target_parent':target_parent,\n",
    "                    'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                    'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                    'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                    'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                    'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                    'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                    'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                    'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                    'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                }]\n",
    "            acquiror_market_share_N_m1 = None\n",
    "            acquiror_market_share_N_m2 = None\n",
    "            acquiror_market_share_N_m3 = None\n",
    "            target_market_share_N_m1 = None\n",
    "            target_market_share_N_m2 = None\n",
    "            target_market_share_N_m3 = None\n",
    "            other_targets_market_share = None\n",
    "            other_targets_market_share_N_m1 = None\n",
    "            other_targets_market_share_N_m2 = None\n",
    "            other_targets_market_share_N_m3 = None\n",
    "    \n",
    "    State_affected = pd.DataFrame(State_affected)\n",
    "    return State_affected\n",
    "\n",
    "MA_dd = dd.from_pandas(MA, npartitions=10)\n",
    "with dask.config.set(scheduler='processes',num_workers=10):\n",
    "    State_affected = MA_dd.map_partitions(proc_list, \n",
    "    meta=pd.DataFrame(columns=\n",
    "    ['State','sale_year','acquiror','target',\n",
    "    'other_targets','acquiror_parent','target_parent',\n",
    "    'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "    'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "    'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "    ])).compute()\n",
    "\n",
    "# Average market share over past three years\n",
    "State_affected['acquiror_market_share_N_avg'] = \\\n",
    "    (State_affected['acquiror_market_share_N_m1']+\\\n",
    "    State_affected['acquiror_market_share_N_m2']+\\\n",
    "    State_affected['acquiror_market_share_N_m3'])/3\n",
    "State_affected['target_market_share_N_avg'] = \\\n",
    "    (State_affected['target_market_share_N_m1']+\\\n",
    "    State_affected['target_market_share_N_m2']+\\\n",
    "    State_affected['target_market_share_N_m3'])/3\n",
    "State_affected['other_targets_market_share_N_avg'] = \\\n",
    "    (State_affected['other_targets_market_share_N_m1']+\\\n",
    "    State_affected['other_targets_market_share_N_m2']+\\\n",
    "    State_affected['other_targets_market_share_N_m3'])/3\n",
    "\n",
    "# As this step takes significant time, export output\n",
    "State_affected.to_parquet('../CleanData/MAEvent/1B_State_affected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2273c8e-d465-48e5-8620-56d7e47f9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "State_affected = pd.read_parquet('../CleanData/MAEvent/1B_State_affected.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2ad52-391d-4c39-9267-0619904fe499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d17b1b9-401a-4a38-915d-4fd1ea154310",
   "metadata": {},
   "source": [
    "## 2.2 Identify merger episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4dcb4-b39f-49ec-9249-3732b19eb207",
   "metadata": {},
   "source": [
    "### 2.2.1 Method 1: By market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127c644-ea09-456a-b956-525f0c7ef8d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------#\n",
    "# Market share by N of deals #\n",
    "#----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the State level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"State_affected\"\n",
    "\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "State_episodes_marketshare_N = []\n",
    "\n",
    "for State in list(State_affected['State'].unique()):\n",
    "\n",
    "    State_affected_part = State_affected[State_affected['State']==State]\n",
    "    State_affected_part = State_affected_part[\n",
    "        (State_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((State_affected_part['target_market_share_N_avg']>0)|\n",
    "        (State_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    years = State_affected_part['sale_year'].unique()\n",
    "    years = sorted(years)\n",
    "    for sale_year in years:\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        State_affected_episode = State_affected_part[(State_affected_part['sale_year']>=sale_year)&(State_affected_part['sale_year']<=sale_year+3)]\n",
    "        # When a firm acquires multiple firms, market share of other firms are in \"other_targets_market_share_N_avg\", so just keeping one record\n",
    "        # is sufficient\n",
    "        State_affected_episode = State_affected_episode.drop_duplicates(['acquiror','sale_year'])\n",
    "        # Alternative aggregation methods might be more reasonable. Also, this does not account for that target tends to be smaller so threshold\n",
    "        # for them should be smaller too. Even better, can compute the implied-HHI change (based on historical data) of this merger, and put threshold\n",
    "        # on that, which is definitely more powerful.\n",
    "        acquiror_market_share_N_avg = np.sum(State_affected_episode['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_avg = np.sum(State_affected_episode['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_avg = np.sum(State_affected_episode['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if acquiror_market_share_N_avg>0.05 and target_market_share_N_avg+other_targets_market_share_N_avg>0.05:\n",
    "            # An episode is identified\n",
    "            State_episodes_marketshare_N = State_episodes_marketshare_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'State':State,\n",
    "                'mergers':State_affected_episode,\n",
    "                'acquiror_market_share_N_avg':acquiror_market_share_N_avg,\n",
    "                'target_market_share_N_avg':target_market_share_N_avg,\n",
    "                'other_targets_market_share_N_avg':other_targets_market_share_N_avg,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "State_episodes_marketshare_N = pd.DataFrame(State_episodes_marketshare_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717bd741-f1e8-4829-856a-1384bcd4c721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad39dd7-647e-4969-ae45-86fc4369c40e",
   "metadata": {},
   "source": [
    "### 2.2.2 Method 2: By implied rise in HHI due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88884a-28fe-449c-8a9b-0fd2ca027c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------#\n",
    "# Change in HHI by N of deals #\n",
    "#-----------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the State level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"State_affected\"\n",
    "\n",
    "State_episodes_impliedHHI_N = []\n",
    "\n",
    "for State in list(State_affected['State'].unique()):\n",
    "\n",
    "    State_affected_part = State_affected[State_affected['State']==State]\n",
    "    State_affected_part = State_affected_part[\n",
    "        (State_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((State_affected_part['target_market_share_N_avg']>0)|\n",
    "        (State_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in State_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        State_affected_episode = State_affected_part[(State_affected_part['sale_year']>=sale_year)&(State_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneState_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['State']==State)]\n",
    "        \n",
    "        # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneState_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneState_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # HHI prior to merger\n",
    "        hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "\n",
    "        # Implied HHI post merger\n",
    "        State_affected_episode = State_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in State_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        hhi_dif = hhi_predicted-hhi_piror\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if hhi_dif>0.01:\n",
    "            # An episode is identified\n",
    "            State_episodes_impliedHHI_N = State_episodes_impliedHHI_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'State':State,\n",
    "                'mergers':State_affected_episode,\n",
    "                'hhi_dif':hhi_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "State_episodes_impliedHHI_N = pd.DataFrame(State_episodes_impliedHHI_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799f794-37e8-4669-b9d8-ca766bb33f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(State_episodes_impliedHHI_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c168d-d230-4319-8cea-2e51ae2dbfb0",
   "metadata": {},
   "source": [
    "### 2.2.3 Method 3: By implied rise in top 5 share due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553d401-b0dc-48ce-a1a8-12cdf6c0aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------#\n",
    "# Change in top 5 share by N of deals #\n",
    "#-------------------------------------#\n",
    "\n",
    "# Identify episodes of mergers at the State level\n",
    "\n",
    "# Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "# identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "# the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "# as in cases where two firms merge into a new one, both will get recorded in \"State_affected\"\n",
    "\n",
    "State_episodes_top5share_N = []\n",
    "\n",
    "for State in list(State_affected['State'].unique()):\n",
    "\n",
    "    State_affected_part = State_affected[State_affected['State']==State]\n",
    "    State_affected_part = State_affected_part[\n",
    "        (State_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "        ((State_affected_part['target_market_share_N_avg']>0)|\n",
    "        (State_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "    \n",
    "    episode_start_year = 1900\n",
    "    for sale_year in State_affected_part['sale_year'].unique():\n",
    "    \n",
    "        # If this year is still within the last merger episode\n",
    "        if sale_year<=episode_start_year+4:\n",
    "            continue\n",
    "        \n",
    "        # Check intensity of M&A activities in that year and three years following\n",
    "        State_affected_episode = State_affected_part[(State_affected_part['sale_year']>=sale_year)&(State_affected_part['sale_year']<=sale_year+3)]\n",
    "        GPF_oneState_priorMA = GPF[(GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)&(GPF['State']==State)]\n",
    "        \n",
    "        # Calculate (1) Top 5 share (by parent firm) in the three years prior (2) Predicted top 5 share after the mergers complete\n",
    "        \n",
    "        # Underwriters in the market\n",
    "        name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneState_priorMA[parent_name_colnames]))))\n",
    "        name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "        name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "        name_GPFs = list(set(name_GPFs))\n",
    "        n_deals = {}\n",
    "        for item in name_GPFs:\n",
    "            n_deals[item] = 0\n",
    "        \n",
    "        # Record market shares before merger episode\n",
    "        parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "        for idx,row in GPF_oneState_priorMA.iterrows():\n",
    "            underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "            n_underwriters = len(underwriters_onedeal)\n",
    "            for item in underwriters_onedeal:\n",
    "                n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "        n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "        n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "        n_deals_prior = n_deals\n",
    "        \n",
    "        # Top 5 share prior to merger\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_prior = 1\n",
    "        else:\n",
    "            top5share_prior = np.sum(n_deals['marketshare'][:5])\n",
    "\n",
    "        # Implied top 5 share post merger\n",
    "        State_affected_episode = State_affected_episode.reset_index(drop=True)\n",
    "        for idx,row in State_affected_episode.iterrows():\n",
    "            n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "        n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "        n_deals['marketshare'] = n_deals['n_deals']/np.sum(n_deals['n_deals'])\n",
    "        n_deals = n_deals.sort_values(by=['n_deals'],ascending=False).reset_index(drop=True)\n",
    "        if len(n_deals)<=5:\n",
    "            top5share_post = 1\n",
    "        else:\n",
    "            top5share_post = np.sum(n_deals['marketshare'][:5])\n",
    "        n_deals_post = n_deals\n",
    "\n",
    "        top5share_dif = top5share_post-top5share_prior\n",
    "\n",
    "        # Market shares used in summary statistics\n",
    "        State_affected_episode['min_share'] = np.minimum(State_affected_episode['acquiror_market_share_N_avg'],\n",
    "            State_affected_episode['target_market_share_N_avg']+\\\n",
    "            State_affected_episode['other_targets_market_share_N_avg'])\n",
    "        State_affected_episode = State_affected_episode.sort_values('min_share')\n",
    "        State_affected_episode_topshare = State_affected_episode[-1:]\n",
    "        acquiror_market_share_N_max = np.max(State_affected_episode_topshare['acquiror_market_share_N_avg'])\n",
    "        target_market_share_N_max = np.max(State_affected_episode_topshare['target_market_share_N_avg'])\n",
    "        other_targets_market_share_N_max = np.max(State_affected_episode_topshare['other_targets_market_share_N_avg'])\n",
    "\n",
    "        # Check if market share in the episode is high enough\n",
    "        if top5share_dif>0.05:\n",
    "            # An episode is identified\n",
    "            State_episodes_top5share_N = State_episodes_top5share_N+[{\n",
    "                'episode_start_year':sale_year,\n",
    "                'State':State,\n",
    "                'mergers':State_affected_episode,\n",
    "                'top5share_dif':top5share_dif,\n",
    "                'n_deals_prior':n_deals_prior,\n",
    "                'n_deals_post':n_deals_post,\n",
    "                'acquiror_market_share_N_max':acquiror_market_share_N_max,\n",
    "                'target_market_share_N_max':target_market_share_N_max,\n",
    "                'other_targets_market_share_N_max':other_targets_market_share_N_max,\n",
    "                }]\n",
    "            episode_start_year = sale_year\n",
    "\n",
    "State_episodes_top5share_N = pd.DataFrame(State_episodes_top5share_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c78f6f-5fa1-462d-8c13-77cbde237d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(State_episodes_top5share_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc1b2f7-dfe5-43a7-a07a-383e5e860375",
   "metadata": {},
   "source": [
    "## 2.3 Assemble a Treatment-Control Matched Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98246ef-2a43-456f-8ef5-919fc51d04b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported regression sample for Withdrawn M&A, >= 0.002\n"
     ]
    }
   ],
   "source": [
    "episodes_files = [\n",
    "    [\"By Market Share in terms of N deals\",State_episodes_marketshare_N,1,\n",
    "        '../CleanData/MAEvent/State_episodes_marketshareByN.csv',\n",
    "    ],\n",
    "    [\"By Implied HHI Increase in terms of N deals, >= 0.01\",State_episodes_impliedHHI_N,1,\n",
    "        '../CleanData/MAEvent/State_episodes_impliedHHIByN.csv',\n",
    "    ],\n",
    "    [\"By Implied Top 5 Share Increase in terms of N deals, >= 0.01\",State_episodes_top5share_N,1,\n",
    "        '../CleanData/MAEvent/State_episodes_top5shareByN.csv',\n",
    "    ]\n",
    "    ]\n",
    "\n",
    "for episodes_file in episodes_files:\n",
    "\n",
    "    criteria = episodes_file[0]\n",
    "    episodes = episodes_file[1]\n",
    "    N_matches = episodes_file[2]\n",
    "    file_path = episodes_file[3]\n",
    "\n",
    "    episodes = episodes.copy()\n",
    "    \n",
    "    ########################################\n",
    "    # Find control for each merger episode #\n",
    "    ########################################\n",
    "    \n",
    "    # State demographics to be used in merger\n",
    "    State_POP = pd.read_csv(\"../CleanData/Demographics/0C_State_Pop.csv\")\n",
    "    State_INC = pd.read_csv(\"../CleanData/Demographics/0C_State_Inc.csv\")\n",
    "    State_Data = State_POP.merge(State_INC,on=['State','year'])\n",
    "    State_Data = State_Data[['State','year','inc','pop']]\n",
    "    \n",
    "    def calculate_distance(row,weightingmat):\n",
    "        return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "            (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "    \n",
    "    episodes['control'] = None\n",
    "    for idx,row in episodes.iterrows():\n",
    "    \n",
    "        # Find population of this state\n",
    "        State_Data_oneyear = State_Data[State_Data['year']==row['episode_start_year']].copy()\n",
    "    \n",
    "        # Demographic data of the treated state\n",
    "        State_Data_oneyear_frag = State_Data_oneyear[State_Data_oneyear['State']==row['State']].copy()\n",
    "        if len(State_Data_oneyear_frag)==0:\n",
    "            continue\n",
    "        episode_pop = State_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "        episode_inc = State_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "        \n",
    "        # Find a match\n",
    "        State_Data_oneyear['treated_pop'] = episode_pop\n",
    "        State_Data_oneyear['treated_inc'] = episode_inc\n",
    "        # Get weighting matrix\n",
    "        State_Data_oneyear['inc'] = winsor2(State_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "        State_Data_oneyear['pop'] = winsor2(State_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "        cov = State_Data_oneyear[['inc','pop']].cov()\n",
    "        invcov = np.linalg.inv(cov)\n",
    "        State_Data_oneyear['dist'] = State_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "        State_Data_oneyear = State_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "        # Remove oneself from potential matches\n",
    "        State_Data_oneyear = State_Data_oneyear[State_Data_oneyear['State']!=row['State']]\n",
    "    \n",
    "        match_counter = 0\n",
    "        control = []\n",
    "        for subidx,subrow in State_Data_oneyear.iterrows():\n",
    "            # Years for which potential control is treated itself\n",
    "            State_affected_frag = State_affected[State_affected['State']==subrow['State']]\n",
    "            State_affected_frag = State_affected_frag[(State_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                (State_affected_frag['target_market_share_N_avg']+State_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "            State_affected_frag_affected_years = list(State_affected_frag['sale_year'].unique())\n",
    "            # \n",
    "            if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                intersection(set(State_affected_frag_affected_years)))>0:\n",
    "                # This potential control is treated\n",
    "                continue\n",
    "            else:\n",
    "                # This potential control is not treated => Good control\n",
    "                control = control+[subrow['State']]\n",
    "                match_counter = match_counter+1\n",
    "                if match_counter==N_matches:\n",
    "                    break\n",
    "    \n",
    "        episodes.at[idx,'control'] = control\n",
    "    \n",
    "    # Exclude cases where a match cannot be found\n",
    "    print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "    episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "\n",
    "    \n",
    "    #############################################\n",
    "    # Expand to include an event time dimension #\n",
    "    #############################################\n",
    "    \n",
    "    episodes_Exploded = episodes\n",
    "    episodes_Exploded['year_to_merger'] = [list(range(-4,11))]*len(episodes_Exploded)\n",
    "    episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "    episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    # Assemble a regression sample #\n",
    "    ################################\n",
    "\n",
    "    #------------------------#\n",
    "    # Issue level, using GPF #\n",
    "    #------------------------#\n",
    "\n",
    "    reg_sample = []\n",
    "    for idx,row in episodes_Exploded.iterrows():\n",
    "\n",
    "        # Event characteristics - strength\n",
    "        if 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "        else:\n",
    "            acquiror_market_share_avg = None\n",
    "\n",
    "        if 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            target_market_share_avg = row['target_market_share_N_avg']\n",
    "        else:\n",
    "            target_market_share_avg = None\n",
    "\n",
    "        if 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "            other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "        else:\n",
    "            other_targets_market_share_avg = None\n",
    "\n",
    "        if 'hhi_dif' in episodes_Exploded.columns:\n",
    "            hhi_dif = row['hhi_dif']\n",
    "        else:\n",
    "            hhi_dif = None\n",
    "\n",
    "        if 'max_sum_share' in episodes_Exploded.columns:\n",
    "            max_sum_share = row['max_sum_share']\n",
    "        else:\n",
    "            max_sum_share = None\n",
    "\n",
    "        if 'max_min_share' in episodes_Exploded.columns:\n",
    "            max_min_share = row['max_min_share']\n",
    "        else:\n",
    "            max_min_share = None\n",
    "\n",
    "        if 'mean_sum_share' in episodes_Exploded.columns:\n",
    "            mean_sum_share = row['mean_sum_share']\n",
    "        else:\n",
    "            mean_sum_share = None\n",
    "    \n",
    "        # Treated observations\n",
    "        GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['State']==row['State'])].copy()\n",
    "        GPF_Seg = GPF_Seg[[\n",
    "            'CBSA Code','sale_year','State','County',\n",
    "            'issuer_type','Issuer',\n",
    "            'avg_maturity','amount',\n",
    "            'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "            'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "            'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "            'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "            'underpricing_15to60','underpricing_15to30',\n",
    "            'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "            'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "            'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "            'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "            'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "            'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "            'if_callable','CB_Eligible',\n",
    "            'num_relationship',\n",
    "            ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "\n",
    "        #------------------------------------#\n",
    "        # Some cross-sectional heterogeneity #\n",
    "        #------------------------------------#\n",
    "\n",
    "        GPF_Seg['treated'] = 1\n",
    "        GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "        GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "        GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "        GPF_Seg['treated_state'] = row['State'] # Used for constructing cohort X issuer FEs\n",
    "        GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "        GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "        GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "        GPF_Seg['hhi_dif'] = hhi_dif\n",
    "        GPF_Seg['max_sum_share'] = max_sum_share\n",
    "        GPF_Seg['max_min_share'] = max_min_share\n",
    "        GPF_Seg['mean_sum_share'] = mean_sum_share\n",
    "        GPF_Seg_Treated = GPF_Seg\n",
    "\n",
    "        # Control observations\n",
    "        if row['control']==None:\n",
    "            continue\n",
    "        GPF_Seg_Control = pd.DataFrame()\n",
    "        for item in row['control']:\n",
    "            GPF_Seg = GPF[(GPF['sale_year']==row['calendar_year'])&(GPF['State']==item)]\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CBSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "            \n",
    "            GPF_Seg['treated'] = 0\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_state'] = row['State'] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "    \n",
    "        if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "            reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "    \n",
    "    reg_sample = pd.concat(reg_sample)\n",
    "    reg_sample = reg_sample.merge(HHI_byState,on=['State','calendar_year'])\n",
    "    County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "    County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "    reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "    reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "    reg_sample.to_csv(file_path)\n",
    "\n",
    "    print('Exported regression sample for '+episodes_file[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bd5e7-e2a3-4fd7-b2c9-b014be189902",
   "metadata": {},
   "source": [
    "# 3. Construct Events of M&As, Using CSA and Divide Based on Issue Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a4a471-487f-411a-ab73-051b2d23cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BondChars = ['amount_bracket','mat_bracket','use_short','has_ratings','Bid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85356535-6d93-45a0-97bb-f3da2787880c",
   "metadata": {},
   "source": [
    "## 3.1. Find CSA X Year X Bond Characteristic Group affected by merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc408290-9c5d-472c-9907-1b1374d4370f",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Go over each merger. Check the CSAs affected by the merger (i.e., either side has business in the CSA in the year prior to the merger). Check if the merger affects just one underwriter or affects multiple underwriters in this CSA.\n",
    "- Note that for the column \"market share of other targets\", the optimal object to put there is the market share of the other target alone. Here I am instead putting in market share of the other target's parent. This should make a minimal difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf33477-6d86-4e45-b21b-3599ea2a4203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process and export data by both CSA and Bond Characteristic -- amount_bracket.\n",
      "Time spent on the block: 1260.8029 seconds\n",
      "Process and export data by both CSA and Bond Characteristic -- mat_bracket.\n",
      "Time spent on the block: 1068.5463 seconds\n",
      "Process and export data by both CSA and Bond Characteristic -- use_short.\n",
      "Time spent on the block: 2385.6474 seconds\n",
      "Process and export data by both CSA and Bond Characteristic -- has_ratings.\n",
      "Time spent on the block: 424.0019 seconds\n",
      "Process and export data by both CSA and Bond Characteristic -- Bid.\n",
      "Time spent on the block: 889.9771 seconds\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# %%script false --no-raise-error\n",
    "\n",
    "# Affected finer markets by each bond characteristics\n",
    "CSAXBondChar_affected_AllBondChars = {}\n",
    "\n",
    "raw_name_GPF_colnames = [column for column in GPF.columns if 'raw_name_GPF_' in column]\n",
    "name_GPF_colnames = ['name_GPF_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "\n",
    "for BondChar in BondChars:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    #-----------------#\n",
    "    # Define function #\n",
    "    #-----------------#\n",
    "\n",
    "    def proc_list(MA_frag):\n",
    "            \n",
    "        CSAXBondChar_affected = []\n",
    "        MA_frag = MA_frag.reset_index(drop=True)\n",
    "        \n",
    "        market_share_all_markets_byCSAXBondChar = pd.read_csv('../CleanData/SDC/1A_market_share_all_markets_byCSA'+'X'+BondChar+'.csv')\n",
    "        market_share_all_markets_byCSAXBondChar_gb = \\\n",
    "            market_share_all_markets_byCSAXBondChar.groupby(['parent_name','CSA Code',BondChar,'calendar_year'])\n",
    "\n",
    "        for idx,row in MA_frag.iterrows():\n",
    "            \n",
    "            # Find CSAs that this merger affects\n",
    "            # Determine if an underwriter is active in an CSA based on activity of PRIOR years\n",
    "            GPF_prioryears = GPF[(GPF['sale_year']>=row['sale_year']-3)&(GPF['sale_year']<=row['sale_year']-1)]\n",
    "\n",
    "            # All markets, defined using both geographic unit and bond characteristics\n",
    "            markets = []\n",
    "            for itemA in list(GPF_prioryears['CSA Code'].unique()):\n",
    "                for itemB in list(GPF_prioryears[BondChar].unique()):\n",
    "                    markets = markets+[(itemA,itemB)]\n",
    "\n",
    "            # Also check other targets of the acquiror in that year. This accounts for cases where post merger the new formed entity\n",
    "            # is new and appear as a name that was not in the sample before. Note that here \"MA_frag\" cannot be used or the other firm\n",
    "            # involved in the merger will be missed. Instead, use the whole sample \"MA\"\n",
    "            other_targets = \\\n",
    "                list(MA[(MA['acquiror']==row['acquiror'])&\n",
    "                (MA['sale_year']==row['sale_year'])&\n",
    "                (MA['target']!=row['target'])]['target'])\n",
    "            \n",
    "            for market in markets:\n",
    "    \n",
    "                GPF_prioryears_oneCSAXBondChar = GPF_prioryears[(GPF_prioryears['CSA Code']==market[0])&(GPF_prioryears[BondChar]==market[1])]\n",
    "    \n",
    "                # Underwriters in this state\n",
    "                underwriters_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSAXBondChar[name_GPF_colnames]))))\n",
    "                underwriters_priorMA = [item for item in underwriters_priorMA if item!=None]\n",
    "                underwriters_priorMA = list(set(underwriters_priorMA))\n",
    "                # Parents of underwriters in this state\n",
    "                parents_priorMA = list(chain.from_iterable(list(np.array(GPF_prioryears_oneCSAXBondChar[parent_name_colnames]))))\n",
    "                parents_priorMA = [item for item in parents_priorMA if item!=None]\n",
    "                parents_priorMA = list(set(parents_priorMA))\n",
    "                # Subsidiaries of parents in this state (using data of PRIOR year)\n",
    "                subsidiaries_priorMA = list(GPF_names[\n",
    "                    (GPF_names['parent_name'].isin(parents_priorMA))&\n",
    "                    (GPF_names['sale_year']>=row['sale_year']-3)&\n",
    "                    (GPF_names['sale_year']<=row['sale_year']-1)]['name_GPF'])\n",
    "    \n",
    "                # Determine if merger affects the CSA, and if both sides have business\n",
    "                IF_acquiror_active = None\n",
    "                IF_target_active = None\n",
    "                IF_other_target_active = None\n",
    "                if (row['acquiror'] in parents_priorMA) or (row['acquiror'] in underwriters_priorMA) or (row['acquiror'] in subsidiaries_priorMA):\n",
    "                    IF_acquiror_active = True\n",
    "                if (row['target'] in parents_priorMA) or (row['target'] in underwriters_priorMA) or (row['target'] in subsidiaries_priorMA):\n",
    "                    IF_target_active = True\n",
    "                for other_target in other_targets:\n",
    "                    if (other_target in parents_priorMA) or (other_target in underwriters_priorMA):\n",
    "                        IF_other_target_active = True\n",
    "    \n",
    "                # Get market share of merged banks. Note that this is the market share in the years prior to M&A. Also note that market \n",
    "                # share \"market_share_all_markets_byCSA\" is calculated at the parent level. There are many cases where market share of a\n",
    "                # firm in an area is unavailable, which is because of no presence.\n",
    "    \n",
    "    \n",
    "    \n",
    "                #-------------------------#\n",
    "                # Market share by N deals #\n",
    "                #-------------------------#\n",
    "    \n",
    "                # (1) Market share of acquiror\n",
    "                # Determine parent of target, as \"market_share_all_markets_byCSA\" is at parent level\n",
    "                try:\n",
    "                    # Situation where acquiror is a subsidiary or standalone firm whose parent is itself. Extract its parent\n",
    "                    acquiror_parent = GPF_names[(GPF_names['name_GPF']==row['acquiror'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                        .reset_index()['parent_name'][0]\n",
    "                except:\n",
    "                    # Situation where acquiror is a parent\n",
    "                    acquiror_parent = row['acquiror']\n",
    "                try:\n",
    "                    acquiror_market_share_N_m1 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (acquiror_parent,market[0],market[1],row['sale_year']-1))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    acquiror_market_share_N_m1 = 0\n",
    "                try:\n",
    "                    acquiror_market_share_N_m2 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (acquiror_parent,market[0],market[1],row['sale_year']-2))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    acquiror_market_share_N_m2 = 0\n",
    "                try:\n",
    "                    acquiror_market_share_N_m3 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (acquiror_parent,market[0],market[1],row['sale_year']-3))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    acquiror_market_share_N_m3 = 0\n",
    "    \n",
    "                # (2) Market share of target\n",
    "                try:\n",
    "                    # Note that I must use \"GPF_names\" (the parent-subsidiary) mapping use the year(s) prior to the MA\n",
    "                    target_parent = GPF_names[(GPF_names['name_GPF']==row['target'])&(GPF_names['sale_year']==row['sale_year']-1)]\\\n",
    "                        .reset_index()['parent_name'][0]\n",
    "                except:\n",
    "                    target_parent = row['target']\n",
    "                try:\n",
    "                    target_market_share_N_m1 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (target_parent,market[0],market[1],row['sale_year']-1))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    target_market_share_N_m1 = 0\n",
    "                try:\n",
    "                    target_market_share_N_m2 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (target_parent,market[0],market[1],row['sale_year']-2))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    target_market_share_N_m2 = 0\n",
    "                try:\n",
    "                    target_market_share_N_m3 = market_share_all_markets_byCSAXBondChar_gb.get_group(\n",
    "                        (target_parent,market[0],market[1],row['sale_year']-3))\\\n",
    "                        .reset_index()['market_share_N'][0]\n",
    "                except:\n",
    "                    target_market_share_N_m3 = 0\n",
    "    \n",
    "                # (3) Market share of other targets in the same transaction\n",
    "                # Account for possibility that other targets can be either a parent or a standalone firm\n",
    "                other_targets_parents = \\\n",
    "                    list(GPF_names[(GPF_names['name_GPF'].isin(other_targets))\n",
    "                    &(GPF_names['sale_year']==row['sale_year']-1)]['parent_name'])+\\\n",
    "                    list(other_targets)\n",
    "                other_targets_parents = list(set(other_targets_parents))\n",
    "\n",
    "                if len(other_targets_parents)==0:\n",
    "                    other_targets_market_share_N_m1 = 0\n",
    "                else:\n",
    "                    other_targets_market_share_N = \\\n",
    "                        market_share_all_markets_byCSAXBondChar[\n",
    "                        (market_share_all_markets_byCSAXBondChar['parent_name'].isin(other_targets_parents))\n",
    "                        &(market_share_all_markets_byCSAXBondChar['CSA Code']==market[0])\n",
    "                        &(market_share_all_markets_byCSAXBondChar[BondChar]==market[1])\n",
    "                        &(market_share_all_markets_byCSAXBondChar['calendar_year']==row['sale_year']-1)]\n",
    "                    if len(other_targets_market_share_N)>0:\n",
    "                        other_targets_market_share_N_m1 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "                    else:\n",
    "                        other_targets_market_share_N_m1 = 0\n",
    "\n",
    "                if len(other_targets_parents)==0:\n",
    "                    other_targets_market_share_N_m2 = 0\n",
    "                else:\n",
    "                    other_targets_market_share_N = \\\n",
    "                        market_share_all_markets_byCSAXBondChar[\n",
    "                        (market_share_all_markets_byCSAXBondChar['parent_name'].isin(other_targets_parents))\n",
    "                        &(market_share_all_markets_byCSAXBondChar['CSA Code']==market[0])\n",
    "                        &(market_share_all_markets_byCSAXBondChar[BondChar]==market[1])\n",
    "                        &(market_share_all_markets_byCSAXBondChar['calendar_year']==row['sale_year']-2)]\n",
    "                    if len(other_targets_market_share_N)>0:\n",
    "                        other_targets_market_share_N_m2 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "                    else:\n",
    "                        other_targets_market_share_N_m2 = 0\n",
    "\n",
    "                if len(other_targets_parents)==0:\n",
    "                    other_targets_market_share_N_m3 = 0\n",
    "                else:\n",
    "                    other_targets_market_share_N = \\\n",
    "                        market_share_all_markets_byCSAXBondChar[\n",
    "                        (market_share_all_markets_byCSAXBondChar['parent_name'].isin(other_targets_parents))\n",
    "                        &(market_share_all_markets_byCSAXBondChar['CSA Code']==market[0])\n",
    "                        &(market_share_all_markets_byCSAXBondChar[BondChar]==market[1])\n",
    "                        &(market_share_all_markets_byCSAXBondChar['calendar_year']==row['sale_year']-3)]\n",
    "                    if len(other_targets_market_share_N)>0:\n",
    "                        other_targets_market_share_N_m3 = np.sum(other_targets_market_share_N['market_share_N'])\n",
    "                    else:\n",
    "                        other_targets_market_share_N_m3 = 0\n",
    "\n",
    "                # Record data\n",
    "                if IF_acquiror_active or IF_target_active or IF_other_target_active:\n",
    "                    CSAXBondChar_affected = CSAXBondChar_affected+[{\n",
    "                        'CSA Code':market[0],\n",
    "                        BondChar:market[1],\n",
    "                        'sale_year':row['sale_year'],\n",
    "                        'acquiror':row['acquiror'],\n",
    "                        'target':row['target'],\n",
    "                        'other_targets':other_targets,\n",
    "                        'acquiror_parent':acquiror_parent,\n",
    "                        'target_parent':target_parent,\n",
    "                        'acquiror_market_share_N_m1':acquiror_market_share_N_m1,\n",
    "                        'acquiror_market_share_N_m2':acquiror_market_share_N_m2,\n",
    "                        'acquiror_market_share_N_m3':acquiror_market_share_N_m3,\n",
    "                        'target_market_share_N_m1':target_market_share_N_m1,\n",
    "                        'target_market_share_N_m2':target_market_share_N_m2,\n",
    "                        'target_market_share_N_m3':target_market_share_N_m3,\n",
    "                        'other_targets_market_share_N_m1':other_targets_market_share_N_m1,\n",
    "                        'other_targets_market_share_N_m2':other_targets_market_share_N_m2,\n",
    "                        'other_targets_market_share_N_m3':other_targets_market_share_N_m3,\n",
    "                    }]\n",
    "                acquiror_market_share_N_m1 = None\n",
    "                acquiror_market_share_N_m2 = None\n",
    "                acquiror_market_share_N_m3 = None\n",
    "                target_market_share_N_m1 = None\n",
    "                target_market_share_N_m2 = None\n",
    "                target_market_share_N_m3 = None\n",
    "                other_targets_market_share = None\n",
    "                other_targets_market_share_N_m1 = None\n",
    "                other_targets_market_share_N_m2 = None\n",
    "                other_targets_market_share_N_m3 = None\n",
    "        \n",
    "        CSAXBondChar_affected = pd.DataFrame(CSAXBondChar_affected)\n",
    "        return CSAXBondChar_affected\n",
    "\n",
    "    #--------------#\n",
    "    # Process data #\n",
    "    #--------------#\n",
    "\n",
    "    MA_dd = dd.from_pandas(MA, npartitions=20)\n",
    "    with dask.config.set(scheduler='processes',num_workers=20):\n",
    "        CSAXBondChar_affected = MA_dd.map_partitions(proc_list, \n",
    "        meta=pd.DataFrame(columns=\n",
    "        ['CSA Code',BondChar,'sale_year','acquiror','target',\n",
    "        'other_targets','acquiror_parent','target_parent',\n",
    "        'acquiror_market_share_N_m1','acquiror_market_share_N_m2','acquiror_market_share_N_m3',\n",
    "        'target_market_share_N_m1','target_market_share_N_m2','target_market_share_N_m3',\n",
    "        'other_targets_market_share_N_m1','other_targets_market_share_N_m2','other_targets_market_share_N_m3',\n",
    "        ])).compute()\n",
    "    \n",
    "    # Average market share over past three years\n",
    "    CSAXBondChar_affected['acquiror_market_share_N_avg'] = \\\n",
    "        (CSAXBondChar_affected['acquiror_market_share_N_m1']+\\\n",
    "        CSAXBondChar_affected['acquiror_market_share_N_m2']+\\\n",
    "        CSAXBondChar_affected['acquiror_market_share_N_m3'])/3\n",
    "    CSAXBondChar_affected['target_market_share_N_avg'] = \\\n",
    "        (CSAXBondChar_affected['target_market_share_N_m1']+\\\n",
    "        CSAXBondChar_affected['target_market_share_N_m2']+\\\n",
    "        CSAXBondChar_affected['target_market_share_N_m3'])/3\n",
    "    CSAXBondChar_affected['other_targets_market_share_N_avg'] = \\\n",
    "        (CSAXBondChar_affected['other_targets_market_share_N_m1']+\\\n",
    "        CSAXBondChar_affected['other_targets_market_share_N_m2']+\\\n",
    "        CSAXBondChar_affected['other_targets_market_share_N_m3'])/3\n",
    "    \n",
    "    # As this step takes significant time, export output\n",
    "    CSAXBondChar_affected.to_parquet('../CleanData/MAEvent/1B_CSA_affected_'+BondChar+'.parquet')\n",
    "\n",
    "    CSAXBondChar_affected_AllBondChars[BondChar] = CSAXBondChar_affected\n",
    "\n",
    "    print('Process and export data by both CSA and Bond Characteristic -- '+BondChar+'.')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time spent on the block: {elapsed_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd968304-af14-4103-8adf-fec9efc79a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f51552-7491-4b44-bb4b-6c0932be385d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ee22c1-c8ce-471a-87b9-247787a93ffe",
   "metadata": {},
   "source": [
    "## 3.2 Identify merger episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9608ec6-4ef5-4564-80e9-a34ac29f15fa",
   "metadata": {},
   "source": [
    "### 3.2.1 Method: By implied rise in HHI due to merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02ec6c2c-a1fc-46e2-9551-58a7a525ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSAXBondChar_episodes_impliedHHI_N_AllBondChars = {}\n",
    "\n",
    "for BondChar in BondChars:\n",
    "\n",
    "    CSAXBondChar_affected = CSAXBondChar_affected_AllBondChars[BondChar]\n",
    "\n",
    "    #-----------------------------#\n",
    "    # Change in HHI by N of deals #\n",
    "    #-----------------------------#\n",
    "    \n",
    "    # Identify episodes of mergers at the CSA level\n",
    "    \n",
    "    # Go over each year with merger event, and check the M&As on this year and three years afterwards. If enough consolidation, an episode is \n",
    "    # identified. Whether there is enough consolidation can be judged by average market share in the past three years, or market share just in \n",
    "    # the year minus one. For each identified merger episode, check if there is reasonable control in the sample. Note that there can be duplicates\n",
    "    # as in cases where two firms merge into a new one, both will get recorded in \"CSA_affected\"\n",
    "    \n",
    "    CSAXBondChar_episodes_impliedHHI_N = []\n",
    "\n",
    "    CSAXBondChar_affected_unique = CSAXBondChar_affected[['CSA Code',BondChar]].drop_duplicates()\n",
    "\n",
    "    for idx,row in CSAXBondChar_affected_unique.iterrows():\n",
    "    \n",
    "        CSAXBondChar_affected_part = \\\n",
    "            CSAXBondChar_affected[(CSAXBondChar_affected['CSA Code']==row['CSA Code'])&(CSAXBondChar_affected[BondChar]==row[BondChar])]\n",
    "        CSAXBondChar_affected_part = CSAXBondChar_affected_part[\n",
    "            (CSAXBondChar_affected_part['acquiror_market_share_N_avg']>0)&\n",
    "            ((CSAXBondChar_affected_part['target_market_share_N_avg']>0)|\n",
    "            (CSAXBondChar_affected_part['other_targets_market_share_N_avg']>0))].sort_values('sale_year')\n",
    "        \n",
    "        episode_start_year = 1900\n",
    "        for sale_year in CSAXBondChar_affected_part['sale_year'].unique():\n",
    "        \n",
    "            # If this year is still within the last merger episode\n",
    "            if sale_year<=episode_start_year+4:\n",
    "                continue\n",
    "            \n",
    "            # Check intensity of M&A activities in that year and three years following\n",
    "            CSAXBondChar_affected_episode = CSAXBondChar_affected_part[\n",
    "                (CSAXBondChar_affected_part['sale_year']>=sale_year)&\n",
    "                (CSAXBondChar_affected_part['sale_year']<=sale_year+3)]\n",
    "            GPF_oneCSAXBondChar_priorMA = GPF[\n",
    "                (GPF['sale_year']>=sale_year-3)&(GPF['sale_year']<=sale_year)\n",
    "                &(GPF['CSA Code']==row['CSA Code'])\n",
    "                &(GPF[BondChar]==row[BondChar])]\n",
    "\n",
    "            # Calculate (1) HHI (by parent firm) in the three years prior (2) Predicted HHI after the mergers complete\n",
    "            \n",
    "            # Underwriters in the market\n",
    "            name_GPFs = list(chain.from_iterable(list(np.array(GPF_oneCSAXBondChar_priorMA[parent_name_colnames]))))\n",
    "            name_GPFs = [item for item in name_GPFs if item!=None]\n",
    "            name_GPFs = [item for item in name_GPFs if str(item)!='nan']\n",
    "            name_GPFs = list(set(name_GPFs))\n",
    "            n_deals = {}\n",
    "            for item in name_GPFs:\n",
    "                n_deals[item] = 0\n",
    "            \n",
    "            # Record market shares before merger episode\n",
    "            parent_name_colnames = ['parent_name_'+str(i) for i in range(0,len(raw_name_GPF_colnames))]\n",
    "            for idx,row in GPF_oneCSAXBondChar_priorMA.iterrows():\n",
    "                underwriters_onedeal = [row[item] for item in parent_name_colnames if row[item]!=None and str(row[item])!='nan']\n",
    "                n_underwriters = len(underwriters_onedeal)\n",
    "                for item in underwriters_onedeal:\n",
    "                    n_deals[item] = n_deals[item]+1/n_underwriters\n",
    "            n_deals = pd.DataFrame.from_dict(n_deals,orient='index').reset_index()\n",
    "            n_deals = n_deals.rename(columns={'index':'underwriter',0:'n_deals'})\n",
    "            n_deals_prior = n_deals\n",
    "            \n",
    "            # HHI prior to merger\n",
    "            hhi_piror = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "    \n",
    "            # Implied HHI post merger\n",
    "            CSAXBondChar_affected_episode = CSAXBondChar_affected_episode.reset_index(drop=True)\n",
    "            for idx,row in CSAXBondChar_affected_episode.iterrows():\n",
    "                n_deals.loc[n_deals['underwriter']==row['target'],'underwriter'] = row['acquiror_parent']\n",
    "            n_deals = n_deals.groupby('underwriter').agg({'n_deals':sum}).reset_index()\n",
    "            hhi_predicted = np.sum((n_deals['n_deals']/np.sum(n_deals['n_deals']))**2)\n",
    "            n_deals_post = n_deals\n",
    "    \n",
    "            hhi_dif = hhi_predicted-hhi_piror\n",
    "        \n",
    "            # Check if market share in the episode is high enough\n",
    "            if hhi_dif>0.01:\n",
    "                # An episode is identified\n",
    "                CSAXBondChar_episodes_impliedHHI_N = CSAXBondChar_episodes_impliedHHI_N+[{\n",
    "                    'episode_start_year':sale_year,\n",
    "                    'CSA Code':row['CSA Code'],\n",
    "                    BondChar:row[BondChar],\n",
    "                    'mergers':CSAXBondChar_affected_episode,\n",
    "                    'hhi_dif':hhi_dif,\n",
    "                    'n_deals_prior':n_deals_prior,\n",
    "                    'n_deals_post':n_deals_post,\n",
    "                    }]\n",
    "                episode_start_year = sale_year\n",
    "    \n",
    "    CSAXBondChar_episodes_impliedHHI_N = pd.DataFrame(CSAXBondChar_episodes_impliedHHI_N)\n",
    "    \n",
    "    CSAXBondChar_episodes_impliedHHI_N_AllBondChars[BondChar] = CSAXBondChar_episodes_impliedHHI_N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657c667-9421-4cac-ae03-ed1cdbde71bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3543f260-353b-43b3-8ca6-8a0f82f38ae0",
   "metadata": {},
   "source": [
    "## 3.3 Assemble a Treatment-Control Matched Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c202e9af-b8e5-4dc8-939a-4cab0588aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 0 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 1 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n",
      "A control cannot be found for 1 episodes.\n",
      "Exported regression sample for By Implied HHI Increase in terms of N deals, >= 0.01\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "for BondChar in BondChars:\n",
    "\n",
    "    CSAXBondChar_affected = CSAXBondChar_affected_AllBondChars[BondChar]\n",
    "    CSAXBondChar_episodes_impliedHHI_N = CSAXBondChar_episodes_impliedHHI_N_AllBondChars[BondChar]\n",
    "\n",
    "    episodes_files = [\n",
    "        [\"By Implied HHI Increase in terms of N deals, >= 0.01\",CSAXBondChar_episodes_impliedHHI_N,1,\n",
    "            '../CleanData/MAEvent/CSAXBondChar_'+BondChar+'_episodes_impliedHHIByN.csv',\n",
    "        ],\n",
    "        ]\n",
    "\n",
    "    for episodes_file in episodes_files:\n",
    "    \n",
    "        criteria = episodes_file[0]\n",
    "        episodes = episodes_file[1]\n",
    "        N_matches = episodes_file[2]\n",
    "        file_path = episodes_file[3]\n",
    "    \n",
    "        episodes = episodes.copy()\n",
    "        \n",
    "        ########################################\n",
    "        # Find control for each merger episode #\n",
    "        ########################################\n",
    "        \n",
    "        # State demographics to be used in merger\n",
    "        CSA_POP = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Pop.csv\")\n",
    "        CSA_INC = pd.read_csv(\"../CleanData/Demographics/0C_CSA_Inc.csv\")\n",
    "        CSA_Data = CSA_POP.merge(CSA_INC,on=['CSA Code','year'])\n",
    "        CSA_Data = CSA_Data[['CSA Code','year','inc','pop']]\n",
    "        Same_State_CSA_pairs = pd.read_csv(\"../CleanData/Demographics/0C_Same_State_CSA_pairs.csv\")\n",
    "        \n",
    "        def calculate_distance(row,weightingmat):\n",
    "            return sp.spatial.distance.mahalanobis((row['inc'],row['pop']),\\\n",
    "                (row['treated_inc'],row['treated_pop']),weightingmat)\n",
    "        \n",
    "        episodes['control'] = None\n",
    "        for idx,row in episodes.iterrows():\n",
    "        \n",
    "            # Find population of this CSA\n",
    "            CSA_Data_oneyear = CSA_Data[CSA_Data['year']==row['episode_start_year']].copy()\n",
    "        \n",
    "            # Demographic data of the treated CSA\n",
    "            CSA_Data_oneyear_frag = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']==row['CSA Code']].copy()\n",
    "            if len(CSA_Data_oneyear_frag)==0:\n",
    "                continue\n",
    "            episode_pop = CSA_Data_oneyear_frag.reset_index()['pop'][0]\n",
    "            episode_inc = CSA_Data_oneyear_frag.reset_index()['inc'][0]\n",
    "            \n",
    "            # Find a match\n",
    "            CSA_Data_oneyear['treated_pop'] = episode_pop\n",
    "            CSA_Data_oneyear['treated_inc'] = episode_inc\n",
    "            # Get weighting matrix\n",
    "            CSA_Data_oneyear['inc'] = winsor2(CSA_Data_oneyear['inc'],cutoffs=[0.05,0.05])\n",
    "            CSA_Data_oneyear['pop'] = winsor2(CSA_Data_oneyear['pop'],cutoffs=[0.05,0.05])\n",
    "            cov = CSA_Data_oneyear[['inc','pop']].cov()\n",
    "            invcov = np.linalg.inv(cov)\n",
    "            CSA_Data_oneyear['dist'] = CSA_Data_oneyear.apply(calculate_distance, axis=1,weightingmat=invcov)\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear.sort_values('dist').reset_index(drop=True)\n",
    "            # Remove oneself from potential matches\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[CSA_Data_oneyear['CSA Code']!=row['CSA Code']]\n",
    "            # Remove other CSAs in the same state from potential matches\n",
    "            Same_State_CSAs = list(Same_State_CSA_pairs[Same_State_CSA_pairs['CSA_1']==row['CSA Code']]['CSA_2'])\n",
    "            CSA_Data_oneyear = CSA_Data_oneyear[~CSA_Data_oneyear['CSA Code'].isin(Same_State_CSAs)]\n",
    "        \n",
    "            match_counter = 0\n",
    "            control = []\n",
    "            for subidx,subrow in CSA_Data_oneyear.iterrows():\n",
    "                # Years for which potential control is treated itself\n",
    "                CSAXBondChar_affected_frag = CSAXBondChar_affected[CSAXBondChar_affected['CSA Code']==subrow['CSA Code']]\n",
    "                CSAXBondChar_affected_frag = CSAXBondChar_affected_frag[(CSAXBondChar_affected_frag['acquiror_market_share_N_avg']>0.01)&\n",
    "                    (CSAXBondChar_affected_frag['target_market_share_N_avg']+CSAXBondChar_affected_frag['other_targets_market_share_N_avg']>0.01)]\n",
    "                CSAXBondChar_affected_frag_affected_years = list(CSAXBondChar_affected_frag['sale_year'].unique())\n",
    "                # \n",
    "                if len(set(list(range(row['episode_start_year']-4,row['episode_start_year']+5))).\\\n",
    "                    intersection(set(CSAXBondChar_affected_frag_affected_years)))>0:\n",
    "                    # This potential control is treated\n",
    "                    continue\n",
    "                else:\n",
    "                    # This potential control is not treated => Good control\n",
    "                    control = control+[subrow['CSA Code']]\n",
    "                    match_counter = match_counter+1\n",
    "                    if match_counter==N_matches:\n",
    "                        break\n",
    "        \n",
    "            episodes.at[idx,'control'] = control\n",
    "        \n",
    "        # Exclude cases where a match cannot be found\n",
    "        print('A control cannot be found for '+str(np.sum(pd.isnull(episodes['control'])))+' episodes.')\n",
    "        episodes = episodes[~pd.isnull(episodes['control'])]\n",
    "    \n",
    "        \n",
    "        #############################################\n",
    "        # Expand to include an event time dimension #\n",
    "        #############################################\n",
    "        \n",
    "        episodes_Exploded = episodes\n",
    "        episodes_Exploded['year_to_merger'] = [list(range(-4,11))]*len(episodes_Exploded)\n",
    "        episodes_Exploded = episodes_Exploded.explode('year_to_merger')\n",
    "        episodes_Exploded['calendar_year'] = episodes_Exploded['episode_start_year']+episodes_Exploded['year_to_merger']    \n",
    "    \n",
    "        \n",
    "        ################################\n",
    "        # Assemble a regression sample #\n",
    "        ################################\n",
    "    \n",
    "        #------------------------#\n",
    "        # Issue level, using GPF #\n",
    "        #------------------------#\n",
    "    \n",
    "        reg_sample = []\n",
    "        for idx,row in episodes_Exploded.iterrows():\n",
    "    \n",
    "            # Event characteristics - strength\n",
    "            if 'acquiror_market_share_N_avg' in episodes_Exploded.columns:\n",
    "                acquiror_market_share_avg = row['acquiror_market_share_N_avg']\n",
    "            else:\n",
    "                acquiror_market_share_avg = None\n",
    "    \n",
    "            if 'target_market_share_N_avg' in episodes_Exploded.columns:\n",
    "                target_market_share_avg = row['target_market_share_N_avg']\n",
    "            else:\n",
    "                target_market_share_avg = None\n",
    "    \n",
    "            if 'other_targets_market_share_N_avg' in episodes_Exploded.columns:\n",
    "                other_targets_market_share_avg = row['other_targets_market_share_N_avg']\n",
    "            else:\n",
    "                other_targets_market_share_avg = None\n",
    "    \n",
    "            if 'hhi_dif' in episodes_Exploded.columns:\n",
    "                hhi_dif = row['hhi_dif']\n",
    "            else:\n",
    "                hhi_dif = None\n",
    "        \n",
    "            # Treated observations\n",
    "            GPF_Seg = GPF[\n",
    "                (GPF['sale_year']==row['calendar_year'])&\n",
    "                (GPF[BondChar]==row[BondChar])&\n",
    "                (GPF['CSA Code']==row['CSA Code'])\n",
    "                ].copy()\n",
    "            GPF_Seg = GPF_Seg[[\n",
    "                'CSA Code','sale_year','State','County',\n",
    "                'issuer_type','Issuer',\n",
    "                'avg_maturity','amount',\n",
    "                'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                'underpricing_15to60','underpricing_15to30',\n",
    "                'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                'if_callable','CB_Eligible',\n",
    "                'num_relationship',\n",
    "                ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "    \n",
    "            #------------------------------------#\n",
    "            # Some cross-sectional heterogeneity #\n",
    "            #------------------------------------#\n",
    "    \n",
    "            # Note that I am check if bank is involved in any mergers in [-4,+4], instead of if bank is involved in mergers (the above\n",
    "            # code block)\n",
    "            mergers = CSAXBondChar_affected[\n",
    "                (CSAXBondChar_affected['CSA Code']==row['CSA Code'])&\n",
    "                (CSAXBondChar_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                (CSAXBondChar_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "            mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "            # Whether the underwriter is the target bank in M&A\n",
    "            GPF_Seg['bank_is_target'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_target'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                |(GPF_Seg['bank_is_target'])\n",
    "            # Whether the underwriter is the acquiror bank in M&A\n",
    "            GPF_Seg['bank_is_acquiror'] = False\n",
    "            for column in name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "            for column in parent_name_GPF_colnames:\n",
    "                GPF_Seg['bank_is_acquiror'] = \\\n",
    "                (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                |(GPF_Seg['bank_is_acquiror'])\n",
    "    \n",
    "            GPF_Seg['treated'] = 1\n",
    "            GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "            GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "            GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "            GPF_Seg['treated_csa'] = row['CSA Code'] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['treated_BondChar'] = row[BondChar] # Used for constructing cohort X issuer FEs\n",
    "            GPF_Seg['acquiror_market_share_avg'] = acquiror_market_share_avg\n",
    "            GPF_Seg['target_market_share_avg'] = target_market_share_avg\n",
    "            GPF_Seg['other_targets_market_share_avg'] = other_targets_market_share_avg\n",
    "            GPF_Seg['hhi_dif'] = hhi_dif\n",
    "            GPF_Seg_Treated = GPF_Seg\n",
    "    \n",
    "            # Control observations\n",
    "            if row['control']==None:\n",
    "                continue\n",
    "            GPF_Seg_Control = pd.DataFrame()\n",
    "            for item in row['control']:\n",
    "                GPF_Seg = GPF[\n",
    "                    (GPF['sale_year']==row['calendar_year'])&\n",
    "                    (GPF[BondChar]==row[BondChar])&\n",
    "                    (GPF['CSA Code']==item)\n",
    "                    ].copy()\n",
    "                GPF_Seg = GPF_Seg[[\n",
    "                    'CSA Code','sale_year','State','County',\n",
    "                    'issuer_type','Issuer',\n",
    "                    'avg_maturity','amount',\n",
    "                    'avg_yield','treasury_avg_spread','MMA_avg_spread',\n",
    "                    'gross_spread','gross_spread_tic_based','gross_spread_nic_based',\n",
    "                    'mod_tic','mod_tic_spread_treasury','mod_tic_spread_MMA',\n",
    "                    'mod_tic_timeFE','mod_tic_spread_treasury_timeFE','mod_tic_spread_MMA_timeFE',\n",
    "                    'underpricing_15to60','underpricing_15to30',\n",
    "                    'Bid','taxable_code','security_type','if_advisor','if_dual_advisor','if_refunding',\n",
    "                    'amount_bracket','mat_bracket','use_short','has_ratings',\n",
    "                    'use_of_proceeds_BB','use_of_proceeds_main','use_of_proceeds_general',\n",
    "                    'has_Moodys','has_Fitch','rating_Moodys','rating_Fitch','insured_amount',\n",
    "                    'AdvisorFeeRatio_hat','CRFeeRatio_hat','InsureFeeRatio_hat',\n",
    "                    'AdvisorFeeRatio_hat_model_timeFE','CRFeeRatio_hat_model_timeFE','InsureFeeRatio_hat_model_timeFE',\n",
    "                    'if_callable','CB_Eligible',\n",
    "                    'num_relationship',\n",
    "                    ]+name_GPF_colnames+parent_name_GPF_colnames]\n",
    "    \n",
    "                # Note that for control banks, \"bank_is_target\" and \"bank_is_acquiror\" use M&A in the specific areas\n",
    "                mergers = CSAXBondChar_affected[\n",
    "                    (CSAXBondChar_affected['CSA Code']==item)&\n",
    "                    (CSAXBondChar_affected['sale_year']>=row['episode_start_year']-4)&\n",
    "                    (CSAXBondChar_affected['sale_year']<=row['episode_start_year']+4)\n",
    "                    ][['acquiror','target','acquiror_parent','target_parent',\n",
    "                    'acquiror_market_share_N_avg','target_market_share_N_avg','other_targets_market_share_N_avg']]\n",
    "                mergers = mergers[(mergers['acquiror_market_share_N_avg']>0)&(mergers['target_market_share_N_avg']+mergers['other_targets_market_share_N_avg']>0)]\n",
    "                # Whether the underwriter is the target bank in M&A\n",
    "                GPF_Seg['bank_is_target'] = False\n",
    "                for column in name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_target'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                    |(GPF_Seg['bank_is_target'])\n",
    "                for column in parent_name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_target'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['target'])+list(mergers['target_parent']))) \\\n",
    "                    |(GPF_Seg['bank_is_target'])\n",
    "                # Whether the underwriter is the acquiror bank in M&A\n",
    "                GPF_Seg['bank_is_acquiror'] = False\n",
    "                for column in name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_acquiror'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                    |(GPF_Seg['bank_is_acquiror'])\n",
    "                for column in parent_name_GPF_colnames:\n",
    "                    GPF_Seg['bank_is_acquiror'] = \\\n",
    "                    (GPF_Seg[column].isin(list(mergers['acquiror'])+list(mergers['acquiror_parent'])))\\\n",
    "                    |(GPF_Seg['bank_is_acquiror'])\n",
    "                \n",
    "                GPF_Seg['treated'] = 0\n",
    "                GPF_Seg['episode_start_year'] = row['episode_start_year']\n",
    "                GPF_Seg['year_to_merger'] = row['year_to_merger']\n",
    "                GPF_Seg['calendar_year'] = row['calendar_year']\n",
    "                GPF_Seg['treated_csa'] = row['CSA Code'] # Used for constructing cohort X issuer FEs\n",
    "                GPF_Seg['treated_BondChar'] = row[BondChar] # Used for constructing cohort X issuer FEs\n",
    "                GPF_Seg['hhi_dif'] = hhi_dif\n",
    "                GPF_Seg_Control = pd.concat([GPF_Seg_Control,GPF_Seg])\n",
    "        \n",
    "            if len(GPF_Seg_Treated)>0 and len(GPF_Seg_Control)>0:\n",
    "                reg_sample = reg_sample+[GPF_Seg_Treated,GPF_Seg_Control]\n",
    "        \n",
    "        reg_sample = pd.concat(reg_sample)\n",
    "        reg_sample = reg_sample.merge(HHI_byCSA,on=['CSA Code','calendar_year'])\n",
    "        County_Composite = pd.read_csv(\"../CleanData/Demographics/0C_County_Composite.csv\")\n",
    "        County_Composite = County_Composite[['year','State','County','black_ratio','pop']].rename(columns={'year':'calendar_year'})\n",
    "        reg_sample = reg_sample.merge(County_Composite,on=['State','County','calendar_year'],how='outer',indicator=True)\n",
    "        reg_sample = reg_sample[reg_sample['_merge']!='right_only'].drop(columns=['_merge'])\n",
    "        reg_sample.to_csv(file_path)\n",
    "    \n",
    "        print('Exported regression sample for '+episodes_file[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
